{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77920bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 04:34:59.008058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-22 04:34:59.256754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:\n",
      "2023-06-22 04:34:59.256774: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-22 04:34:59.284195: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-22 04:35:00.169517: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:\n",
      "2023-06-22 04:35:00.169628: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:\n",
      "2023-06-22 04:35:00.169634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jayasanka/Projects/nervotech test/HR_estimator_redesign/Data_preprocessing.py:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  pd.np.random.seed(random_seed)\n"
     ]
    }
   ],
   "source": [
    "from models import Models\n",
    "from Data_preprocessing import split_60 ,split_train_val_test ,load_data\n",
    "from utils import save_data, split_train_test ,plot_scatter_hr ,plot_loss ,calculate_loss_hr\n",
    "from custom_callbacks import EarlyStoppingIncreasingValLoss\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "from keras.optimizers import RMSprop ,Adam ,SGD ,Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58945a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model2_hr'\n",
    "NUM_EPOCH = 180\n",
    "scaled = False\n",
    "prepros = '60_no_overlap'\n",
    "target = 'calculated_hr'\n",
    "\n",
    "# lr = 5e-4\n",
    "# optimizer = RMSprop(learning_rate=lr)\n",
    "# loss = \"mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5462fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3108, 1036, 1036)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>hr</th>\n",
       "      <th>calculated_hr</th>\n",
       "      <th>DBP</th>\n",
       "      <th>SBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022165</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>-0.005694</td>\n",
       "      <td>-0.018277</td>\n",
       "      <td>-0.028401</td>\n",
       "      <td>-0.034445</td>\n",
       "      <td>-0.034832</td>\n",
       "      <td>-0.028548</td>\n",
       "      <td>-0.015653</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006408</td>\n",
       "      <td>-0.014542</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>-0.049071</td>\n",
       "      <td>-0.058827</td>\n",
       "      <td>-0.061907</td>\n",
       "      <td>70</td>\n",
       "      <td>99.23</td>\n",
       "      <td>75</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002396</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>-0.009561</td>\n",
       "      <td>-0.010251</td>\n",
       "      <td>-0.009101</td>\n",
       "      <td>-0.005841</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023915</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.027309</td>\n",
       "      <td>85</td>\n",
       "      <td>86.31</td>\n",
       "      <td>86</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-0.004098</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>-0.001656</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055547</td>\n",
       "      <td>-0.063149</td>\n",
       "      <td>-0.066136</td>\n",
       "      <td>-0.062301</td>\n",
       "      <td>-0.050670</td>\n",
       "      <td>-0.031967</td>\n",
       "      <td>59</td>\n",
       "      <td>57.83</td>\n",
       "      <td>87</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010544</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027160</td>\n",
       "      <td>-0.043717</td>\n",
       "      <td>-0.057046</td>\n",
       "      <td>-0.066155</td>\n",
       "      <td>-0.070509</td>\n",
       "      <td>-0.070022</td>\n",
       "      <td>86</td>\n",
       "      <td>68.22</td>\n",
       "      <td>80</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.007411</td>\n",
       "      <td>-0.007268</td>\n",
       "      <td>-0.007012</td>\n",
       "      <td>-0.006467</td>\n",
       "      <td>-0.005435</td>\n",
       "      <td>-0.003734</td>\n",
       "      <td>-0.001218</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023944</td>\n",
       "      <td>-0.009014</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.022151</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.047166</td>\n",
       "      <td>85</td>\n",
       "      <td>72.06</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>0.028062</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>-0.014660</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>-0.036512</td>\n",
       "      <td>-0.043327</td>\n",
       "      <td>-0.046852</td>\n",
       "      <td>-0.046867</td>\n",
       "      <td>-0.043359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031265</td>\n",
       "      <td>-0.024912</td>\n",
       "      <td>-0.017063</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>82</td>\n",
       "      <td>82.40</td>\n",
       "      <td>79</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>0.054225</td>\n",
       "      <td>0.071998</td>\n",
       "      <td>0.084893</td>\n",
       "      <td>0.091340</td>\n",
       "      <td>0.090327</td>\n",
       "      <td>0.081546</td>\n",
       "      <td>0.065480</td>\n",
       "      <td>0.043417</td>\n",
       "      <td>0.017377</td>\n",
       "      <td>-0.010103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.020267</td>\n",
       "      <td>-0.026010</td>\n",
       "      <td>86</td>\n",
       "      <td>77.07</td>\n",
       "      <td>80</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5177</th>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.033181</td>\n",
       "      <td>0.017340</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>-0.045684</td>\n",
       "      <td>-0.052995</td>\n",
       "      <td>-0.054832</td>\n",
       "      <td>-0.050976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068069</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>0.078598</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>0.058184</td>\n",
       "      <td>0.038529</td>\n",
       "      <td>93</td>\n",
       "      <td>90.76</td>\n",
       "      <td>81</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>-0.108834</td>\n",
       "      <td>-0.099573</td>\n",
       "      <td>-0.085189</td>\n",
       "      <td>-0.067508</td>\n",
       "      <td>-0.048111</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>-0.008312</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>0.029816</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067566</td>\n",
       "      <td>-0.058927</td>\n",
       "      <td>-0.048947</td>\n",
       "      <td>-0.037186</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>68</td>\n",
       "      <td>69.99</td>\n",
       "      <td>84</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>0.026051</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>0.023384</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>-0.003093</td>\n",
       "      <td>-0.010552</td>\n",
       "      <td>-0.016825</td>\n",
       "      <td>-0.021344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>93</td>\n",
       "      <td>96.10</td>\n",
       "      <td>81</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5180 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.022165  0.008058 -0.005694 -0.018277 -0.028401 -0.034445 -0.034832   \n",
       "1     0.002396 -0.001148 -0.004544 -0.007492 -0.009561 -0.010251 -0.009101   \n",
       "2    -0.007708 -0.005857 -0.004098 -0.002622 -0.001656 -0.001300 -0.001325   \n",
       "3    -0.010544 -0.005010  0.000461  0.005682  0.010287  0.013727  0.015341   \n",
       "4    -0.007411 -0.007268 -0.007012 -0.006467 -0.005435 -0.003734 -0.001218   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5175  0.028062  0.013806 -0.000827 -0.014660 -0.026783 -0.036512 -0.043327   \n",
       "5176  0.054225  0.071998  0.084893  0.091340  0.090327  0.081546  0.065480   \n",
       "5177  0.045568  0.033181  0.017340 -0.000325 -0.017933 -0.033597 -0.045684   \n",
       "5178 -0.108834 -0.099573 -0.085189 -0.067508 -0.048111 -0.028148 -0.008312   \n",
       "5179  0.026051  0.025899  0.023384  0.018722  0.012327  0.004804 -0.003093   \n",
       "\n",
       "             7         8         9  ...        54        55        56  \\\n",
       "0    -0.028548 -0.015653  0.002471  ...  0.006408 -0.014542 -0.033742   \n",
       "1    -0.005841 -0.000541  0.006277  ...  0.023915  0.031500  0.035881   \n",
       "2    -0.001099  0.000269  0.003553  ... -0.055547 -0.063149 -0.066136   \n",
       "3     0.014503  0.010822  0.004327  ... -0.027160 -0.043717 -0.057046   \n",
       "4     0.002173  0.006360  0.011074  ... -0.023944 -0.009014  0.006739   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5175 -0.046852 -0.046867 -0.043359  ... -0.031265 -0.024912 -0.017063   \n",
       "5176  0.043417  0.017377 -0.010103  ...  0.020310  0.008572 -0.002583   \n",
       "5177 -0.052995 -0.054832 -0.050976  ...  0.068069  0.077176  0.078598   \n",
       "5178  0.011064  0.029816  0.047741  ... -0.067566 -0.058927 -0.048947   \n",
       "5179 -0.010552 -0.016825 -0.021344  ...  0.005536  0.016443  0.026390   \n",
       "\n",
       "            57        58        59  hr  calculated_hr  DBP  SBP  \n",
       "0    -0.049071 -0.058827 -0.061907  70          99.23   75  111  \n",
       "1     0.036621  0.033664  0.027309  85          86.31   86  133  \n",
       "2    -0.062301 -0.050670 -0.031967  59          57.83   87  139  \n",
       "3    -0.066155 -0.070509 -0.070022  86          68.22   80  146  \n",
       "4     0.022151  0.036009  0.047166  85          72.06   80  120  \n",
       "...        ...       ...       ...  ..            ...  ...  ...  \n",
       "5175 -0.008171  0.001152  0.010138  82          82.40   79  133  \n",
       "5176 -0.012376 -0.020267 -0.026010  86          77.07   80  146  \n",
       "5177  0.072051  0.058184  0.038529  93          90.76   81  118  \n",
       "5178 -0.037186 -0.023145 -0.006584  68          69.99   84  148  \n",
       "5179  0.034048  0.038254  0.038254  93          96.10   81  118  \n",
       "\n",
       "[5180 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df , ratio = load_data(prepros)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ede8eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calculated_hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>82.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>77.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5177</th>\n",
       "      <td>90.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>96.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5180 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      calculated_hr\n",
       "0             99.23\n",
       "1             86.31\n",
       "2             57.83\n",
       "3             68.22\n",
       "4             72.06\n",
       "...             ...\n",
       "5175          82.40\n",
       "5176          77.07\n",
       "5177          90.76\n",
       "5178          69.99\n",
       "5179          96.10\n",
       "\n",
       "[5180 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5180, 60)\n",
      "(5180, 1)\n"
     ]
    }
   ],
   "source": [
    "df_input = df.drop(['DBP','SBP','hr','calculated_hr'], axis=1)\n",
    "df_target = df[[target]]\n",
    "\n",
    "display(df_target)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df_target = scaler.fit_transform(df_target)\n",
    "print(df_input.shape)\n",
    "print(scaled_df_target.shape)\n",
    "\n",
    "if scaled == True:\n",
    "    df_target = scaled_df_target\n",
    "        \n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_train_test(df_input ,df_target ,ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efece8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 04:35:02.476152: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-22 04:35:02.476192: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (atamach01): /proc/driver/nvidia/version does not exist\n",
      "2023-06-22 04:35:02.478355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 60, 240)          173760    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 120)              144480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 121       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/180\n",
      "78/78 [==============================] - 16s 133ms/step - loss: 4464.1895 - mse: 4464.1895 - mae: 65.2981 - val_loss: 3885.6392 - val_mse: 3885.6392 - val_mae: 61.0113\n",
      "Epoch 2/180\n",
      "78/78 [==============================] - 12s 157ms/step - loss: 3622.9380 - mse: 3622.9380 - mae: 58.6656 - val_loss: 3330.5754 - val_mse: 3330.5754 - val_mae: 56.2788\n",
      "Epoch 3/180\n",
      "55/78 [====================>.........] - ETA: 3s - loss: 3219.5833 - mse: 3219.5833 - mae: 55.0192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_64483/776062557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStoppingIncreasingValLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincrease_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCH\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       (graph_function,\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lr_list = [5e-4]\n",
    "loss_list = ['mse']\n",
    "\n",
    "for loss in loss_list:\n",
    "    for lr in lr_list:\n",
    "#         optimizer_list = [RMSprop(learning_rate=lr) ,Adam(learning_rate=lr) ,SGD(learning_rate=lr) ,Adagrad(learning_rate=lr)]\n",
    "        optimizer_list = [RMSprop(learning_rate=lr)]\n",
    "\n",
    "        for optimizer in optimizer_list:\n",
    "            \n",
    "            models = Models()\n",
    "            model = models.import_model(model_name)\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss=loss, optimizer=optimizer, metrics=['mse','mae'])\n",
    "            \n",
    "            early_stopping = EarlyStoppingIncreasingValLoss(patience=2, delta=0, check_interval=20, increase_threshold=0.002)\n",
    "            history = model.fit(X_train, y_train, validation_split=0.2, epochs=NUM_EPOCH ,callbacks=[early_stopping])\n",
    "\n",
    "            optimizer_name = optimizer.get_config()['name']\n",
    "\n",
    "            result_name = f'{model_name}_lr:{lr}_NUM_EPOCH:{NUM_EPOCH}_optimizer:{optimizer_name}_loss:{loss}_scaled:{scaled}_prepros:{prepros}_target:{target}'\n",
    "            folder_path = f'./hr_optimization_results/{result_name}'\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "            history_dict = history.history\n",
    "            with open(f'{folder_path}/history.json', 'w') as f:\n",
    "                json.dump(history_dict, f)\n",
    "\n",
    "            results = model.evaluate(X_test, y_test)\n",
    "            save_model(model, f\"{folder_path}/model.h5\")\n",
    "\n",
    "            y_predicted = model.predict(X_test)\n",
    "\n",
    "            if scaled == True:\n",
    "                y_predicted_temp = scaler.inverse_transform(y_predicted)\n",
    "                y_test_temp = scaler.inverse_transform(y_test)\n",
    "            else:\n",
    "                y_predicted_temp = y_predicted\n",
    "                y_test_temp = y_test\n",
    "\n",
    "\n",
    "            y_predicted_df = pd.DataFrame(y_predicted_temp)\n",
    "            y_test_df = pd.DataFrame(y_test_temp)\n",
    "\n",
    "            plot_scatter_hr(y_predicted_df,y_test_df ,folder_path)\n",
    "\n",
    "            mae , mse = calculate_loss_hr(y_predicted_df,y_test_df)\n",
    "\n",
    "            plot_loss(history_dict ,folder_path)\n",
    "\n",
    "            out_dict = {\n",
    "                    'model_name':[model_name],\n",
    "                    'lr':[lr],\n",
    "                    'epochs':[NUM_EPOCH],\n",
    "                    'optimizer':[optimizer_name],\n",
    "                    'loss':[loss],\n",
    "                    'results':[results],\n",
    "                    'mae':[mae],\n",
    "                    'mse':[mse],\n",
    "                    'scaled':[scaled],\n",
    "                    'prepros':[prepros], \n",
    "                    'target':[target],\n",
    "                    'time':[datetime.datetime.now()]\n",
    "\n",
    "                    }\n",
    "\n",
    "            save_data(out_dict, 'sheets/hr_model_optimizing_cal_hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b3241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HR",
   "language": "python",
   "name": "hr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

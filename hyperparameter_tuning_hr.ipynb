{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77920bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 10:15:44.577440: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 10:15:44.916217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:\n",
      "2023-06-14 10:15:44.916249: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-14 10:15:44.945844: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-14 10:15:46.152324: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:\n",
      "2023-06-14 10:15:46.152418: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.1/lib64:\n",
      "2023-06-14 10:15:46.152426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jayasanka/Projects/nervotech test/HR_estimator_redesign/Data_preprocessing.py:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  pd.np.random.seed(random_seed)\n",
      "/home/jayasanka/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    }
   ],
   "source": [
    "from models import Models\n",
    "from Data_preprocessing import split_60 ,split_train_val_test ,load_data\n",
    "from utils import save_data, split_train_test ,plot_scatter_hr ,plot_loss ,calculate_loss_hr\n",
    "from custom_callbacks import EarlyStoppingIncreasingValLoss\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "from keras.optimizers import RMSprop ,Adam ,SGD ,Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58945a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model2_hr'\n",
    "NUM_EPOCH = 300\n",
    "scaled = False\n",
    "\n",
    "# lr = 5e-4\n",
    "# optimizer = RMSprop(learning_rate=lr)\n",
    "# loss = \"mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5462fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5439, 1813, 1813)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>hr</th>\n",
       "      <th>calculated_hr</th>\n",
       "      <th>DBP</th>\n",
       "      <th>SBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022165</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>-0.005694</td>\n",
       "      <td>-0.018277</td>\n",
       "      <td>-0.028401</td>\n",
       "      <td>-0.034445</td>\n",
       "      <td>-0.034832</td>\n",
       "      <td>-0.028548</td>\n",
       "      <td>-0.015653</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006408</td>\n",
       "      <td>-0.014542</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>-0.049071</td>\n",
       "      <td>-0.058827</td>\n",
       "      <td>-0.061907</td>\n",
       "      <td>70</td>\n",
       "      <td>99.23</td>\n",
       "      <td>75</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002396</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>-0.009561</td>\n",
       "      <td>-0.010251</td>\n",
       "      <td>-0.009101</td>\n",
       "      <td>-0.005841</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023915</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.027309</td>\n",
       "      <td>85</td>\n",
       "      <td>86.31</td>\n",
       "      <td>86</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-0.004098</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>-0.001656</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055547</td>\n",
       "      <td>-0.063149</td>\n",
       "      <td>-0.066136</td>\n",
       "      <td>-0.062301</td>\n",
       "      <td>-0.050670</td>\n",
       "      <td>-0.031967</td>\n",
       "      <td>59</td>\n",
       "      <td>57.83</td>\n",
       "      <td>87</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010544</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027160</td>\n",
       "      <td>-0.043717</td>\n",
       "      <td>-0.057046</td>\n",
       "      <td>-0.066155</td>\n",
       "      <td>-0.070509</td>\n",
       "      <td>-0.070022</td>\n",
       "      <td>86</td>\n",
       "      <td>68.22</td>\n",
       "      <td>80</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.007411</td>\n",
       "      <td>-0.007268</td>\n",
       "      <td>-0.007012</td>\n",
       "      <td>-0.006467</td>\n",
       "      <td>-0.005435</td>\n",
       "      <td>-0.003734</td>\n",
       "      <td>-0.001218</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023944</td>\n",
       "      <td>-0.009014</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.022151</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.047166</td>\n",
       "      <td>85</td>\n",
       "      <td>72.06</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9060</th>\n",
       "      <td>0.028062</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>-0.014660</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>-0.036512</td>\n",
       "      <td>-0.043327</td>\n",
       "      <td>-0.046852</td>\n",
       "      <td>-0.046867</td>\n",
       "      <td>-0.043359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031265</td>\n",
       "      <td>-0.024912</td>\n",
       "      <td>-0.017063</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>82</td>\n",
       "      <td>82.40</td>\n",
       "      <td>79</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>0.054225</td>\n",
       "      <td>0.071998</td>\n",
       "      <td>0.084893</td>\n",
       "      <td>0.091340</td>\n",
       "      <td>0.090327</td>\n",
       "      <td>0.081546</td>\n",
       "      <td>0.065480</td>\n",
       "      <td>0.043417</td>\n",
       "      <td>0.017377</td>\n",
       "      <td>-0.010103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>-0.002583</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.020267</td>\n",
       "      <td>-0.026010</td>\n",
       "      <td>86</td>\n",
       "      <td>77.07</td>\n",
       "      <td>80</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.033181</td>\n",
       "      <td>0.017340</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>-0.045684</td>\n",
       "      <td>-0.052995</td>\n",
       "      <td>-0.054832</td>\n",
       "      <td>-0.050976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068069</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>0.078598</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>0.058184</td>\n",
       "      <td>0.038529</td>\n",
       "      <td>93</td>\n",
       "      <td>90.76</td>\n",
       "      <td>81</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9063</th>\n",
       "      <td>-0.108834</td>\n",
       "      <td>-0.099573</td>\n",
       "      <td>-0.085189</td>\n",
       "      <td>-0.067508</td>\n",
       "      <td>-0.048111</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>-0.008312</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>0.029816</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067566</td>\n",
       "      <td>-0.058927</td>\n",
       "      <td>-0.048947</td>\n",
       "      <td>-0.037186</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>68</td>\n",
       "      <td>69.99</td>\n",
       "      <td>84</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9064</th>\n",
       "      <td>0.026051</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>0.023384</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>-0.003093</td>\n",
       "      <td>-0.010552</td>\n",
       "      <td>-0.016825</td>\n",
       "      <td>-0.021344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>93</td>\n",
       "      <td>96.10</td>\n",
       "      <td>81</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9065 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.022165  0.008058 -0.005694 -0.018277 -0.028401 -0.034445 -0.034832   \n",
       "1     0.002396 -0.001148 -0.004544 -0.007492 -0.009561 -0.010251 -0.009101   \n",
       "2    -0.007708 -0.005857 -0.004098 -0.002622 -0.001656 -0.001300 -0.001325   \n",
       "3    -0.010544 -0.005010  0.000461  0.005682  0.010287  0.013727  0.015341   \n",
       "4    -0.007411 -0.007268 -0.007012 -0.006467 -0.005435 -0.003734 -0.001218   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9060  0.028062  0.013806 -0.000827 -0.014660 -0.026783 -0.036512 -0.043327   \n",
       "9061  0.054225  0.071998  0.084893  0.091340  0.090327  0.081546  0.065480   \n",
       "9062  0.045568  0.033181  0.017340 -0.000325 -0.017933 -0.033597 -0.045684   \n",
       "9063 -0.108834 -0.099573 -0.085189 -0.067508 -0.048111 -0.028148 -0.008312   \n",
       "9064  0.026051  0.025899  0.023384  0.018722  0.012327  0.004804 -0.003093   \n",
       "\n",
       "             7         8         9  ...        54        55        56  \\\n",
       "0    -0.028548 -0.015653  0.002471  ...  0.006408 -0.014542 -0.033742   \n",
       "1    -0.005841 -0.000541  0.006277  ...  0.023915  0.031500  0.035881   \n",
       "2    -0.001099  0.000269  0.003553  ... -0.055547 -0.063149 -0.066136   \n",
       "3     0.014503  0.010822  0.004327  ... -0.027160 -0.043717 -0.057046   \n",
       "4     0.002173  0.006360  0.011074  ... -0.023944 -0.009014  0.006739   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9060 -0.046852 -0.046867 -0.043359  ... -0.031265 -0.024912 -0.017063   \n",
       "9061  0.043417  0.017377 -0.010103  ...  0.020310  0.008572 -0.002583   \n",
       "9062 -0.052995 -0.054832 -0.050976  ...  0.068069  0.077176  0.078598   \n",
       "9063  0.011064  0.029816  0.047741  ... -0.067566 -0.058927 -0.048947   \n",
       "9064 -0.010552 -0.016825 -0.021344  ...  0.005536  0.016443  0.026390   \n",
       "\n",
       "            57        58        59  hr  calculated_hr  DBP  SBP  \n",
       "0    -0.049071 -0.058827 -0.061907  70          99.23   75  111  \n",
       "1     0.036621  0.033664  0.027309  85          86.31   86  133  \n",
       "2    -0.062301 -0.050670 -0.031967  59          57.83   87  139  \n",
       "3    -0.066155 -0.070509 -0.070022  86          68.22   80  146  \n",
       "4     0.022151  0.036009  0.047166  85          72.06   80  120  \n",
       "...        ...       ...       ...  ..            ...  ...  ...  \n",
       "9060 -0.008171  0.001152  0.010138  82          82.40   79  133  \n",
       "9061 -0.012376 -0.020267 -0.026010  86          77.07   80  146  \n",
       "9062  0.072051  0.058184  0.038529  93          90.76   81  118  \n",
       "9063 -0.037186 -0.023145 -0.006584  68          69.99   84  148  \n",
       "9064  0.034048  0.038254  0.038254  93          96.10   81  118  \n",
       "\n",
       "[9065 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df , ratio = load_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ede8eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9060</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9063</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9064</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9065 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hr\n",
       "0     70\n",
       "1     85\n",
       "2     59\n",
       "3     86\n",
       "4     85\n",
       "...   ..\n",
       "9060  82\n",
       "9061  86\n",
       "9062  93\n",
       "9063  68\n",
       "9064  93\n",
       "\n",
       "[9065 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9065, 60)\n",
      "(9065, 1)\n"
     ]
    }
   ],
   "source": [
    "df_input = df.drop(['DBP','SBP','hr','calculated_hr'], axis=1)\n",
    "df_target = df[['hr']]\n",
    "\n",
    "display(df_target)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df_target = scaler.fit_transform(df_target)\n",
    "print(df_input.shape)\n",
    "print(scaled_df_target.shape)\n",
    "\n",
    "if scaled == True:\n",
    "    df_target = scaled_df_target\n",
    "        \n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_train_test(df_input ,df_target ,ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efece8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 10:15:48.982887: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-14 10:15:48.982910: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (atamach01): /proc/driver/nvidia/version does not exist\n",
      "2023-06-14 10:15:48.983366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 60, 240)          173760    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 120)              144480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 121       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 31s 178ms/step - loss: 3906.8774 - mse: 3906.8774 - mae: 61.3402 - val_loss: 3268.9084 - val_mse: 3268.9084 - val_mae: 56.0913\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 24s 173ms/step - loss: 2841.6624 - mse: 2841.6624 - mae: 52.0707 - val_loss: 2420.8738 - val_mse: 2420.8738 - val_mae: 47.9395\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 2060.2773 - mse: 2060.2773 - mae: 43.9394 - val_loss: 1707.8337 - val_mse: 1707.8337 - val_mae: 39.8140\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 29s 212ms/step - loss: 1412.0135 - mse: 1412.0135 - mae: 35.8319 - val_loss: 1128.4813 - val_mse: 1128.4813 - val_mae: 31.7144\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 901.0154 - mse: 901.0154 - mae: 27.7602 - val_loss: 683.3305 - val_mse: 683.3305 - val_mae: 23.7085\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 520.7070 - mse: 520.7070 - mae: 20.1129 - val_loss: 370.4699 - val_mse: 370.4699 - val_mae: 16.6489\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 270.1450 - mse: 270.1450 - mae: 13.9414 - val_loss: 185.2874 - val_mse: 185.2874 - val_mae: 11.4030\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 148.5578 - mse: 148.5578 - mae: 10.0683 - val_loss: 124.7448 - val_mse: 124.7448 - val_mae: 9.2285\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.8559 - mse: 124.8559 - mae: 9.1658 - val_loss: 122.7367 - val_mse: 122.7367 - val_mae: 9.1117\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4760 - mse: 124.4760 - mae: 9.1410 - val_loss: 122.7392 - val_mse: 122.7392 - val_mae: 9.0919\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 124.4855 - mse: 124.4855 - mae: 9.1394 - val_loss: 122.7043 - val_mse: 122.7043 - val_mae: 9.1043\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.6871 - mse: 124.6871 - mae: 9.1559 - val_loss: 122.7202 - val_mse: 122.7202 - val_mae: 9.0922\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 124.8598 - mse: 124.8598 - mae: 9.1522 - val_loss: 122.8736 - val_mse: 122.8736 - val_mae: 9.0901\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.7136 - mse: 124.7136 - mae: 9.1538 - val_loss: 122.7496 - val_mse: 122.7496 - val_mae: 9.0917\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.5703 - mse: 124.5703 - mae: 9.1412 - val_loss: 122.6844 - val_mse: 122.6844 - val_mae: 9.0969\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.5364 - mse: 124.5364 - mae: 9.1440 - val_loss: 122.7976 - val_mse: 122.7976 - val_mae: 9.0910\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5468 - mse: 124.5468 - mae: 9.1433 - val_loss: 122.7032 - val_mse: 122.7032 - val_mae: 9.0927\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 124.8469 - mse: 124.8469 - mae: 9.1634 - val_loss: 122.7569 - val_mse: 122.7569 - val_mae: 9.0915\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.5627 - mse: 124.5627 - mae: 9.1393 - val_loss: 122.6993 - val_mse: 122.6993 - val_mae: 9.1029\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 124.5683 - mse: 124.5683 - mae: 9.1446 - val_loss: 122.7352 - val_mse: 122.7352 - val_mae: 9.0919\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.5126 - mse: 124.5126 - mae: 9.1363 - val_loss: 122.7008 - val_mse: 122.7008 - val_mae: 9.1033\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 124.5599 - mse: 124.5599 - mae: 9.1412 - val_loss: 122.7179 - val_mse: 122.7179 - val_mae: 9.0923\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 124.5279 - mse: 124.5279 - mae: 9.1382 - val_loss: 122.7082 - val_mse: 122.7082 - val_mae: 9.1054\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.5337 - mse: 124.5337 - mae: 9.1422 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0945\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.5194 - mse: 124.5194 - mae: 9.1386 - val_loss: 122.6965 - val_mse: 122.6965 - val_mae: 9.0929\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.5229 - mse: 124.5229 - mae: 9.1333 - val_loss: 122.7317 - val_mse: 122.7317 - val_mae: 9.1107\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.8273 - mse: 124.8273 - mae: 9.1600 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8057 - mse: 124.8057 - mae: 9.1622 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6481 - mse: 124.6481 - mae: 9.1504 - val_loss: 124.5205 - val_mse: 124.5205 - val_mae: 9.2196\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.6551 - mse: 124.6551 - mae: 9.1475 - val_loss: 122.7452 - val_mse: 122.7452 - val_mae: 9.1132\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5313 - mse: 124.5313 - mae: 9.1481 - val_loss: 122.7011 - val_mse: 122.7011 - val_mae: 9.0927\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.8388 - mse: 124.8388 - mae: 9.1548 - val_loss: 122.7015 - val_mse: 122.7015 - val_mae: 9.0927\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4972 - mse: 124.4972 - mae: 9.1401 - val_loss: 123.2272 - val_mse: 123.2272 - val_mae: 9.1575\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.8655 - mse: 124.8655 - mae: 9.1579 - val_loss: 122.7367 - val_mse: 122.7367 - val_mae: 9.1116\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5803 - mse: 124.5803 - mae: 9.1489 - val_loss: 122.7721 - val_mse: 122.7721 - val_mae: 9.0913\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6221 - mse: 124.6221 - mae: 9.1502 - val_loss: 122.7656 - val_mse: 122.7656 - val_mae: 9.0914\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.9111 - mse: 124.9111 - mae: 9.1517 - val_loss: 122.7188 - val_mse: 122.7188 - val_mae: 9.1080\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5188 - mse: 124.5188 - mae: 9.1458 - val_loss: 122.8575 - val_mse: 122.8575 - val_mae: 9.0902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4906 - mse: 124.4906 - mae: 9.1354 - val_loss: 122.7420 - val_mse: 122.7420 - val_mae: 9.1126\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.6212 - mse: 124.6212 - mae: 9.1543 - val_loss: 122.7225 - val_mse: 122.7225 - val_mae: 9.0922\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.8877 - mse: 124.8877 - mae: 9.1407 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0939\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 124.5598 - mse: 124.5598 - mae: 9.1421 - val_loss: 122.8180 - val_mse: 122.8180 - val_mae: 9.1237\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7804 - mse: 124.7804 - mae: 9.1681 - val_loss: 122.6903 - val_mse: 122.6903 - val_mae: 9.0979\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6233 - mse: 124.6233 - mae: 9.1517 - val_loss: 122.7052 - val_mse: 122.7052 - val_mae: 9.0927\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5925 - mse: 124.5925 - mae: 9.1400 - val_loss: 122.7037 - val_mse: 122.7037 - val_mae: 9.0927\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 125.1116 - mse: 125.1116 - mae: 9.1660 - val_loss: 122.7069 - val_mse: 122.7069 - val_mae: 9.1046\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4987 - mse: 124.4987 - mae: 9.1491 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0935\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7493 - mse: 124.7493 - mae: 9.1512 - val_loss: 122.7774 - val_mse: 122.7774 - val_mae: 9.1177\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8653 - mse: 124.8653 - mae: 9.1625 - val_loss: 124.5690 - val_mse: 124.5690 - val_mae: 9.0942\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 125.5802 - mse: 125.5802 - mae: 9.1890 - val_loss: 122.7737 - val_mse: 122.7737 - val_mae: 9.0912\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.5921 - mse: 124.5921 - mae: 9.1387 - val_loss: 124.0009 - val_mse: 124.0009 - val_mae: 9.0899\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.9450 - mse: 124.9450 - mae: 9.1627 - val_loss: 123.1332 - val_mse: 123.1332 - val_mae: 9.0880\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.7100 - mse: 124.7100 - mae: 9.1481 - val_loss: 122.7389 - val_mse: 122.7389 - val_mae: 9.1118\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.5174 - mse: 124.5174 - mae: 9.1419 - val_loss: 123.5002 - val_mse: 123.5002 - val_mae: 9.0858\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.7704 - mse: 124.7704 - mae: 9.1540 - val_loss: 122.9513 - val_mse: 122.9513 - val_mae: 9.0896\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.8827 - mse: 124.8827 - mae: 9.1612 - val_loss: 122.9970 - val_mse: 122.9970 - val_mae: 9.1138\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.5540 - mse: 124.5540 - mae: 9.1372 - val_loss: 121.9785 - val_mse: 121.9785 - val_mae: 9.0871\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8759 - mse: 124.8759 - mae: 9.1548 - val_loss: 123.4789 - val_mse: 123.4789 - val_mae: 9.1673\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 124.7438 - mse: 124.7438 - mae: 9.1590 - val_loss: 121.9998 - val_mse: 121.9998 - val_mae: 9.0794\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 122.8366 - mse: 122.8366 - mae: 9.0797 - val_loss: 118.8001 - val_mse: 118.8001 - val_mae: 8.9315\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 123.3656 - mse: 123.3656 - mae: 9.1135 - val_loss: 118.3602 - val_mse: 118.3602 - val_mae: 8.9471\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 23s 172ms/step - loss: 122.4755 - mse: 122.4755 - mae: 9.0584 - val_loss: 119.6959 - val_mse: 119.6959 - val_mae: 8.9919\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 122.1791 - mse: 122.1791 - mae: 9.0273 - val_loss: 122.2670 - val_mse: 122.2670 - val_mae: 9.1030\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 121.7914 - mse: 121.7914 - mae: 9.0275 - val_loss: 115.4651 - val_mse: 115.4651 - val_mae: 8.7631\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 118.9366 - mse: 118.9366 - mae: 8.8842 - val_loss: 115.2351 - val_mse: 115.2351 - val_mae: 8.7415\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 118.6082 - mse: 118.6082 - mae: 8.8983 - val_loss: 115.4351 - val_mse: 115.4351 - val_mae: 8.6497\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 117.2461 - mse: 117.2461 - mae: 8.7892 - val_loss: 121.8423 - val_mse: 121.8423 - val_mae: 8.9002\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 115.9173 - mse: 115.9173 - mae: 8.7474 - val_loss: 111.2676 - val_mse: 111.2676 - val_mae: 8.4909\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 115.0560 - mse: 115.0560 - mae: 8.6905 - val_loss: 109.4164 - val_mse: 109.4164 - val_mae: 8.4599\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 114.9020 - mse: 114.9020 - mae: 8.7163 - val_loss: 106.9577 - val_mse: 106.9577 - val_mae: 8.2270\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 113.5011 - mse: 113.5011 - mae: 8.6265 - val_loss: 109.1534 - val_mse: 109.1534 - val_mae: 8.4697\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 113.2544 - mse: 113.2544 - mae: 8.6086 - val_loss: 107.6831 - val_mse: 107.6831 - val_mae: 8.3304\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 111.8549 - mse: 111.8549 - mae: 8.5449 - val_loss: 106.0131 - val_mse: 106.0131 - val_mae: 8.2807\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 112.2574 - mse: 112.2574 - mae: 8.5567 - val_loss: 105.6824 - val_mse: 105.6824 - val_mae: 8.2490\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 110.7436 - mse: 110.7436 - mae: 8.5089 - val_loss: 104.2770 - val_mse: 104.2770 - val_mae: 8.1417\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 110.9506 - mse: 110.9506 - mae: 8.5025 - val_loss: 105.6575 - val_mse: 105.6575 - val_mae: 8.2357\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 110.4365 - mse: 110.4365 - mae: 8.4780 - val_loss: 105.0353 - val_mse: 105.0353 - val_mae: 8.2290\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 109.5885 - mse: 109.5885 - mae: 8.4108 - val_loss: 107.0881 - val_mse: 107.0881 - val_mae: 8.2774\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 109.5834 - mse: 109.5834 - mae: 8.4094 - val_loss: 104.0519 - val_mse: 104.0519 - val_mae: 8.2147\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 108.8297 - mse: 108.8297 - mae: 8.3775 - val_loss: 108.7807 - val_mse: 108.7807 - val_mae: 8.3417\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 108.3183 - mse: 108.3183 - mae: 8.3689 - val_loss: 104.2366 - val_mse: 104.2366 - val_mae: 8.1003\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 108.4174 - mse: 108.4174 - mae: 8.3572 - val_loss: 104.0631 - val_mse: 104.0631 - val_mae: 8.1342\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 107.4592 - mse: 107.4592 - mae: 8.3234 - val_loss: 101.5496 - val_mse: 101.5496 - val_mae: 8.0624\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 107.3679 - mse: 107.3679 - mae: 8.3249 - val_loss: 103.9814 - val_mse: 103.9814 - val_mae: 8.2135\n",
      "Epoch 85/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 106.9599 - mse: 106.9599 - mae: 8.3216 - val_loss: 103.4756 - val_mse: 103.4756 - val_mae: 8.2039\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 105.4727 - mse: 105.4727 - mae: 8.2714 - val_loss: 103.5212 - val_mse: 103.5212 - val_mae: 8.0725\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 23s 165ms/step - loss: 105.8094 - mse: 105.8094 - mae: 8.2538 - val_loss: 100.7622 - val_mse: 100.7622 - val_mae: 7.9425\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 105.5449 - mse: 105.5449 - mae: 8.2664 - val_loss: 102.6401 - val_mse: 102.6401 - val_mae: 8.0946\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 106.0370 - mse: 106.0370 - mae: 8.2765 - val_loss: 100.1791 - val_mse: 100.1791 - val_mae: 8.0077\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 106.4009 - mse: 106.4009 - mae: 8.2980 - val_loss: 100.9199 - val_mse: 100.9199 - val_mae: 7.9580\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 104.7100 - mse: 104.7100 - mae: 8.2290 - val_loss: 100.6700 - val_mse: 100.6700 - val_mae: 8.0235\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 104.3943 - mse: 104.3943 - mae: 8.2077 - val_loss: 103.1319 - val_mse: 103.1319 - val_mae: 8.0669\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 103.7128 - mse: 103.7128 - mae: 8.1723 - val_loss: 102.2131 - val_mse: 102.2131 - val_mae: 8.1026\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 103.6404 - mse: 103.6404 - mae: 8.1605 - val_loss: 102.0355 - val_mse: 102.0355 - val_mae: 8.1700\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 102.9511 - mse: 102.9511 - mae: 8.1263 - val_loss: 100.3555 - val_mse: 100.3555 - val_mae: 8.0143\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 103.5173 - mse: 103.5173 - mae: 8.1470 - val_loss: 101.0627 - val_mse: 101.0627 - val_mae: 8.0011\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 101.7181 - mse: 101.7181 - mae: 8.0601 - val_loss: 98.5726 - val_mse: 98.5726 - val_mae: 7.8701\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 23s 173ms/step - loss: 101.4782 - mse: 101.4782 - mae: 8.0678 - val_loss: 99.7844 - val_mse: 99.7844 - val_mae: 7.9381\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 101.0672 - mse: 101.0672 - mae: 8.0482 - val_loss: 101.8413 - val_mse: 101.8413 - val_mae: 8.0544\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 24s 173ms/step - loss: 100.4296 - mse: 100.4296 - mae: 8.0362 - val_loss: 99.3387 - val_mse: 99.3387 - val_mae: 7.7977\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 100.1658 - mse: 100.1658 - mae: 7.9880 - val_loss: 101.5604 - val_mse: 101.5604 - val_mae: 8.1338\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 99.8705 - mse: 99.8705 - mae: 8.0189 - val_loss: 98.7899 - val_mse: 98.7899 - val_mae: 7.9607\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 98.9492 - mse: 98.9492 - mae: 7.9854 - val_loss: 96.6749 - val_mse: 96.6749 - val_mae: 7.9192\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 98.3905 - mse: 98.3905 - mae: 7.9490 - val_loss: 98.6416 - val_mse: 98.6416 - val_mae: 7.8846\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 98.9414 - mse: 98.9414 - mae: 7.9622 - val_loss: 98.2265 - val_mse: 98.2265 - val_mae: 7.9060\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 99.3787 - mse: 99.3787 - mae: 7.9697 - val_loss: 99.5075 - val_mse: 99.5075 - val_mae: 8.0168\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 98.7516 - mse: 98.7516 - mae: 7.9617 - val_loss: 99.0917 - val_mse: 99.0917 - val_mae: 7.9300\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 98.3749 - mse: 98.3749 - mae: 7.9403 - val_loss: 96.6878 - val_mse: 96.6878 - val_mae: 7.8567\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 98.4863 - mse: 98.4863 - mae: 7.9702 - val_loss: 98.1197 - val_mse: 98.1197 - val_mae: 7.8947\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 98.4949 - mse: 98.4949 - mae: 7.9570 - val_loss: 98.5286 - val_mse: 98.5286 - val_mae: 7.8581\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 97.7054 - mse: 97.7054 - mae: 7.9186 - val_loss: 98.0461 - val_mse: 98.0461 - val_mae: 7.8801\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 98.5957 - mse: 98.5957 - mae: 7.9363 - val_loss: 97.2490 - val_mse: 97.2490 - val_mae: 7.8737\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 97.9776 - mse: 97.9776 - mae: 7.9261 - val_loss: 101.2877 - val_mse: 101.2877 - val_mae: 8.0512\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 97.4981 - mse: 97.4981 - mae: 7.9055 - val_loss: 95.9340 - val_mse: 95.9340 - val_mae: 7.9071\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 96.8839 - mse: 96.8839 - mae: 7.8897 - val_loss: 97.7902 - val_mse: 97.7902 - val_mae: 7.8965\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 96.3415 - mse: 96.3415 - mae: 7.8555 - val_loss: 98.8261 - val_mse: 98.8261 - val_mae: 7.8968\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 97.0394 - mse: 97.0394 - mae: 7.8960 - val_loss: 99.5115 - val_mse: 99.5115 - val_mae: 8.0423\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 97.2665 - mse: 97.2665 - mae: 7.8866 - val_loss: 99.0327 - val_mse: 99.0327 - val_mae: 7.8868\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 96.1549 - mse: 96.1549 - mae: 7.8513 - val_loss: 100.3081 - val_mse: 100.3081 - val_mae: 7.9345\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 96.4108 - mse: 96.4108 - mae: 7.8935 - val_loss: 96.8008 - val_mse: 96.8008 - val_mae: 7.8306\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 96.2381 - mse: 96.2381 - mae: 7.8677 - val_loss: 97.6898 - val_mse: 97.6898 - val_mae: 7.9733\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 96.9072 - mse: 96.9072 - mae: 7.8931 - val_loss: 98.5100 - val_mse: 98.5100 - val_mae: 7.8942\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 95.8429 - mse: 95.8429 - mae: 7.8135 - val_loss: 97.5755 - val_mse: 97.5755 - val_mae: 7.9411\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 95.1464 - mse: 95.1464 - mae: 7.7999 - val_loss: 103.3109 - val_mse: 103.3109 - val_mae: 8.1071\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 95.3198 - mse: 95.3198 - mae: 7.8065 - val_loss: 98.4821 - val_mse: 98.4821 - val_mae: 7.7681\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 94.4516 - mse: 94.4516 - mae: 7.7876 - val_loss: 96.9788 - val_mse: 96.9788 - val_mae: 7.8520\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 95.1329 - mse: 95.1329 - mae: 7.7863 - val_loss: 99.4346 - val_mse: 99.4346 - val_mae: 7.8708\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 95.6912 - mse: 95.6912 - mae: 7.8237 - val_loss: 97.6614 - val_mse: 97.6614 - val_mae: 7.8117\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 95.1899 - mse: 95.1899 - mae: 7.7969 - val_loss: 97.7622 - val_mse: 97.7622 - val_mae: 7.8476\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 94.6087 - mse: 94.6087 - mae: 7.7628 - val_loss: 98.5991 - val_mse: 98.5991 - val_mae: 7.9613\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 95.2054 - mse: 95.2054 - mae: 7.8070 - val_loss: 98.4791 - val_mse: 98.4791 - val_mae: 8.0532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 93.8862 - mse: 93.8862 - mae: 7.7457 - val_loss: 101.2962 - val_mse: 101.2962 - val_mae: 7.9912\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 94.3526 - mse: 94.3526 - mae: 7.7880 - val_loss: 97.7970 - val_mse: 97.7970 - val_mae: 7.8935\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 93.8089 - mse: 93.8089 - mae: 7.7355 - val_loss: 99.0996 - val_mse: 99.0996 - val_mae: 7.9850\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 94.2005 - mse: 94.2005 - mae: 7.7547 - val_loss: 99.4994 - val_mse: 99.4994 - val_mae: 7.8287\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 93.9481 - mse: 93.9481 - mae: 7.7454 - val_loss: 98.8433 - val_mse: 98.8433 - val_mae: 7.9241\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 93.9329 - mse: 93.9329 - mae: 7.7427 - val_loss: 97.9373 - val_mse: 97.9373 - val_mae: 7.8240\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 93.4703 - mse: 93.4703 - mae: 7.7335 - val_loss: 101.2222 - val_mse: 101.2222 - val_mae: 7.9453\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 92.6158 - mse: 92.6158 - mae: 7.6766 - val_loss: 97.4294 - val_mse: 97.4294 - val_mae: 7.8242\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 93.0685 - mse: 93.0685 - mae: 7.6878 - val_loss: 96.9157 - val_mse: 96.9157 - val_mae: 7.8162\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 94.0333 - mse: 94.0333 - mae: 7.7227 - val_loss: 97.3038 - val_mse: 97.3038 - val_mae: 7.9078\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 91.7534 - mse: 91.7534 - mae: 7.6562 - val_loss: 98.6726 - val_mse: 98.6726 - val_mae: 7.7919\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 92.6853 - mse: 92.6853 - mae: 7.6923 - val_loss: 97.0171 - val_mse: 97.0171 - val_mae: 7.7368\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 92.3663 - mse: 92.3663 - mae: 7.6486 - val_loss: 99.6903 - val_mse: 99.6903 - val_mae: 7.9568\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 91.3575 - mse: 91.3575 - mae: 7.6335 - val_loss: 98.4065 - val_mse: 98.4065 - val_mae: 7.9092\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 92.6786 - mse: 92.6786 - mae: 7.6527 - val_loss: 97.3359 - val_mse: 97.3359 - val_mae: 7.7518\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 91.2302 - mse: 91.2302 - mae: 7.6420 - val_loss: 99.9235 - val_mse: 99.9235 - val_mae: 7.7678\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 91.9162 - mse: 91.9162 - mae: 7.6204 - val_loss: 97.6793 - val_mse: 97.6793 - val_mae: 7.8544\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 90.4265 - mse: 90.4265 - mae: 7.5651 - val_loss: 97.6504 - val_mse: 97.6504 - val_mae: 7.8959\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 90.8988 - mse: 90.8988 - mae: 7.6196 - val_loss: 99.9842 - val_mse: 99.9842 - val_mae: 7.8943\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 90.7366 - mse: 90.7366 - mae: 7.5824 - val_loss: 102.4352 - val_mse: 102.4352 - val_mae: 7.9765\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 89.9022 - mse: 89.9022 - mae: 7.5492 - val_loss: 99.1403 - val_mse: 99.1403 - val_mae: 7.8910\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 90.6852 - mse: 90.6852 - mae: 7.5904 - val_loss: 96.9559 - val_mse: 96.9559 - val_mae: 7.7691\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 89.5751 - mse: 89.5751 - mae: 7.5279 - val_loss: 98.6436 - val_mse: 98.6436 - val_mae: 7.9105\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 89.5774 - mse: 89.5774 - mae: 7.5485 - val_loss: 102.9041 - val_mse: 102.9041 - val_mae: 7.9794\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 88.7961 - mse: 88.7961 - mae: 7.5124 - val_loss: 98.1447 - val_mse: 98.1447 - val_mae: 7.8724\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 88.8358 - mse: 88.8358 - mae: 7.4911 - val_loss: 102.4618 - val_mse: 102.4618 - val_mae: 7.9055\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 88.8944 - mse: 88.8944 - mae: 7.5005 - val_loss: 101.5493 - val_mse: 101.5493 - val_mae: 7.8917\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 91.0514 - mse: 91.0514 - mae: 7.6077 - val_loss: 100.4514 - val_mse: 100.4514 - val_mae: 8.0083\n",
      "Epoch 160/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 90.5268 - mse: 90.5268 - mae: 7.5539 - val_loss: 99.3617 - val_mse: 99.3617 - val_mae: 7.8717\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 88.6115 - mse: 88.6115 - mae: 7.4950 - val_loss: 101.2373 - val_mse: 101.2373 - val_mae: 7.9381\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 89.4666 - mse: 89.4666 - mae: 7.5164 - val_loss: 100.9947 - val_mse: 100.9947 - val_mae: 7.8902\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 90.8450 - mse: 90.8450 - mae: 7.5888 - val_loss: 103.0313 - val_mse: 103.0313 - val_mae: 7.9592\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 89.8245 - mse: 89.8245 - mae: 7.5307 - val_loss: 100.4743 - val_mse: 100.4743 - val_mae: 7.9181\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 87.1055 - mse: 87.1055 - mae: 7.4452 - val_loss: 104.7656 - val_mse: 104.7656 - val_mae: 7.9561\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 86.9116 - mse: 86.9116 - mae: 7.4331 - val_loss: 99.3001 - val_mse: 99.3001 - val_mae: 7.8374\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 86.8495 - mse: 86.8495 - mae: 7.4095 - val_loss: 102.8711 - val_mse: 102.8711 - val_mae: 7.9473\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 86.0104 - mse: 86.0104 - mae: 7.3830 - val_loss: 101.6292 - val_mse: 101.6292 - val_mae: 7.9427\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 86.1685 - mse: 86.1685 - mae: 7.3773 - val_loss: 98.2358 - val_mse: 98.2358 - val_mae: 7.8011\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 86.7576 - mse: 86.7576 - mae: 7.3990 - val_loss: 101.0199 - val_mse: 101.0199 - val_mae: 7.8838\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 84.9567 - mse: 84.9567 - mae: 7.3437 - val_loss: 102.3505 - val_mse: 102.3505 - val_mae: 7.8450\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 84.9227 - mse: 84.9227 - mae: 7.3056 - val_loss: 99.3285 - val_mse: 99.3285 - val_mae: 7.9130\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 85.3950 - mse: 85.3950 - mae: 7.3144 - val_loss: 100.7592 - val_mse: 100.7592 - val_mae: 7.9641\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 84.7391 - mse: 84.7391 - mae: 7.3215 - val_loss: 102.9028 - val_mse: 102.9028 - val_mae: 7.9582\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 84.7585 - mse: 84.7585 - mae: 7.3077 - val_loss: 101.1252 - val_mse: 101.1252 - val_mae: 7.8979\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 83.1000 - mse: 83.1000 - mae: 7.2355 - val_loss: 104.2544 - val_mse: 104.2544 - val_mae: 7.9526\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 86.0416 - mse: 86.0416 - mae: 7.3320 - val_loss: 102.9280 - val_mse: 102.9280 - val_mae: 8.0325\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 83.7770 - mse: 83.7770 - mae: 7.2688 - val_loss: 105.5420 - val_mse: 105.5420 - val_mae: 7.9858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 85.1219 - mse: 85.1219 - mae: 7.2962 - val_loss: 102.1067 - val_mse: 102.1067 - val_mae: 7.9995\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 83.8967 - mse: 83.8967 - mae: 7.2513 - val_loss: 101.3513 - val_mse: 101.3513 - val_mae: 7.9195\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 83.2967 - mse: 83.2967 - mae: 7.2591 - val_loss: 104.0150 - val_mse: 104.0150 - val_mae: 7.8879\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 83.1720 - mse: 83.1720 - mae: 7.2166 - val_loss: 103.3238 - val_mse: 103.3238 - val_mae: 7.9457\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 83.4106 - mse: 83.4106 - mae: 7.2163 - val_loss: 101.6935 - val_mse: 101.6935 - val_mae: 7.8661\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 82.5379 - mse: 82.5379 - mae: 7.1896 - val_loss: 103.0460 - val_mse: 103.0460 - val_mae: 7.8807\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 82.3434 - mse: 82.3434 - mae: 7.1564 - val_loss: 102.1039 - val_mse: 102.1039 - val_mae: 7.9354\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 79.9699 - mse: 79.9699 - mae: 7.0757 - val_loss: 100.1642 - val_mse: 100.1642 - val_mae: 7.8728\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 79.9501 - mse: 79.9501 - mae: 7.0798 - val_loss: 103.1423 - val_mse: 103.1423 - val_mae: 7.9612\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 81.2736 - mse: 81.2736 - mae: 7.1427 - val_loss: 105.4473 - val_mse: 105.4473 - val_mae: 8.0930\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 81.3955 - mse: 81.3955 - mae: 7.1295 - val_loss: 102.2793 - val_mse: 102.2793 - val_mae: 8.0242\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 79.8536 - mse: 79.8536 - mae: 7.0501 - val_loss: 105.6727 - val_mse: 105.6727 - val_mae: 8.0450\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 80.7797 - mse: 80.7797 - mae: 7.0874 - val_loss: 103.0964 - val_mse: 103.0964 - val_mae: 7.9586\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 79.6388 - mse: 79.6388 - mae: 7.0691 - val_loss: 104.6051 - val_mse: 104.6051 - val_mae: 8.0302\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 79.6918 - mse: 79.6918 - mae: 7.0364 - val_loss: 105.9429 - val_mse: 105.9429 - val_mae: 8.0993\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 78.4337 - mse: 78.4337 - mae: 6.9898 - val_loss: 105.5269 - val_mse: 105.5269 - val_mae: 8.0503\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 76.8649 - mse: 76.8649 - mae: 6.9281 - val_loss: 108.2714 - val_mse: 108.2714 - val_mae: 8.0529\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 78.4279 - mse: 78.4279 - mae: 6.9626 - val_loss: 107.3146 - val_mse: 107.3146 - val_mae: 8.0554\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 79.3518 - mse: 79.3518 - mae: 7.0019 - val_loss: 106.0516 - val_mse: 106.0516 - val_mae: 8.0552\n",
      "Epoch 198/300\n",
      "136/136 [==============================] - 25s 188ms/step - loss: 76.7587 - mse: 76.7587 - mae: 6.9109 - val_loss: 108.0595 - val_mse: 108.0595 - val_mae: 8.0709\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 76.4926 - mse: 76.4926 - mae: 6.8921 - val_loss: 105.7873 - val_mse: 105.7873 - val_mae: 8.0405\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 76.1179 - mse: 76.1179 - mae: 6.8505 - val_loss: 104.9626 - val_mse: 104.9626 - val_mae: 8.0935\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 75.8966 - mse: 75.8966 - mae: 6.8531 - val_loss: 102.5685 - val_mse: 102.5685 - val_mae: 7.9397\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 74.1691 - mse: 74.1691 - mae: 6.7821 - val_loss: 109.5786 - val_mse: 109.5786 - val_mae: 8.0928\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 74.6023 - mse: 74.6023 - mae: 6.7792 - val_loss: 107.6665 - val_mse: 107.6665 - val_mae: 8.0445\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 74.8583 - mse: 74.8583 - mae: 6.8073 - val_loss: 108.1033 - val_mse: 108.1033 - val_mae: 8.0916\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 75.7954 - mse: 75.7954 - mae: 6.8322 - val_loss: 110.0003 - val_mse: 110.0003 - val_mae: 8.0910\n",
      "Epoch 206/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 74.1519 - mse: 74.1519 - mae: 6.7261 - val_loss: 114.7036 - val_mse: 114.7036 - val_mae: 8.3141\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 29s 216ms/step - loss: 72.3899 - mse: 72.3899 - mae: 6.6708 - val_loss: 107.7765 - val_mse: 107.7765 - val_mae: 8.1421\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 72.6481 - mse: 72.6481 - mae: 6.6882 - val_loss: 108.5516 - val_mse: 108.5516 - val_mae: 8.0397\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 71.8785 - mse: 71.8785 - mae: 6.6635 - val_loss: 111.9917 - val_mse: 111.9917 - val_mae: 8.1593\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 71.2379 - mse: 71.2379 - mae: 6.6532 - val_loss: 112.0479 - val_mse: 112.0479 - val_mae: 8.2082\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 72.7033 - mse: 72.7033 - mae: 6.7036 - val_loss: 107.8052 - val_mse: 107.8052 - val_mae: 8.1421\n",
      "Epoch 212/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 72.0628 - mse: 72.0628 - mae: 6.6325 - val_loss: 111.9909 - val_mse: 111.9909 - val_mae: 8.2390\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 71.9418 - mse: 71.9418 - mae: 6.6405 - val_loss: 107.2874 - val_mse: 107.2874 - val_mae: 8.0499\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 70.9024 - mse: 70.9024 - mae: 6.5900 - val_loss: 110.2451 - val_mse: 110.2451 - val_mae: 8.1851\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 69.0176 - mse: 69.0176 - mae: 6.5264 - val_loss: 107.5260 - val_mse: 107.5260 - val_mae: 8.1274\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 69.4910 - mse: 69.4910 - mae: 6.5369 - val_loss: 109.3346 - val_mse: 109.3346 - val_mae: 8.1101\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 69.0861 - mse: 69.0861 - mae: 6.5140 - val_loss: 108.1338 - val_mse: 108.1338 - val_mae: 8.1091\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 68.6499 - mse: 68.6499 - mae: 6.5073 - val_loss: 109.7592 - val_mse: 109.7592 - val_mae: 8.1813\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 70.1264 - mse: 70.1264 - mae: 6.5619 - val_loss: 110.9583 - val_mse: 110.9583 - val_mae: 8.1463\n",
      "Epoch 220/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 71.3749 - mse: 71.3749 - mae: 6.6092 - val_loss: 111.2442 - val_mse: 111.2442 - val_mae: 8.2332\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 69.6247 - mse: 69.6247 - mae: 6.5429 - val_loss: 112.1202 - val_mse: 112.1202 - val_mae: 8.2485\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 69.5600 - mse: 69.5600 - mae: 6.5814 - val_loss: 110.7825 - val_mse: 110.7825 - val_mae: 8.1765\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 68.0601 - mse: 68.0601 - mae: 6.4542 - val_loss: 111.8615 - val_mse: 111.8615 - val_mae: 8.1530\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 23s 165ms/step - loss: 71.0219 - mse: 71.0219 - mae: 6.5694 - val_loss: 111.0035 - val_mse: 111.0035 - val_mae: 8.1968\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 21s 155ms/step - loss: 68.8842 - mse: 68.8842 - mae: 6.5250 - val_loss: 112.2304 - val_mse: 112.2304 - val_mae: 8.1184\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 67.9045 - mse: 67.9045 - mae: 6.4286 - val_loss: 115.8269 - val_mse: 115.8269 - val_mae: 8.2742\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 66.3421 - mse: 66.3421 - mae: 6.3737 - val_loss: 114.3211 - val_mse: 114.3211 - val_mae: 8.2289\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 67.2277 - mse: 67.2277 - mae: 6.3573 - val_loss: 117.8512 - val_mse: 117.8512 - val_mae: 8.3076\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 65.1840 - mse: 65.1840 - mae: 6.2921 - val_loss: 113.2802 - val_mse: 113.2802 - val_mae: 8.2263\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 65.6233 - mse: 65.6233 - mae: 6.3057 - val_loss: 110.9388 - val_mse: 110.9388 - val_mae: 8.1503\n",
      "Epoch 231/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 64.5220 - mse: 64.5220 - mae: 6.2623 - val_loss: 112.0450 - val_mse: 112.0450 - val_mae: 8.1042\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 63.4731 - mse: 63.4731 - mae: 6.2050 - val_loss: 111.5862 - val_mse: 111.5862 - val_mae: 8.1387\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 64.1654 - mse: 64.1654 - mae: 6.2484 - val_loss: 112.0356 - val_mse: 112.0356 - val_mae: 8.1378\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 62.9101 - mse: 62.9101 - mae: 6.1741 - val_loss: 109.7809 - val_mse: 109.7809 - val_mae: 8.0665\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 62.9704 - mse: 62.9704 - mae: 6.1746 - val_loss: 112.7822 - val_mse: 112.7822 - val_mae: 8.2143\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 62.3618 - mse: 62.3618 - mae: 6.1292 - val_loss: 107.9373 - val_mse: 107.9373 - val_mae: 8.0563\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 61.7445 - mse: 61.7445 - mae: 6.1259 - val_loss: 116.3205 - val_mse: 116.3205 - val_mae: 8.1942\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 60.0171 - mse: 60.0171 - mae: 6.0024 - val_loss: 114.2566 - val_mse: 114.2566 - val_mae: 8.2029\n",
      "Epoch 239/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 60.1395 - mse: 60.1395 - mae: 6.0400 - val_loss: 118.0519 - val_mse: 118.0519 - val_mae: 8.3223\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 59.1617 - mse: 59.1617 - mae: 5.9895 - val_loss: 112.0551 - val_mse: 112.0551 - val_mae: 8.1049\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 59.1416 - mse: 59.1416 - mae: 5.9610 - val_loss: 113.5056 - val_mse: 113.5056 - val_mae: 8.1310\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 59.0516 - mse: 59.0516 - mae: 5.9723 - val_loss: 118.2029 - val_mse: 118.2029 - val_mae: 8.3303\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 57.2332 - mse: 57.2332 - mae: 5.8767 - val_loss: 113.3879 - val_mse: 113.3879 - val_mae: 8.2651\n",
      "Epoch 244/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 56.8850 - mse: 56.8850 - mae: 5.8466 - val_loss: 122.1442 - val_mse: 122.1442 - val_mae: 8.3623\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 22s 166ms/step - loss: 57.2532 - mse: 57.2532 - mae: 5.8400 - val_loss: 119.4165 - val_mse: 119.4165 - val_mae: 8.2984\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 56.1932 - mse: 56.1932 - mae: 5.7754 - val_loss: 116.3657 - val_mse: 116.3657 - val_mae: 8.2867\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 56.4864 - mse: 56.4864 - mae: 5.8218 - val_loss: 115.0961 - val_mse: 115.0961 - val_mae: 8.2333\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 56.1957 - mse: 56.1957 - mae: 5.8046 - val_loss: 117.0100 - val_mse: 117.0100 - val_mae: 8.3111\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 23s 172ms/step - loss: 56.9712 - mse: 56.9712 - mae: 5.8479 - val_loss: 118.4197 - val_mse: 118.4197 - val_mae: 8.3090\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 55.3318 - mse: 55.3318 - mae: 5.7636 - val_loss: 116.5839 - val_mse: 116.5839 - val_mae: 8.2265\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 54.8709 - mse: 54.8709 - mae: 5.7237 - val_loss: 117.3028 - val_mse: 117.3028 - val_mae: 8.3079\n",
      "Epoch 252/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 53.6073 - mse: 53.6073 - mae: 5.6456 - val_loss: 118.3050 - val_mse: 118.3050 - val_mae: 8.2601\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 54.1824 - mse: 54.1824 - mae: 5.6361 - val_loss: 121.2698 - val_mse: 121.2698 - val_mae: 8.4241\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 54.4312 - mse: 54.4312 - mae: 5.6442 - val_loss: 115.5575 - val_mse: 115.5575 - val_mae: 8.1970\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 52.8999 - mse: 52.8999 - mae: 5.5851 - val_loss: 121.4725 - val_mse: 121.4725 - val_mae: 8.4283\n",
      "Epoch 256/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 53.4199 - mse: 53.4199 - mae: 5.6321 - val_loss: 121.4614 - val_mse: 121.4614 - val_mae: 8.4120\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 54.5029 - mse: 54.5029 - mae: 5.6628 - val_loss: 117.8203 - val_mse: 117.8203 - val_mae: 8.2838\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 30s 221ms/step - loss: 53.1256 - mse: 53.1256 - mae: 5.5834 - val_loss: 118.1550 - val_mse: 118.1550 - val_mae: 8.2874\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 51.4793 - mse: 51.4793 - mae: 5.5379 - val_loss: 118.7921 - val_mse: 118.7921 - val_mae: 8.1389\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 52.1369 - mse: 52.1369 - mae: 5.5374 - val_loss: 117.0859 - val_mse: 117.0859 - val_mae: 8.1950\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 52.6783 - mse: 52.6783 - mae: 5.5857 - val_loss: 119.6548 - val_mse: 119.6548 - val_mae: 8.2657\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 50.1485 - mse: 50.1485 - mae: 5.4124 - val_loss: 117.1921 - val_mse: 117.1921 - val_mae: 8.2450\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 50.2585 - mse: 50.2585 - mae: 5.4521 - val_loss: 122.0914 - val_mse: 122.0914 - val_mae: 8.4350\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 50.7718 - mse: 50.7718 - mae: 5.4481 - val_loss: 125.6537 - val_mse: 125.6537 - val_mae: 8.5484\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 49.0515 - mse: 49.0515 - mae: 5.3188 - val_loss: 120.6093 - val_mse: 120.6093 - val_mae: 8.3995\n",
      "Epoch 266/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 49.1566 - mse: 49.1566 - mae: 5.3428 - val_loss: 125.6662 - val_mse: 125.6662 - val_mae: 8.5411\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 48.1631 - mse: 48.1631 - mae: 5.3095 - val_loss: 124.1365 - val_mse: 124.1365 - val_mae: 8.4345\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 47.4690 - mse: 47.4690 - mae: 5.2525 - val_loss: 124.7397 - val_mse: 124.7397 - val_mae: 8.4587\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 48.4001 - mse: 48.4001 - mae: 5.3287 - val_loss: 129.2748 - val_mse: 129.2748 - val_mae: 8.6551\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 47.0457 - mse: 47.0457 - mae: 5.2492 - val_loss: 119.8386 - val_mse: 119.8386 - val_mae: 8.3871\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 45.8009 - mse: 45.8009 - mae: 5.1884 - val_loss: 130.3580 - val_mse: 130.3580 - val_mae: 8.5983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 46.6494 - mse: 46.6494 - mae: 5.1842 - val_loss: 116.6986 - val_mse: 116.6986 - val_mae: 8.2034\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 46.3519 - mse: 46.3519 - mae: 5.1648 - val_loss: 128.8600 - val_mse: 128.8600 - val_mae: 8.5453\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 44.9291 - mse: 44.9291 - mae: 5.1232 - val_loss: 122.6570 - val_mse: 122.6570 - val_mae: 8.4496\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 44.9802 - mse: 44.9802 - mae: 5.1139 - val_loss: 123.7883 - val_mse: 123.7883 - val_mae: 8.4896\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 44.1860 - mse: 44.1860 - mae: 5.0418 - val_loss: 120.9451 - val_mse: 120.9451 - val_mae: 8.3821\n",
      "Epoch 277/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 43.4862 - mse: 43.4862 - mae: 5.0371 - val_loss: 127.3350 - val_mse: 127.3350 - val_mae: 8.5498\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 44.0084 - mse: 44.0084 - mae: 5.0338 - val_loss: 120.7333 - val_mse: 120.7333 - val_mae: 8.4024\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 42.6255 - mse: 42.6255 - mae: 4.9721 - val_loss: 122.1089 - val_mse: 122.1089 - val_mae: 8.3698\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 43.1672 - mse: 43.1672 - mae: 4.9594 - val_loss: 129.1494 - val_mse: 129.1494 - val_mae: 8.6479\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 42.8152 - mse: 42.8152 - mae: 4.9765 - val_loss: 123.3406 - val_mse: 123.3406 - val_mae: 8.4274\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 42.5239 - mse: 42.5239 - mae: 4.9653 - val_loss: 124.1238 - val_mse: 124.1238 - val_mae: 8.4085\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 41.5911 - mse: 41.5911 - mae: 4.8859 - val_loss: 121.9719 - val_mse: 121.9719 - val_mae: 8.3421\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 41.2375 - mse: 41.2375 - mae: 4.8535 - val_loss: 129.2825 - val_mse: 129.2825 - val_mae: 8.6011\n",
      "Epoch 285/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 40.7554 - mse: 40.7554 - mae: 4.8336 - val_loss: 123.7840 - val_mse: 123.7840 - val_mae: 8.4255\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 41.0166 - mse: 41.0166 - mae: 4.8722 - val_loss: 126.4649 - val_mse: 126.4649 - val_mae: 8.5060\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 41.0771 - mse: 41.0771 - mae: 4.8532 - val_loss: 122.3781 - val_mse: 122.3781 - val_mae: 8.4594\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 40.1564 - mse: 40.1564 - mae: 4.7853 - val_loss: 126.5149 - val_mse: 126.5149 - val_mae: 8.5825\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 40.7801 - mse: 40.7801 - mae: 4.8205 - val_loss: 130.9201 - val_mse: 130.9201 - val_mae: 8.6824\n",
      "Epoch 290/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 40.5098 - mse: 40.5098 - mae: 4.7583 - val_loss: 126.6243 - val_mse: 126.6243 - val_mae: 8.6768\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 40.6155 - mse: 40.6155 - mae: 4.8138 - val_loss: 129.5661 - val_mse: 129.5661 - val_mae: 8.6434\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 38.5499 - mse: 38.5499 - mae: 4.6924 - val_loss: 128.5905 - val_mse: 128.5905 - val_mae: 8.5976\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 39.9266 - mse: 39.9266 - mae: 4.7408 - val_loss: 126.4030 - val_mse: 126.4030 - val_mae: 8.5723\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 37.8076 - mse: 37.8076 - mae: 4.6528 - val_loss: 123.2798 - val_mse: 123.2798 - val_mae: 8.4604\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 39.0778 - mse: 39.0778 - mae: 4.7155 - val_loss: 130.9144 - val_mse: 130.9144 - val_mae: 8.6822\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 37.8734 - mse: 37.8734 - mae: 4.6226 - val_loss: 132.5170 - val_mse: 132.5170 - val_mae: 8.7486\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 38.9244 - mse: 38.9244 - mae: 4.6377 - val_loss: 126.7231 - val_mse: 126.7231 - val_mae: 8.5407\n",
      "Epoch 298/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 37.5484 - mse: 37.5484 - mae: 4.6291 - val_loss: 133.4943 - val_mse: 133.4943 - val_mae: 8.7520\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 37.9353 - mse: 37.9353 - mae: 4.5914 - val_loss: 124.3817 - val_mse: 124.3817 - val_mae: 8.5424\n",
      "Epoch 300/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 36.7154 - mse: 36.7154 - mae: 4.5607 - val_loss: 130.1960 - val_mse: 130.1960 - val_mae: 8.7000\n",
      "57/57 [==============================] - 4s 37ms/step - loss: 149.2583 - mse: 149.2583 - mae: 9.5943\n",
      "57/57 [==============================] - 4s 39ms/step\n",
      "MAE: 9.594330475728812\n",
      "MSE: 149.25825021272024\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 60, 240)          173760    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 120)              144480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 28s 159ms/step - loss: 4017.1350 - mse: 4017.1350 - mae: 62.0645 - val_loss: 3174.1665 - val_mse: 3174.1665 - val_mae: 55.2403\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2771.0415 - mse: 2771.0415 - mae: 51.3948 - val_loss: 2388.7422 - val_mse: 2388.7422 - val_mae: 47.6032\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2077.6113 - mse: 2077.6113 - mae: 44.1522 - val_loss: 1775.2275 - val_mse: 1775.2275 - val_mae: 40.6516\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1532.9503 - mse: 1532.9503 - mae: 37.5048 - val_loss: 1299.1316 - val_mse: 1299.1316 - val_mae: 34.2995\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1113.2083 - mse: 1113.2083 - mae: 31.3928 - val_loss: 936.2974 - val_mse: 936.2974 - val_mae: 28.5240\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 800.8013 - mse: 800.8013 - mae: 25.9560 - val_loss: 670.3751 - val_mse: 670.3751 - val_mae: 23.4500\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 573.7552 - mse: 573.7552 - mae: 21.3661 - val_loss: 480.7680 - val_mse: 480.7680 - val_mae: 19.3117\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 413.1086 - mse: 413.1086 - mae: 17.6676 - val_loss: 346.9988 - val_mse: 346.9988 - val_mae: 16.0787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 300.3021 - mse: 300.3021 - mae: 14.8208 - val_loss: 254.7622 - val_mse: 254.7622 - val_mae: 13.6387\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 226.5113 - mse: 226.5113 - mae: 12.7229 - val_loss: 197.6487 - val_mse: 197.6487 - val_mae: 11.8348\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 181.2280 - mse: 181.2280 - mae: 11.2114 - val_loss: 163.2054 - val_mse: 163.2054 - val_mae: 10.6423\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 154.5181 - mse: 154.5181 - mae: 10.3000 - val_loss: 143.5488 - val_mse: 143.5488 - val_mae: 9.9640\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 139.4642 - mse: 139.4642 - mae: 9.7599 - val_loss: 132.6458 - val_mse: 132.6458 - val_mae: 9.5550\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 131.4790 - mse: 131.4790 - mae: 9.4400 - val_loss: 127.1556 - val_mse: 127.1556 - val_mae: 9.3104\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 127.5519 - mse: 127.5519 - mae: 9.2801 - val_loss: 124.5597 - val_mse: 124.5597 - val_mae: 9.2212\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 125.7035 - mse: 125.7035 - mae: 9.2185 - val_loss: 123.3961 - val_mse: 123.3961 - val_mae: 9.1672\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8981 - mse: 124.8981 - mae: 9.1816 - val_loss: 122.9312 - val_mse: 122.9312 - val_mae: 9.1356\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5897 - mse: 124.5897 - mae: 9.1606 - val_loss: 122.7531 - val_mse: 122.7531 - val_mae: 9.1146\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4722 - mse: 124.4722 - mae: 9.1442 - val_loss: 122.6977 - val_mse: 122.6977 - val_mae: 9.1023\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4312 - mse: 124.4312 - mae: 9.1403 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0946\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4258 - mse: 124.4258 - mae: 9.1345 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4422 - mse: 124.4422 - mae: 9.1354 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4212 - mse: 124.4212 - mae: 9.1349 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4260 - mse: 124.4260 - mae: 9.1352 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4265 - mse: 124.4265 - mae: 9.1347 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4217 - mse: 124.4217 - mae: 9.1346 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4210 - mse: 124.4210 - mae: 9.1342 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4314 - mse: 124.4314 - mae: 9.1350 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4271 - mse: 124.4271 - mae: 9.1351 - val_loss: 122.6817 - val_mse: 122.6817 - val_mae: 9.0936\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4258 - mse: 124.4258 - mae: 9.1356 - val_loss: 122.6828 - val_mse: 122.6828 - val_mae: 9.0959\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4215 - mse: 124.4215 - mae: 9.1371 - val_loss: 122.6805 - val_mse: 122.6805 - val_mae: 9.0936\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4360 - mse: 124.4360 - mae: 9.1357 - val_loss: 122.6802 - val_mse: 122.6802 - val_mae: 9.0937\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4282 - mse: 124.4282 - mae: 9.1355 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4806 - mse: 124.4806 - mae: 9.1393 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4518 - mse: 124.4518 - mae: 9.1364 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4369 - mse: 124.4369 - mae: 9.1351 - val_loss: 122.6855 - val_mse: 122.6855 - val_mae: 9.0933\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4346 - mse: 124.4346 - mae: 9.1353 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0944\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4312 - mse: 124.4312 - mae: 9.1366 - val_loss: 122.6813 - val_mse: 122.6813 - val_mae: 9.0936\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4546 - mse: 124.4546 - mae: 9.1346 - val_loss: 122.6858 - val_mse: 122.6858 - val_mae: 9.0976\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4365 - mse: 124.4365 - mae: 9.1371 - val_loss: 122.6848 - val_mse: 122.6848 - val_mae: 9.0934\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4470 - mse: 124.4470 - mae: 9.1362 - val_loss: 122.6876 - val_mse: 122.6876 - val_mae: 9.0932\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4145 - mse: 124.4145 - mae: 9.1334 - val_loss: 122.6883 - val_mse: 122.6883 - val_mae: 9.0988\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4685 - mse: 124.4685 - mae: 9.1368 - val_loss: 122.6875 - val_mse: 122.6875 - val_mae: 9.0984\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4279 - mse: 124.4279 - mae: 9.1391 - val_loss: 122.7003 - val_mse: 122.7003 - val_mae: 9.0928\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4562 - mse: 124.4562 - mae: 9.1373 - val_loss: 122.7312 - val_mse: 122.7312 - val_mae: 9.0920\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4412 - mse: 124.4412 - mae: 9.1356 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0937\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5017 - mse: 124.5017 - mae: 9.1461 - val_loss: 122.7522 - val_mse: 122.7522 - val_mae: 9.0916\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4156 - mse: 124.4156 - mae: 9.1339 - val_loss: 122.6930 - val_mse: 122.6930 - val_mae: 9.1007\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4887 - mse: 124.4887 - mae: 9.1395 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4565 - mse: 124.4565 - mae: 9.1360 - val_loss: 122.7864 - val_mse: 122.7864 - val_mae: 9.0911\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4703 - mse: 124.4703 - mae: 9.1368 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5263 - mse: 124.5263 - mae: 9.1437 - val_loss: 122.7003 - val_mse: 122.7003 - val_mae: 9.0928\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4767 - mse: 124.4767 - mae: 9.1372 - val_loss: 122.6845 - val_mse: 122.6845 - val_mae: 9.0934\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4677 - mse: 124.4677 - mae: 9.1383 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5076 - mse: 124.5076 - mae: 9.1415 - val_loss: 122.7239 - val_mse: 122.7239 - val_mae: 9.0922\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4976 - mse: 124.4976 - mae: 9.1375 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4807 - mse: 124.4807 - mae: 9.1379 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4761 - mse: 124.4761 - mae: 9.1381 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4478 - mse: 124.4478 - mae: 9.1362 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4311 - mse: 124.4311 - mae: 9.1380 - val_loss: 122.7089 - val_mse: 122.7089 - val_mae: 9.0925\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4229 - mse: 124.4229 - mae: 9.1349 - val_loss: 122.7305 - val_mse: 122.7305 - val_mae: 9.1105\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4492 - mse: 124.4492 - mae: 9.1430 - val_loss: 122.7726 - val_mse: 122.7726 - val_mae: 9.0913\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4080 - mse: 124.4080 - mae: 9.1338 - val_loss: 122.7116 - val_mse: 122.7116 - val_mae: 9.1063\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5067 - mse: 124.5067 - mae: 9.1401 - val_loss: 122.6843 - val_mse: 122.6843 - val_mae: 9.0934\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4952 - mse: 124.4952 - mae: 9.1428 - val_loss: 122.6918 - val_mse: 122.6918 - val_mae: 9.0930\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4432 - mse: 124.4432 - mae: 9.1367 - val_loss: 122.7030 - val_mse: 122.7030 - val_mae: 9.1040\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4750 - mse: 124.4750 - mae: 9.1393 - val_loss: 122.7293 - val_mse: 122.7293 - val_mae: 9.1102\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5064 - mse: 124.5064 - mae: 9.1378 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0946\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4530 - mse: 124.4530 - mae: 9.1352 - val_loss: 122.8499 - val_mse: 122.8499 - val_mae: 9.1275\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4940 - mse: 124.4940 - mae: 9.1428 - val_loss: 122.7217 - val_mse: 122.7217 - val_mae: 9.0922\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4039 - mse: 124.4039 - mae: 9.1353 - val_loss: 122.7934 - val_mse: 122.7934 - val_mae: 9.1206\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5773 - mse: 124.5773 - mae: 9.1471 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4718 - mse: 124.4718 - mae: 9.1373 - val_loss: 122.7062 - val_mse: 122.7062 - val_mae: 9.1049\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4631 - mse: 124.4631 - mae: 9.1371 - val_loss: 122.7103 - val_mse: 122.7103 - val_mae: 9.0925\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4638 - mse: 124.4638 - mae: 9.1399 - val_loss: 122.8212 - val_mse: 122.8212 - val_mae: 9.0907\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4597 - mse: 124.4597 - mae: 9.1303 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4553 - mse: 124.4553 - mae: 9.1374 - val_loss: 122.7529 - val_mse: 122.7529 - val_mae: 9.1146\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.5181 - mse: 124.5181 - mae: 9.1398 - val_loss: 122.6851 - val_mse: 122.6851 - val_mae: 9.0972\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4611 - mse: 124.4611 - mae: 9.1399 - val_loss: 122.7241 - val_mse: 122.7241 - val_mae: 9.0922\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4816 - mse: 124.4816 - mae: 9.1384 - val_loss: 122.7264 - val_mse: 122.7264 - val_mae: 9.0921\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4949 - mse: 124.4949 - mae: 9.1360 - val_loss: 122.7083 - val_mse: 122.7083 - val_mae: 9.0925\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4599 - mse: 124.4599 - mae: 9.1391 - val_loss: 122.6883 - val_mse: 122.6883 - val_mae: 9.0932\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.4604 - mse: 124.4604 - mae: 9.1374 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4887 - mse: 124.4887 - mae: 9.1392 - val_loss: 122.6864 - val_mse: 122.6864 - val_mae: 9.0979\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4314 - mse: 124.4314 - mae: 9.1358 - val_loss: 122.7124 - val_mse: 122.7124 - val_mae: 9.0924\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4647 - mse: 124.4647 - mae: 9.1389 - val_loss: 122.6977 - val_mse: 122.6977 - val_mae: 9.1023\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4879 - mse: 124.4879 - mae: 9.1397 - val_loss: 122.7197 - val_mse: 122.7197 - val_mae: 9.0923\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4545 - mse: 124.4545 - mae: 9.1345 - val_loss: 122.7646 - val_mse: 122.7646 - val_mae: 9.1164\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5739 - mse: 124.5739 - mae: 9.1516 - val_loss: 122.8093 - val_mse: 122.8093 - val_mae: 9.0908\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5075 - mse: 124.5075 - mae: 9.1414 - val_loss: 122.7789 - val_mse: 122.7789 - val_mae: 9.0912\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5466 - mse: 124.5466 - mae: 9.1389 - val_loss: 122.7235 - val_mse: 122.7235 - val_mae: 9.0922\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5108 - mse: 124.5108 - mae: 9.1363 - val_loss: 122.6890 - val_mse: 122.6890 - val_mae: 9.0932\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4735 - mse: 124.4735 - mae: 9.1359 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0951\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4738 - mse: 124.4738 - mae: 9.1407 - val_loss: 122.6921 - val_mse: 122.6921 - val_mae: 9.0930\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4898 - mse: 124.4898 - mae: 9.1398 - val_loss: 122.6912 - val_mse: 122.6912 - val_mae: 9.0931\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.4704 - mse: 124.4704 - mae: 9.1353 - val_loss: 122.6817 - val_mse: 122.6817 - val_mae: 9.0951\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4851 - mse: 124.4851 - mae: 9.1373 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0943\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4690 - mse: 124.4690 - mae: 9.1396 - val_loss: 122.6866 - val_mse: 122.6866 - val_mae: 9.0933\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4393 - mse: 124.4393 - mae: 9.1326 - val_loss: 122.6981 - val_mse: 122.6981 - val_mae: 9.1025\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4506 - mse: 124.4506 - mae: 9.1345 - val_loss: 122.7819 - val_mse: 122.7819 - val_mae: 9.1190\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4968 - mse: 124.4968 - mae: 9.1486 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4861 - mse: 124.4861 - mae: 9.1373 - val_loss: 122.7015 - val_mse: 122.7015 - val_mae: 9.0927\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4463 - mse: 124.4463 - mae: 9.1363 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5033 - mse: 124.5033 - mae: 9.1394 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4764 - mse: 124.4764 - mae: 9.1358 - val_loss: 122.6824 - val_mse: 122.6824 - val_mae: 9.0957\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4669 - mse: 124.4669 - mae: 9.1319 - val_loss: 122.7143 - val_mse: 122.7143 - val_mae: 9.1069\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4826 - mse: 124.4826 - mae: 9.1430 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5066 - mse: 124.5066 - mae: 9.1399 - val_loss: 122.6873 - val_mse: 122.6873 - val_mae: 9.0932\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4570 - mse: 124.4570 - mae: 9.1368 - val_loss: 122.6946 - val_mse: 122.6946 - val_mae: 9.1013\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4538 - mse: 124.4538 - mae: 9.1420 - val_loss: 122.7072 - val_mse: 122.7072 - val_mae: 9.0926\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4335 - mse: 124.4335 - mae: 9.1412 - val_loss: 122.8365 - val_mse: 122.8365 - val_mae: 9.0905\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4431 - mse: 124.4431 - mae: 9.1336 - val_loss: 122.7457 - val_mse: 122.7457 - val_mae: 9.1133\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5604 - mse: 124.5604 - mae: 9.1405 - val_loss: 122.6853 - val_mse: 122.6853 - val_mae: 9.0973\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4898 - mse: 124.4898 - mae: 9.1411 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4630 - mse: 124.4630 - mae: 9.1365 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4586 - mse: 124.4586 - mae: 9.1394 - val_loss: 122.7136 - val_mse: 122.7136 - val_mae: 9.0924\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4631 - mse: 124.4631 - mae: 9.1362 - val_loss: 122.6832 - val_mse: 122.6832 - val_mae: 9.0935\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5253 - mse: 124.5253 - mae: 9.1393 - val_loss: 122.7133 - val_mse: 122.7133 - val_mae: 9.1067\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4688 - mse: 124.4688 - mae: 9.1374 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5604 - mse: 124.5604 - mae: 9.1395 - val_loss: 122.7661 - val_mse: 122.7661 - val_mae: 9.1167\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5048 - mse: 124.5048 - mae: 9.1416 - val_loss: 122.7460 - val_mse: 122.7460 - val_mae: 9.0917\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5115 - mse: 124.5115 - mae: 9.1369 - val_loss: 122.6804 - val_mse: 122.6804 - val_mae: 9.0937\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5077 - mse: 124.5077 - mae: 9.1406 - val_loss: 122.8151 - val_mse: 122.8151 - val_mae: 9.0907\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4873 - mse: 124.4873 - mae: 9.1440 - val_loss: 122.7864 - val_mse: 122.7864 - val_mae: 9.0911\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4362 - mse: 124.4362 - mae: 9.1347 - val_loss: 122.7389 - val_mse: 122.7389 - val_mae: 9.1121\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4988 - mse: 124.4988 - mae: 9.1433 - val_loss: 122.7048 - val_mse: 122.7048 - val_mae: 9.0926\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4995 - mse: 124.4995 - mae: 9.1383 - val_loss: 122.7010 - val_mse: 122.7010 - val_mae: 9.1034\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5062 - mse: 124.5062 - mae: 9.1387 - val_loss: 122.7001 - val_mse: 122.7001 - val_mae: 9.1031\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4662 - mse: 124.4662 - mae: 9.1401 - val_loss: 122.6887 - val_mse: 122.6887 - val_mae: 9.0932\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5009 - mse: 124.5009 - mae: 9.1373 - val_loss: 122.6808 - val_mse: 122.6808 - val_mae: 9.0936\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5037 - mse: 124.5037 - mae: 9.1397 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4482 - mse: 124.4482 - mae: 9.1353 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4632 - mse: 124.4632 - mae: 9.1368 - val_loss: 122.6903 - val_mse: 122.6903 - val_mae: 9.0931\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5450 - mse: 124.5450 - mae: 9.1394 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4649 - mse: 124.4649 - mae: 9.1372 - val_loss: 122.7075 - val_mse: 122.7075 - val_mae: 9.1052\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4941 - mse: 124.4941 - mae: 9.1431 - val_loss: 122.7167 - val_mse: 122.7167 - val_mae: 9.0923\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4732 - mse: 124.4732 - mae: 9.1362 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4917 - mse: 124.4917 - mae: 9.1357 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0944\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4900 - mse: 124.4900 - mae: 9.1399 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1347 - val_loss: 122.8163 - val_mse: 122.8163 - val_mae: 9.1236\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4546 - mse: 124.4546 - mae: 9.1429 - val_loss: 122.6930 - val_mse: 122.6930 - val_mae: 9.0930\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4759 - mse: 124.4759 - mae: 9.1376 - val_loss: 122.7192 - val_mse: 122.7192 - val_mae: 9.1081\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4626 - mse: 124.4626 - mae: 9.1425 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0937\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4595 - mse: 124.4595 - mae: 9.1354 - val_loss: 122.6923 - val_mse: 122.6923 - val_mae: 9.0930\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4877 - mse: 124.4877 - mae: 9.1402 - val_loss: 122.6833 - val_mse: 122.6833 - val_mae: 9.0935\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4942 - mse: 124.4942 - mae: 9.1378 - val_loss: 122.7124 - val_mse: 122.7124 - val_mae: 9.0924\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4608 - mse: 124.4608 - mae: 9.1367 - val_loss: 122.6948 - val_mse: 122.6948 - val_mae: 9.0929\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4797 - mse: 124.4797 - mae: 9.1380 - val_loss: 122.6981 - val_mse: 122.6981 - val_mae: 9.0928\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4996 - mse: 124.4996 - mae: 9.1367 - val_loss: 122.6916 - val_mse: 122.6916 - val_mae: 9.1002\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4456 - mse: 124.4456 - mae: 9.1358 - val_loss: 122.6940 - val_mse: 122.6940 - val_mae: 9.0930\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5211 - mse: 124.5211 - mae: 9.1381 - val_loss: 122.7160 - val_mse: 122.7160 - val_mae: 9.1073\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4358 - mse: 124.4358 - mae: 9.1440 - val_loss: 122.7474 - val_mse: 122.7474 - val_mae: 9.0917\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.5370 - mse: 124.5370 - mae: 9.1374 - val_loss: 122.6887 - val_mse: 122.6887 - val_mae: 9.0990\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4441 - mse: 124.4441 - mae: 9.1366 - val_loss: 122.8059 - val_mse: 122.8059 - val_mae: 9.0908\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4784 - mse: 124.4784 - mae: 9.1350 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5296 - mse: 124.5296 - mae: 9.1402 - val_loss: 122.8270 - val_mse: 122.8270 - val_mae: 9.0906\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5140 - mse: 124.5140 - mae: 9.1366 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4865 - mse: 124.4865 - mae: 9.1412 - val_loss: 122.6946 - val_mse: 122.6946 - val_mae: 9.1013\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4511 - mse: 124.4511 - mae: 9.1359 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 160/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4783 - mse: 124.4783 - mae: 9.1379 - val_loss: 122.7294 - val_mse: 122.7294 - val_mae: 9.1102\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4971 - mse: 124.4971 - mae: 9.1399 - val_loss: 122.7697 - val_mse: 122.7697 - val_mae: 9.0913\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5411 - mse: 124.5411 - mae: 9.1404 - val_loss: 122.6820 - val_mse: 122.6820 - val_mae: 9.0935\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4634 - mse: 124.4634 - mae: 9.1417 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4760 - mse: 124.4760 - mae: 9.1380 - val_loss: 122.7278 - val_mse: 122.7278 - val_mae: 9.1099\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4783 - mse: 124.4783 - mae: 9.1377 - val_loss: 122.7267 - val_mse: 122.7267 - val_mae: 9.0921\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5170 - mse: 124.5170 - mae: 9.1403 - val_loss: 122.6912 - val_mse: 122.6912 - val_mae: 9.0931\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4728 - mse: 124.4728 - mae: 9.1390 - val_loss: 122.7427 - val_mse: 122.7427 - val_mae: 9.1128\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5190 - mse: 124.5190 - mae: 9.1401 - val_loss: 122.7358 - val_mse: 122.7358 - val_mae: 9.0919\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5318 - mse: 124.5318 - mae: 9.1393 - val_loss: 122.7796 - val_mse: 122.7796 - val_mae: 9.0912\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4005 - mse: 124.4005 - mae: 9.1334 - val_loss: 122.7421 - val_mse: 122.7421 - val_mae: 9.1127\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4772 - mse: 124.4772 - mae: 9.1428 - val_loss: 122.6993 - val_mse: 122.6993 - val_mae: 9.1029\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.4826 - mse: 124.4826 - mae: 9.1382 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4830 - mse: 124.4830 - mae: 9.1415 - val_loss: 122.7009 - val_mse: 122.7009 - val_mae: 9.0927\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4868 - mse: 124.4868 - mae: 9.1397 - val_loss: 122.6823 - val_mse: 122.6823 - val_mae: 9.0935\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4868 - mse: 124.4868 - mae: 9.1377 - val_loss: 122.6952 - val_mse: 122.6952 - val_mae: 9.0929\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.5404 - mse: 124.5404 - mae: 9.1395 - val_loss: 122.7002 - val_mse: 122.7002 - val_mae: 9.0928\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5025 - mse: 124.5025 - mae: 9.1448 - val_loss: 122.6981 - val_mse: 122.6981 - val_mae: 9.0928\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4553 - mse: 124.4553 - mae: 9.1398 - val_loss: 122.7846 - val_mse: 122.7846 - val_mae: 9.0911\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5089 - mse: 124.5089 - mae: 9.1383 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4374 - mse: 124.4374 - mae: 9.1350 - val_loss: 122.7650 - val_mse: 122.7650 - val_mae: 9.1165\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5167 - mse: 124.5167 - mae: 9.1405 - val_loss: 122.7333 - val_mse: 122.7333 - val_mae: 9.1110\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4897 - mse: 124.4897 - mae: 9.1480 - val_loss: 122.7569 - val_mse: 122.7569 - val_mae: 9.0915\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4374 - mse: 124.4374 - mae: 9.1363 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0945\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5257 - mse: 124.5257 - mae: 9.1385 - val_loss: 122.7103 - val_mse: 122.7103 - val_mae: 9.1059\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4787 - mse: 124.4787 - mae: 9.1432 - val_loss: 122.7393 - val_mse: 122.7393 - val_mae: 9.0919\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4741 - mse: 124.4741 - mae: 9.1374 - val_loss: 122.7531 - val_mse: 122.7531 - val_mae: 9.0916\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4811 - mse: 124.4811 - mae: 9.1365 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4348 - mse: 124.4348 - mae: 9.1398 - val_loss: 122.7586 - val_mse: 122.7586 - val_mae: 9.0915\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4862 - mse: 124.4862 - mae: 9.1385 - val_loss: 122.8074 - val_mse: 122.8074 - val_mae: 9.1224\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5476 - mse: 124.5476 - mae: 9.1435 - val_loss: 122.7509 - val_mse: 122.7509 - val_mae: 9.1142\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5174 - mse: 124.5174 - mae: 9.1406 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0944\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5099 - mse: 124.5099 - mae: 9.1370 - val_loss: 122.7002 - val_mse: 122.7002 - val_mae: 9.1031\n",
      "Epoch 193/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5930 - mse: 124.5930 - mae: 9.1460 - val_loss: 122.6836 - val_mse: 122.6836 - val_mae: 9.0964\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4578 - mse: 124.4578 - mae: 9.1372 - val_loss: 122.7346 - val_mse: 122.7346 - val_mae: 9.1113\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4867 - mse: 124.4867 - mae: 9.1380 - val_loss: 122.7233 - val_mse: 122.7233 - val_mae: 9.1090\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4592 - mse: 124.4592 - mae: 9.1413 - val_loss: 122.7020 - val_mse: 122.7020 - val_mae: 9.0927\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4990 - mse: 124.4990 - mae: 9.1399 - val_loss: 122.6966 - val_mse: 122.6966 - val_mae: 9.0929\n",
      "Epoch 198/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4913 - mse: 124.4913 - mae: 9.1387 - val_loss: 122.6942 - val_mse: 122.6942 - val_mae: 9.0930\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4943 - mse: 124.4943 - mae: 9.1384 - val_loss: 122.6968 - val_mse: 122.6968 - val_mae: 9.0929\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4141 - mse: 124.4141 - mae: 9.1365 - val_loss: 122.7668 - val_mse: 122.7668 - val_mae: 9.1168\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5180 - mse: 124.5180 - mae: 9.1419 - val_loss: 122.7337 - val_mse: 122.7337 - val_mae: 9.0920\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5424 - mse: 124.5424 - mae: 9.1409 - val_loss: 122.7322 - val_mse: 122.7322 - val_mae: 9.1108\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4952 - mse: 124.4952 - mae: 9.1363 - val_loss: 122.6808 - val_mse: 122.6808 - val_mae: 9.0947\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4937 - mse: 124.4937 - mae: 9.1422 - val_loss: 122.6877 - val_mse: 122.6877 - val_mae: 9.0932\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4817 - mse: 124.4817 - mae: 9.1368 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0945\n",
      "Epoch 206/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4765 - mse: 124.4765 - mae: 9.1389 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4722 - mse: 124.4722 - mae: 9.1400 - val_loss: 122.7267 - val_mse: 122.7267 - val_mae: 9.1097\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5040 - mse: 124.5040 - mae: 9.1398 - val_loss: 122.7198 - val_mse: 122.7198 - val_mae: 9.1082\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5323 - mse: 124.5323 - mae: 9.1441 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4925 - mse: 124.4925 - mae: 9.1391 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4617 - mse: 124.4617 - mae: 9.1378 - val_loss: 122.7041 - val_mse: 122.7041 - val_mae: 9.1043\n",
      "Epoch 212/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4060 - mse: 124.4060 - mae: 9.1431 - val_loss: 122.8815 - val_mse: 122.8815 - val_mae: 9.0900\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4788 - mse: 124.4788 - mae: 9.1373 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0936\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4976 - mse: 124.4976 - mae: 9.1394 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4945 - mse: 124.4945 - mae: 9.1426 - val_loss: 122.7761 - val_mse: 122.7761 - val_mae: 9.0913\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5051 - mse: 124.5051 - mae: 9.1393 - val_loss: 122.6972 - val_mse: 122.6972 - val_mae: 9.1022\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4793 - mse: 124.4793 - mae: 9.1364 - val_loss: 122.6921 - val_mse: 122.6921 - val_mae: 9.1004\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4739 - mse: 124.4739 - mae: 9.1371 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0943\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4863 - mse: 124.4863 - mae: 9.1368 - val_loss: 122.7005 - val_mse: 122.7005 - val_mae: 9.1032\n",
      "Epoch 220/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4428 - mse: 124.4428 - mae: 9.1393 - val_loss: 122.6882 - val_mse: 122.6882 - val_mae: 9.0932\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4338 - mse: 124.4338 - mae: 9.1345 - val_loss: 122.7834 - val_mse: 122.7834 - val_mae: 9.1192\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4640 - mse: 124.4640 - mae: 9.1415 - val_loss: 122.7074 - val_mse: 122.7074 - val_mae: 9.0926\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4462 - mse: 124.4462 - mae: 9.1354 - val_loss: 122.7257 - val_mse: 122.7257 - val_mae: 9.1095\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4342 - mse: 124.4342 - mae: 9.1453 - val_loss: 122.7772 - val_mse: 122.7772 - val_mae: 9.0912\n",
      "Epoch 225/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5476 - mse: 124.5476 - mae: 9.1405 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 124.4048 - mse: 124.4048 - mae: 9.1396 - val_loss: 122.7507 - val_mse: 122.7507 - val_mae: 9.0917\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.5718 - mse: 124.5718 - mae: 9.1380 - val_loss: 122.7079 - val_mse: 122.7079 - val_mae: 9.0925\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4529 - mse: 124.4529 - mae: 9.1361 - val_loss: 122.6991 - val_mse: 122.6991 - val_mae: 9.1028\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4992 - mse: 124.4992 - mae: 9.1440 - val_loss: 122.7037 - val_mse: 122.7037 - val_mae: 9.0927\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 124.5371 - mse: 124.5371 - mae: 9.1402 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 231/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 124.4529 - mse: 124.4529 - mae: 9.1374 - val_loss: 122.6956 - val_mse: 122.6956 - val_mae: 9.0929\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4734 - mse: 124.4734 - mae: 9.1357 - val_loss: 122.6809 - val_mse: 122.6809 - val_mae: 9.0947\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4633 - mse: 124.4633 - mae: 9.1358 - val_loss: 122.6823 - val_mse: 122.6823 - val_mae: 9.0956\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5778 - mse: 124.5778 - mae: 9.1445 - val_loss: 122.6818 - val_mse: 122.6818 - val_mae: 9.0936\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 124.4510 - mse: 124.4510 - mae: 9.1387 - val_loss: 122.7057 - val_mse: 122.7057 - val_mae: 9.0926\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4461 - mse: 124.4461 - mae: 9.1418 - val_loss: 122.8302 - val_mse: 122.8302 - val_mae: 9.1252\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4663 - mse: 124.4663 - mae: 9.1386 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4702 - mse: 124.4702 - mae: 9.1376 - val_loss: 122.6841 - val_mse: 122.6841 - val_mae: 9.0934\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4521 - mse: 124.4521 - mae: 9.1359 - val_loss: 122.6953 - val_mse: 122.6953 - val_mae: 9.1015\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4507 - mse: 124.4507 - mae: 9.1419 - val_loss: 122.7390 - val_mse: 122.7390 - val_mae: 9.0919\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5155 - mse: 124.5155 - mae: 9.1371 - val_loss: 122.6822 - val_mse: 122.6822 - val_mae: 9.0955\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4761 - mse: 124.4761 - mae: 9.1372 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4531 - mse: 124.4531 - mae: 9.1420 - val_loss: 122.6850 - val_mse: 122.6850 - val_mae: 9.0934\n",
      "Epoch 244/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4802 - mse: 124.4802 - mae: 9.1356 - val_loss: 122.6854 - val_mse: 122.6854 - val_mae: 9.0933\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4881 - mse: 124.4881 - mae: 9.1383 - val_loss: 122.6859 - val_mse: 122.6859 - val_mae: 9.0976\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4599 - mse: 124.4599 - mae: 9.1396 - val_loss: 122.8121 - val_mse: 122.8121 - val_mae: 9.0908\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5231 - mse: 124.5231 - mae: 9.1377 - val_loss: 122.7017 - val_mse: 122.7017 - val_mae: 9.0927\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5004 - mse: 124.5004 - mae: 9.1423 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0946\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4462 - mse: 124.4462 - mae: 9.1398 - val_loss: 122.7322 - val_mse: 122.7322 - val_mae: 9.1108\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4926 - mse: 124.4926 - mae: 9.1404 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0945\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.3958 - mse: 124.3958 - mae: 9.1340 - val_loss: 122.7702 - val_mse: 122.7702 - val_mae: 9.1173\n",
      "Epoch 252/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4953 - mse: 124.4953 - mae: 9.1444 - val_loss: 122.7378 - val_mse: 122.7378 - val_mae: 9.0919\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.4506 - mse: 124.4506 - mae: 9.1380 - val_loss: 122.7171 - val_mse: 122.7171 - val_mae: 9.0923\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.4903 - mse: 124.4903 - mae: 9.1396 - val_loss: 122.6936 - val_mse: 122.6936 - val_mae: 9.0930\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.5067 - mse: 124.5067 - mae: 9.1367 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 256/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4969 - mse: 124.4969 - mae: 9.1398 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0944\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4216 - mse: 124.4216 - mae: 9.1350 - val_loss: 122.7478 - val_mse: 122.7478 - val_mae: 9.1137\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.5253 - mse: 124.5253 - mae: 9.1419 - val_loss: 122.6863 - val_mse: 122.6863 - val_mae: 9.0979\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5039 - mse: 124.5039 - mae: 9.1370 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4318 - mse: 124.4318 - mae: 9.1397 - val_loss: 122.7472 - val_mse: 122.7472 - val_mae: 9.0917\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4807 - mse: 124.4807 - mae: 9.1327 - val_loss: 122.6914 - val_mse: 122.6914 - val_mae: 9.1001\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5148 - mse: 124.5148 - mae: 9.1395 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0944\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4648 - mse: 124.4648 - mae: 9.1378 - val_loss: 122.6916 - val_mse: 122.6916 - val_mae: 9.1002\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4164 - mse: 124.4164 - mae: 9.1443 - val_loss: 122.8197 - val_mse: 122.8197 - val_mae: 9.0907\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4054 - mse: 124.4054 - mae: 9.1351 - val_loss: 122.7405 - val_mse: 122.7405 - val_mae: 9.1124\n",
      "Epoch 266/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4376 - mse: 124.4376 - mae: 9.1406 - val_loss: 122.7299 - val_mse: 122.7299 - val_mae: 9.0920\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4404 - mse: 124.4404 - mae: 9.1367 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0946\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4703 - mse: 124.4703 - mae: 9.1374 - val_loss: 122.6988 - val_mse: 122.6988 - val_mae: 9.0928\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4954 - mse: 124.4954 - mae: 9.1402 - val_loss: 122.6896 - val_mse: 122.6896 - val_mae: 9.0931\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4438 - mse: 124.4438 - mae: 9.1436 - val_loss: 122.7643 - val_mse: 122.7643 - val_mae: 9.0914\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4650 - mse: 124.4650 - mae: 9.1352 - val_loss: 122.6902 - val_mse: 122.6902 - val_mae: 9.0996\n",
      "Epoch 272/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5930 - mse: 124.5930 - mae: 9.1453 - val_loss: 122.6857 - val_mse: 122.6857 - val_mae: 9.0976\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4949 - mse: 124.4949 - mae: 9.1381 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4929 - mse: 124.4929 - mae: 9.1395 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4998 - mse: 124.4998 - mae: 9.1397 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0945\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4326 - mse: 124.4326 - mae: 9.1422 - val_loss: 122.7439 - val_mse: 122.7439 - val_mae: 9.0918\n",
      "Epoch 277/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5435 - mse: 124.5435 - mae: 9.1427 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5228 - mse: 124.5228 - mae: 9.1390 - val_loss: 122.6927 - val_mse: 122.6927 - val_mae: 9.0930\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4519 - mse: 124.4519 - mae: 9.1371 - val_loss: 122.7010 - val_mse: 122.7010 - val_mae: 9.0927\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5265 - mse: 124.5265 - mae: 9.1434 - val_loss: 122.6834 - val_mse: 122.6834 - val_mae: 9.0934\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4525 - mse: 124.4525 - mae: 9.1389 - val_loss: 122.7006 - val_mse: 122.7006 - val_mae: 9.0927\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4756 - mse: 124.4756 - mae: 9.1383 - val_loss: 122.7110 - val_mse: 122.7110 - val_mae: 9.0925\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4673 - mse: 124.4673 - mae: 9.1377 - val_loss: 122.6826 - val_mse: 122.6826 - val_mae: 9.0935\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4956 - mse: 124.4956 - mae: 9.1388 - val_loss: 122.6838 - val_mse: 122.6838 - val_mae: 9.0934\n",
      "Epoch 285/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4332 - mse: 124.4332 - mae: 9.1382 - val_loss: 122.8014 - val_mse: 122.8014 - val_mae: 9.0909\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4948 - mse: 124.4948 - mae: 9.1362 - val_loss: 122.6876 - val_mse: 122.6876 - val_mae: 9.0985\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5327 - mse: 124.5327 - mae: 9.1397 - val_loss: 122.7820 - val_mse: 122.7820 - val_mae: 9.1190\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4846 - mse: 124.4846 - mae: 9.1389 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0945\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4970 - mse: 124.4970 - mae: 9.1389 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0946\n",
      "Epoch 290/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4476 - mse: 124.4476 - mae: 9.1400 - val_loss: 122.6957 - val_mse: 122.6957 - val_mae: 9.0929\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4590 - mse: 124.4590 - mae: 9.1354 - val_loss: 122.6862 - val_mse: 122.6862 - val_mae: 9.0933\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4683 - mse: 124.4683 - mae: 9.1369 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0945\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4437 - mse: 124.4437 - mae: 9.1351 - val_loss: 122.7550 - val_mse: 122.7550 - val_mae: 9.0916\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4831 - mse: 124.4831 - mae: 9.1364 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4493 - mse: 124.4493 - mae: 9.1411 - val_loss: 122.7073 - val_mse: 122.7073 - val_mae: 9.0926\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4987 - mse: 124.4987 - mae: 9.1375 - val_loss: 122.6876 - val_mse: 122.6876 - val_mae: 9.0932\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.3854 - mse: 124.3854 - mae: 9.1284 - val_loss: 122.7743 - val_mse: 122.7743 - val_mae: 9.1179\n",
      "Epoch 298/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5143 - mse: 124.5143 - mae: 9.1443 - val_loss: 122.6903 - val_mse: 122.6903 - val_mae: 9.0931\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5086 - mse: 124.5086 - mae: 9.1386 - val_loss: 122.7094 - val_mse: 122.7094 - val_mae: 9.1057\n",
      "Epoch 300/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.5193 - mse: 124.5193 - mae: 9.1418 - val_loss: 122.6913 - val_mse: 122.6913 - val_mae: 9.0931\n",
      "57/57 [==============================] - 8s 34ms/step - loss: 133.8589 - mse: 133.8589 - mae: 9.4393\n",
      "57/57 [==============================] - 3s 33ms/step\n",
      "MAE: 9.439302952593358\n",
      "MSE: 133.85893226506522\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 60, 240)          173760    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 120)              144480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 27s 159ms/step - loss: 755.0946 - mse: 755.0946 - mae: 17.8900 - val_loss: 122.6805 - val_mse: 122.6805 - val_mae: 9.0936\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7065 - mse: 124.7065 - mae: 9.1499 - val_loss: 122.7601 - val_mse: 122.7601 - val_mae: 9.0915\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5923 - mse: 124.5923 - mae: 9.1541 - val_loss: 123.0584 - val_mse: 123.0584 - val_mae: 9.0885\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6214 - mse: 124.6214 - mae: 9.1505 - val_loss: 123.5125 - val_mse: 123.5125 - val_mae: 9.0857\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7395 - mse: 124.7395 - mae: 9.1438 - val_loss: 122.7303 - val_mse: 122.7303 - val_mae: 9.0920\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6194 - mse: 124.6194 - mae: 9.1463 - val_loss: 123.2303 - val_mse: 123.2303 - val_mae: 9.1577\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6437 - mse: 124.6437 - mae: 9.1532 - val_loss: 123.7189 - val_mse: 123.7189 - val_mae: 9.0871\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6854 - mse: 124.6854 - mae: 9.1419 - val_loss: 122.7425 - val_mse: 122.7425 - val_mae: 9.1128\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7022 - mse: 124.7022 - mae: 9.1543 - val_loss: 122.9203 - val_mse: 122.9203 - val_mae: 9.1346\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6921 - mse: 124.6921 - mae: 9.1483 - val_loss: 122.6953 - val_mse: 122.6953 - val_mae: 9.0929\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7082 - mse: 124.7082 - mae: 9.1521 - val_loss: 122.6832 - val_mse: 122.6832 - val_mae: 9.0962\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7851 - mse: 124.7851 - mae: 9.1522 - val_loss: 123.1106 - val_mse: 123.1106 - val_mae: 9.0881\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5549 - mse: 124.5549 - mae: 9.1385 - val_loss: 122.7580 - val_mse: 122.7580 - val_mae: 9.0915\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7026 - mse: 124.7026 - mae: 9.1482 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7066 - mse: 124.7066 - mae: 9.1439 - val_loss: 122.6967 - val_mse: 122.6967 - val_mae: 9.1020\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7726 - mse: 124.7726 - mae: 9.1541 - val_loss: 122.7263 - val_mse: 122.7263 - val_mae: 9.0921\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7454 - mse: 124.7454 - mae: 9.1508 - val_loss: 122.7597 - val_mse: 122.7597 - val_mae: 9.0915\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6259 - mse: 124.6259 - mae: 9.1559 - val_loss: 122.8333 - val_mse: 122.8333 - val_mae: 9.0905\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7732 - mse: 124.7732 - mae: 9.1487 - val_loss: 122.6893 - val_mse: 122.6893 - val_mae: 9.0993\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7308 - mse: 124.7308 - mae: 9.1530 - val_loss: 122.7134 - val_mse: 122.7134 - val_mae: 9.0924\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.7049 - mse: 124.7049 - mae: 9.1478 - val_loss: 122.7516 - val_mse: 122.7516 - val_mae: 9.1143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5655 - mse: 124.5655 - mae: 9.1434 - val_loss: 122.8496 - val_mse: 122.8496 - val_mae: 9.0903\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8080 - mse: 124.8080 - mae: 9.1511 - val_loss: 122.8608 - val_mse: 122.8608 - val_mae: 9.1287\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8766 - mse: 124.8766 - mae: 9.1544 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.7313 - mse: 124.7313 - mae: 9.1490 - val_loss: 122.9755 - val_mse: 122.9755 - val_mae: 9.0891\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7402 - mse: 124.7402 - mae: 9.1435 - val_loss: 122.6959 - val_mse: 122.6959 - val_mae: 9.1018\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7528 - mse: 124.7528 - mae: 9.1525 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6642 - mse: 124.6642 - mae: 9.1448 - val_loss: 122.7882 - val_mse: 122.7882 - val_mae: 9.0911\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6722 - mse: 124.6722 - mae: 9.1381 - val_loss: 122.9461 - val_mse: 122.9461 - val_mae: 9.1370\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7517 - mse: 124.7517 - mae: 9.1563 - val_loss: 122.6950 - val_mse: 122.6950 - val_mae: 9.0929\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7433 - mse: 124.7433 - mae: 9.1536 - val_loss: 122.7050 - val_mse: 122.7050 - val_mae: 9.0926\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8022 - mse: 124.8022 - mae: 9.1480 - val_loss: 122.6897 - val_mse: 122.6897 - val_mae: 9.0931\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7384 - mse: 124.7384 - mae: 9.1525 - val_loss: 123.9039 - val_mse: 123.9039 - val_mae: 9.1920\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5600 - mse: 124.5600 - mae: 9.1350 - val_loss: 122.8080 - val_mse: 122.8080 - val_mae: 9.0908\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6997 - mse: 124.6997 - mae: 9.1524 - val_loss: 122.9747 - val_mse: 122.9747 - val_mae: 9.0891\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6724 - mse: 124.6724 - mae: 9.1540 - val_loss: 122.6856 - val_mse: 122.6856 - val_mae: 9.0933\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7032 - mse: 124.7032 - mae: 9.1429 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0942\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7115 - mse: 124.7115 - mae: 9.1492 - val_loss: 123.4608 - val_mse: 123.4608 - val_mae: 9.0860\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.5577 - mse: 124.5577 - mae: 9.1320 - val_loss: 122.6818 - val_mse: 122.6818 - val_mae: 9.0935\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.6860 - mse: 124.6860 - mae: 9.1513 - val_loss: 122.8001 - val_mse: 122.8001 - val_mae: 9.0909\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6878 - mse: 124.6878 - mae: 9.1445 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6943 - mse: 124.6943 - mae: 9.1475 - val_loss: 122.9932 - val_mse: 122.9932 - val_mae: 9.1410\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.8391 - mse: 124.8391 - mae: 9.1601 - val_loss: 122.6927 - val_mse: 122.6927 - val_mae: 9.0930\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7137 - mse: 124.7137 - mae: 9.1554 - val_loss: 122.7295 - val_mse: 122.7295 - val_mae: 9.0920\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.7637 - mse: 124.7637 - mae: 9.1432 - val_loss: 122.7041 - val_mse: 122.7041 - val_mae: 9.0926\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8225 - mse: 124.8225 - mae: 9.1529 - val_loss: 122.7477 - val_mse: 122.7477 - val_mae: 9.0917\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4276 - mse: 124.4276 - mae: 9.1437 - val_loss: 122.6837 - val_mse: 122.6837 - val_mae: 9.0934\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6610 - mse: 124.6610 - mae: 9.1486 - val_loss: 122.7781 - val_mse: 122.7781 - val_mae: 9.1185\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6755 - mse: 124.6755 - mae: 9.1496 - val_loss: 124.0400 - val_mse: 124.0400 - val_mae: 9.0900\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7116 - mse: 124.7116 - mae: 9.1539 - val_loss: 122.6809 - val_mse: 122.6809 - val_mae: 9.0947\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7774 - mse: 124.7774 - mae: 9.1510 - val_loss: 123.3065 - val_mse: 123.3065 - val_mae: 9.0868\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5664 - mse: 124.5664 - mae: 9.1524 - val_loss: 123.2348 - val_mse: 123.2348 - val_mae: 9.0873\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7478 - mse: 124.7478 - mae: 9.1486 - val_loss: 122.7300 - val_mse: 122.7300 - val_mae: 9.1104\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5620 - mse: 124.5620 - mae: 9.1433 - val_loss: 124.1359 - val_mse: 124.1359 - val_mae: 9.0908\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5589 - mse: 124.5589 - mae: 9.1470 - val_loss: 122.9991 - val_mse: 122.9991 - val_mae: 9.0889\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7541 - mse: 124.7541 - mae: 9.1531 - val_loss: 122.7592 - val_mse: 122.7592 - val_mae: 9.0915\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6510 - mse: 124.6510 - mae: 9.1382 - val_loss: 122.7081 - val_mse: 122.7081 - val_mae: 9.0925\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7197 - mse: 124.7197 - mae: 9.1501 - val_loss: 122.6879 - val_mse: 122.6879 - val_mae: 9.0932\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.9775 - mse: 124.9775 - mae: 9.1646 - val_loss: 122.7019 - val_mse: 122.7019 - val_mae: 9.0927\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6635 - mse: 124.6635 - mae: 9.1516 - val_loss: 122.9148 - val_mse: 122.9148 - val_mae: 9.0897\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6125 - mse: 124.6125 - mae: 9.1494 - val_loss: 122.7949 - val_mse: 122.7949 - val_mae: 9.1208\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6286 - mse: 124.6286 - mae: 9.1461 - val_loss: 122.8605 - val_mse: 122.8605 - val_mae: 9.0902\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7178 - mse: 124.7178 - mae: 9.1483 - val_loss: 123.1913 - val_mse: 123.1913 - val_mae: 9.1552\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7985 - mse: 124.7985 - mae: 9.1610 - val_loss: 122.8221 - val_mse: 122.8221 - val_mae: 9.1243\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7332 - mse: 124.7332 - mae: 9.1467 - val_loss: 122.6848 - val_mse: 122.6848 - val_mae: 9.0971\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6946 - mse: 124.6946 - mae: 9.1517 - val_loss: 122.7448 - val_mse: 122.7448 - val_mae: 9.0918\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6629 - mse: 124.6629 - mae: 9.1442 - val_loss: 122.6881 - val_mse: 122.6881 - val_mae: 9.0932\n",
      "Epoch 68/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7514 - mse: 124.7514 - mae: 9.1442 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5318 - mse: 124.5318 - mae: 9.1350 - val_loss: 122.8654 - val_mse: 122.8654 - val_mae: 9.1292\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6682 - mse: 124.6682 - mae: 9.1507 - val_loss: 122.7193 - val_mse: 122.7193 - val_mae: 9.0923\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4645 - mse: 124.4645 - mae: 9.1444 - val_loss: 123.5853 - val_mse: 123.5853 - val_mae: 9.1769\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5357 - mse: 124.5357 - mae: 9.1396 - val_loss: 123.2457 - val_mse: 123.2457 - val_mae: 9.0872\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6628 - mse: 124.6628 - mae: 9.1480 - val_loss: 122.6887 - val_mse: 122.6887 - val_mae: 9.0932\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6961 - mse: 124.6961 - mae: 9.1457 - val_loss: 122.7247 - val_mse: 122.7247 - val_mae: 9.0921\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4940 - mse: 124.4940 - mae: 9.1458 - val_loss: 123.0397 - val_mse: 123.0397 - val_mae: 9.0886\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5934 - mse: 124.5934 - mae: 9.1401 - val_loss: 122.8014 - val_mse: 122.8014 - val_mae: 9.1217\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6354 - mse: 124.6354 - mae: 9.1440 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6099 - mse: 124.6099 - mae: 9.1425 - val_loss: 123.8363 - val_mse: 123.8363 - val_mae: 9.1886\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7638 - mse: 124.7638 - mae: 9.1524 - val_loss: 122.6818 - val_mse: 122.6818 - val_mae: 9.0953\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7775 - mse: 124.7775 - mae: 9.1534 - val_loss: 122.6979 - val_mse: 122.6979 - val_mae: 9.0928\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7045 - mse: 124.7045 - mae: 9.1494 - val_loss: 122.6829 - val_mse: 122.6829 - val_mae: 9.0960\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7257 - mse: 124.7257 - mae: 9.1440 - val_loss: 122.6854 - val_mse: 122.6854 - val_mae: 9.0933\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7078 - mse: 124.7078 - mae: 9.1543 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6894 - mse: 124.6894 - mae: 9.1388 - val_loss: 122.8468 - val_mse: 122.8468 - val_mae: 9.0904\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7786 - mse: 124.7786 - mae: 9.1525 - val_loss: 122.7025 - val_mse: 122.7025 - val_mae: 9.0927\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7566 - mse: 124.7566 - mae: 9.1536 - val_loss: 122.7612 - val_mse: 122.7612 - val_mae: 9.0915\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.6850 - mse: 124.6850 - mae: 9.1390 - val_loss: 122.8638 - val_mse: 122.8638 - val_mae: 9.0902\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6255 - mse: 124.6255 - mae: 9.1388 - val_loss: 122.6917 - val_mse: 122.6917 - val_mae: 9.0930\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4409 - mse: 124.4409 - mae: 9.1430 - val_loss: 123.1309 - val_mse: 123.1309 - val_mae: 9.0879\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 124.6059 - mse: 124.6059 - mae: 9.1535 - val_loss: 124.0621 - val_mse: 124.0621 - val_mae: 9.0902\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.7244 - mse: 124.7244 - mae: 9.1552 - val_loss: 122.6877 - val_mse: 122.6877 - val_mae: 9.0932\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.7046 - mse: 124.7046 - mae: 9.1462 - val_loss: 123.4455 - val_mse: 123.4455 - val_mae: 9.1699\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.7945 - mse: 124.7945 - mae: 9.1523 - val_loss: 122.7536 - val_mse: 122.7536 - val_mae: 9.1147\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 25s 180ms/step - loss: 124.6411 - mse: 124.6411 - mae: 9.1474 - val_loss: 122.7542 - val_mse: 122.7542 - val_mae: 9.1148\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.7719 - mse: 124.7719 - mae: 9.1483 - val_loss: 122.7741 - val_mse: 122.7741 - val_mae: 9.0913\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.7673 - mse: 124.7673 - mae: 9.1532 - val_loss: 122.7448 - val_mse: 122.7448 - val_mae: 9.0918\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.7556 - mse: 124.7556 - mae: 9.1497 - val_loss: 122.6903 - val_mse: 122.6903 - val_mae: 9.0931\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6337 - mse: 124.6337 - mae: 9.1518 - val_loss: 122.7691 - val_mse: 122.7691 - val_mae: 9.0914\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7189 - mse: 124.7189 - mae: 9.1475 - val_loss: 123.1337 - val_mse: 123.1337 - val_mae: 9.1514\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5326 - mse: 124.5326 - mae: 9.1517 - val_loss: 123.4649 - val_mse: 123.4649 - val_mae: 9.1709\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.8220 - mse: 124.8220 - mae: 9.1567 - val_loss: 122.7898 - val_mse: 122.7898 - val_mae: 9.0911\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6899 - mse: 124.6899 - mae: 9.1442 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0943\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.8571 - mse: 124.8571 - mae: 9.1530 - val_loss: 122.6866 - val_mse: 122.6866 - val_mae: 9.0933\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7158 - mse: 124.7158 - mae: 9.1457 - val_loss: 122.6891 - val_mse: 122.6891 - val_mae: 9.0931\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5693 - mse: 124.5693 - mae: 9.1358 - val_loss: 122.7647 - val_mse: 122.7647 - val_mae: 9.1165\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5225 - mse: 124.5225 - mae: 9.1333 - val_loss: 123.8260 - val_mse: 123.8260 - val_mae: 9.1881\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.9351 - mse: 124.9351 - mae: 9.1552 - val_loss: 122.7569 - val_mse: 122.7569 - val_mae: 9.0915\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6170 - mse: 124.6170 - mae: 9.1426 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6596 - mse: 124.6596 - mae: 9.1496 - val_loss: 122.7161 - val_mse: 122.7161 - val_mae: 9.1074\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5861 - mse: 124.5861 - mae: 9.1450 - val_loss: 122.7174 - val_mse: 122.7174 - val_mae: 9.0923\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6263 - mse: 124.6263 - mae: 9.1439 - val_loss: 122.9480 - val_mse: 122.9480 - val_mae: 9.0894\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5832 - mse: 124.5832 - mae: 9.1413 - val_loss: 122.7011 - val_mse: 122.7011 - val_mae: 9.1034\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6697 - mse: 124.6697 - mae: 9.1413 - val_loss: 123.1440 - val_mse: 123.1440 - val_mae: 9.1521\n",
      "Epoch 114/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7385 - mse: 124.7385 - mae: 9.1627 - val_loss: 123.2443 - val_mse: 123.2443 - val_mae: 9.0872\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6834 - mse: 124.6834 - mae: 9.1486 - val_loss: 122.8302 - val_mse: 122.8302 - val_mae: 9.0905\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6707 - mse: 124.6707 - mae: 9.1514 - val_loss: 122.8547 - val_mse: 122.8547 - val_mae: 9.0903\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8453 - mse: 124.8453 - mae: 9.1524 - val_loss: 122.9157 - val_mse: 122.9157 - val_mae: 9.1342\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6249 - mse: 124.6249 - mae: 9.1475 - val_loss: 122.8527 - val_mse: 122.8527 - val_mae: 9.0903\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7505 - mse: 124.7505 - mae: 9.1421 - val_loss: 122.8072 - val_mse: 122.8072 - val_mae: 9.1224\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6205 - mse: 124.6205 - mae: 9.1535 - val_loss: 122.9390 - val_mse: 122.9390 - val_mae: 9.0894\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6052 - mse: 124.6052 - mae: 9.1436 - val_loss: 122.6865 - val_mse: 122.6865 - val_mae: 9.0933\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6332 - mse: 124.6332 - mae: 9.1427 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6164 - mse: 124.6164 - mae: 9.1465 - val_loss: 122.7007 - val_mse: 122.7007 - val_mae: 9.1033\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5785 - mse: 124.5785 - mae: 9.1512 - val_loss: 123.0009 - val_mse: 123.0009 - val_mae: 9.0889\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6897 - mse: 124.6897 - mae: 9.1540 - val_loss: 122.6812 - val_mse: 122.6812 - val_mae: 9.0948\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6901 - mse: 124.6901 - mae: 9.1485 - val_loss: 122.7918 - val_mse: 122.7918 - val_mae: 9.0910\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5374 - mse: 124.5374 - mae: 9.1432 - val_loss: 122.7458 - val_mse: 122.7458 - val_mae: 9.1133\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.8438 - mse: 124.8438 - mae: 9.1595 - val_loss: 122.7500 - val_mse: 122.7500 - val_mae: 9.0917\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6965 - mse: 124.6965 - mae: 9.1491 - val_loss: 122.9419 - val_mse: 122.9419 - val_mae: 9.0894\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.6955 - mse: 124.6955 - mae: 9.1450 - val_loss: 122.7090 - val_mse: 122.7090 - val_mae: 9.0925\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7171 - mse: 124.7171 - mae: 9.1450 - val_loss: 122.7076 - val_mse: 122.7076 - val_mae: 9.1053\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.7147 - mse: 124.7147 - mae: 9.1527 - val_loss: 122.7809 - val_mse: 122.7809 - val_mae: 9.0912\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6944 - mse: 124.6944 - mae: 9.1440 - val_loss: 123.3083 - val_mse: 123.3083 - val_mae: 9.0868\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6921 - mse: 124.6921 - mae: 9.1473 - val_loss: 123.0583 - val_mse: 123.0583 - val_mae: 9.0885\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7072 - mse: 124.7072 - mae: 9.1545 - val_loss: 122.7513 - val_mse: 122.7513 - val_mae: 9.1143\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5496 - mse: 124.5496 - mae: 9.1464 - val_loss: 122.8430 - val_mse: 122.8430 - val_mae: 9.0904\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6993 - mse: 124.6993 - mae: 9.1482 - val_loss: 122.7144 - val_mse: 122.7144 - val_mae: 9.1070\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7980 - mse: 124.7980 - mae: 9.1495 - val_loss: 122.7246 - val_mse: 122.7246 - val_mae: 9.1093\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6770 - mse: 124.6770 - mae: 9.1476 - val_loss: 122.7076 - val_mse: 122.7076 - val_mae: 9.0925\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4966 - mse: 124.4966 - mae: 9.1408 - val_loss: 122.7492 - val_mse: 122.7492 - val_mae: 9.1139\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6928 - mse: 124.6928 - mae: 9.1493 - val_loss: 122.6945 - val_mse: 122.6945 - val_mae: 9.0929\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6159 - mse: 124.6159 - mae: 9.1498 - val_loss: 122.8758 - val_mse: 122.8758 - val_mae: 9.1303\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6396 - mse: 124.6396 - mae: 9.1431 - val_loss: 122.9452 - val_mse: 122.9452 - val_mae: 9.0894\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.6814 - mse: 124.6814 - mae: 9.1421 - val_loss: 122.7853 - val_mse: 122.7853 - val_mae: 9.1195\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.6060 - mse: 124.6060 - mae: 9.1553 - val_loss: 122.9885 - val_mse: 122.9885 - val_mae: 9.0890\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 30s 224ms/step - loss: 124.6901 - mse: 124.6901 - mae: 9.1464 - val_loss: 122.9416 - val_mse: 122.9416 - val_mae: 9.1366\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 124.7911 - mse: 124.7911 - mae: 9.1514 - val_loss: 122.7056 - val_mse: 122.7056 - val_mae: 9.1047\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 124.7393 - mse: 124.7393 - mae: 9.1546 - val_loss: 122.8708 - val_mse: 122.8708 - val_mae: 9.0901\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 30s 217ms/step - loss: 124.6496 - mse: 124.6496 - mae: 9.1457 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 24s 175ms/step - loss: 124.5655 - mse: 124.5655 - mae: 9.1429 - val_loss: 122.8794 - val_mse: 122.8794 - val_mae: 9.0900\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.5908 - mse: 124.5908 - mae: 9.1395 - val_loss: 123.1883 - val_mse: 123.1883 - val_mae: 9.1550\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7234 - mse: 124.7234 - mae: 9.1506 - val_loss: 122.9294 - val_mse: 122.9294 - val_mae: 9.0895\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.5187 - mse: 124.5187 - mae: 9.1359 - val_loss: 123.2394 - val_mse: 123.2394 - val_mae: 9.0872\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.7578 - mse: 124.7578 - mae: 9.1476 - val_loss: 122.9217 - val_mse: 122.9217 - val_mae: 9.0896\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7222 - mse: 124.7222 - mae: 9.1468 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4352 - mse: 124.4352 - mae: 9.1335 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0944\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6232 - mse: 124.6232 - mae: 9.1400 - val_loss: 122.7159 - val_mse: 122.7159 - val_mae: 9.0923\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5198 - mse: 124.5198 - mae: 9.1459 - val_loss: 122.8692 - val_mse: 122.8692 - val_mae: 9.0901\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7068 - mse: 124.7068 - mae: 9.1471 - val_loss: 122.7748 - val_mse: 122.7748 - val_mae: 9.1180\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7057 - mse: 124.7057 - mae: 9.1520 - val_loss: 122.7745 - val_mse: 122.7745 - val_mae: 9.0913\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6098 - mse: 124.6098 - mae: 9.1493 - val_loss: 122.6843 - val_mse: 122.6843 - val_mae: 9.0968\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5489 - mse: 124.5489 - mae: 9.1445 - val_loss: 122.9893 - val_mse: 122.9893 - val_mae: 9.1407\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5850 - mse: 124.5850 - mae: 9.1470 - val_loss: 123.0152 - val_mse: 123.0152 - val_mae: 9.1427\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.8362 - mse: 124.8362 - mae: 9.1574 - val_loss: 122.6851 - val_mse: 122.6851 - val_mae: 9.0933\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7281 - mse: 124.7281 - mae: 9.1465 - val_loss: 122.7108 - val_mse: 122.7108 - val_mae: 9.1061\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 124.5547 - mse: 124.5547 - mae: 9.1464 - val_loss: 122.7351 - val_mse: 122.7351 - val_mae: 9.1114\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7752 - mse: 124.7752 - mae: 9.1551 - val_loss: 122.6815 - val_mse: 122.6815 - val_mae: 9.0951\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4276 - mse: 124.4276 - mae: 9.1349 - val_loss: 122.7171 - val_mse: 122.7171 - val_mae: 9.0923\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7071 - mse: 124.7071 - mae: 9.1436 - val_loss: 122.6893 - val_mse: 122.6893 - val_mae: 9.0992\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.8072 - mse: 124.8072 - mae: 9.1504 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6936 - mse: 124.6936 - mae: 9.1532 - val_loss: 122.6837 - val_mse: 122.6837 - val_mae: 9.0965\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6518 - mse: 124.6518 - mae: 9.1476 - val_loss: 122.7633 - val_mse: 122.7633 - val_mae: 9.1162\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6759 - mse: 124.6759 - mae: 9.1500 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.7224 - mse: 124.7224 - mae: 9.1481 - val_loss: 122.7460 - val_mse: 122.7460 - val_mae: 9.0917\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 124.6499 - mse: 124.6499 - mae: 9.1492 - val_loss: 122.7950 - val_mse: 122.7950 - val_mae: 9.0910\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6789 - mse: 124.6789 - mae: 9.1475 - val_loss: 122.7760 - val_mse: 122.7760 - val_mae: 9.1182\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6594 - mse: 124.6594 - mae: 9.1461 - val_loss: 123.1841 - val_mse: 123.1841 - val_mae: 9.0876\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7052 - mse: 124.7052 - mae: 9.1459 - val_loss: 122.7133 - val_mse: 122.7133 - val_mae: 9.0924\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6577 - mse: 124.6577 - mae: 9.1471 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0945\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5839 - mse: 124.5839 - mae: 9.1414 - val_loss: 122.8924 - val_mse: 122.8924 - val_mae: 9.1319\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6015 - mse: 124.6015 - mae: 9.1457 - val_loss: 123.3690 - val_mse: 123.3690 - val_mae: 9.1658\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6025 - mse: 124.6025 - mae: 9.1560 - val_loss: 122.6959 - val_mse: 122.6959 - val_mae: 9.0929\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7165 - mse: 124.7165 - mae: 9.1493 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6411 - mse: 124.6411 - mae: 9.1471 - val_loss: 122.7782 - val_mse: 122.7782 - val_mae: 9.1185\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6329 - mse: 124.6329 - mae: 9.1518 - val_loss: 123.0582 - val_mse: 123.0582 - val_mae: 9.0885\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6850 - mse: 124.6850 - mae: 9.1454 - val_loss: 122.6820 - val_mse: 122.6820 - val_mae: 9.0935\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7331 - mse: 124.7331 - mae: 9.1483 - val_loss: 122.6945 - val_mse: 122.6945 - val_mae: 9.1013\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5402 - mse: 124.5402 - mae: 9.1427 - val_loss: 124.1020 - val_mse: 124.1020 - val_mae: 9.0906\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4461 - mse: 124.4461 - mae: 9.1373 - val_loss: 122.7338 - val_mse: 122.7338 - val_mae: 9.1111\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7068 - mse: 124.7068 - mae: 9.1489 - val_loss: 122.7599 - val_mse: 122.7599 - val_mae: 9.0915\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6337 - mse: 124.6337 - mae: 9.1436 - val_loss: 123.1138 - val_mse: 123.1138 - val_mae: 9.0881\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5484 - mse: 124.5484 - mae: 9.1462 - val_loss: 122.6814 - val_mse: 122.6814 - val_mae: 9.0950\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.2977 - mse: 124.2977 - mae: 9.1330 - val_loss: 122.7427 - val_mse: 122.7427 - val_mae: 9.1128\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6851 - mse: 124.6851 - mae: 9.1435 - val_loss: 122.8171 - val_mse: 122.8171 - val_mae: 9.1237\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6788 - mse: 124.6788 - mae: 9.1468 - val_loss: 122.7432 - val_mse: 122.7432 - val_mae: 9.1129\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6454 - mse: 124.6454 - mae: 9.1480 - val_loss: 122.6865 - val_mse: 122.6865 - val_mae: 9.0980\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6302 - mse: 124.6302 - mae: 9.1466 - val_loss: 122.7074 - val_mse: 122.7074 - val_mae: 9.0926\n",
      "Epoch 198/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 124.6171 - mse: 124.6171 - mae: 9.1445 - val_loss: 123.0174 - val_mse: 123.0174 - val_mae: 9.0888\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.6808 - mse: 124.6808 - mae: 9.1466 - val_loss: 123.0091 - val_mse: 123.0091 - val_mae: 9.0888\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6146 - mse: 124.6146 - mae: 9.1446 - val_loss: 122.7013 - val_mse: 122.7013 - val_mae: 9.0927\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5967 - mse: 124.5967 - mae: 9.1405 - val_loss: 122.6984 - val_mse: 122.6984 - val_mae: 9.1026\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6659 - mse: 124.6659 - mae: 9.1500 - val_loss: 122.6957 - val_mse: 122.6957 - val_mae: 9.0929\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.6759 - mse: 124.6759 - mae: 9.1406 - val_loss: 122.6828 - val_mse: 122.6828 - val_mae: 9.0959\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6727 - mse: 124.6727 - mae: 9.1484 - val_loss: 122.9077 - val_mse: 122.9077 - val_mae: 9.0897\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6365 - mse: 124.6365 - mae: 9.1405 - val_loss: 123.2637 - val_mse: 123.2637 - val_mae: 9.1597\n",
      "Epoch 206/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6739 - mse: 124.6739 - mae: 9.1496 - val_loss: 123.0156 - val_mse: 123.0156 - val_mae: 9.1428\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6699 - mse: 124.6699 - mae: 9.1506 - val_loss: 122.7082 - val_mse: 122.7082 - val_mae: 9.0925\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6563 - mse: 124.6563 - mae: 9.1500 - val_loss: 122.9887 - val_mse: 122.9887 - val_mae: 9.0890\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5243 - mse: 124.5243 - mae: 9.1441 - val_loss: 123.3325 - val_mse: 123.3325 - val_mae: 9.0867\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7790 - mse: 124.7790 - mae: 9.1471 - val_loss: 122.6856 - val_mse: 122.6856 - val_mae: 9.0976\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7525 - mse: 124.7525 - mae: 9.1526 - val_loss: 122.9515 - val_mse: 122.9515 - val_mae: 9.0893\n",
      "Epoch 212/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5351 - mse: 124.5351 - mae: 9.1484 - val_loss: 122.9460 - val_mse: 122.9460 - val_mae: 9.0894\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.8184 - mse: 124.8184 - mae: 9.1527 - val_loss: 122.6992 - val_mse: 122.6992 - val_mae: 9.1028\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6578 - mse: 124.6578 - mae: 9.1503 - val_loss: 122.8415 - val_mse: 122.8415 - val_mae: 9.0904\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4415 - mse: 124.4415 - mae: 9.1453 - val_loss: 122.7012 - val_mse: 122.7012 - val_mae: 9.0927\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 124.6603 - mse: 124.6603 - mae: 9.1473 - val_loss: 123.0960 - val_mse: 123.0960 - val_mae: 9.0882\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6672 - mse: 124.6672 - mae: 9.1490 - val_loss: 122.6974 - val_mse: 122.6974 - val_mae: 9.1023\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6886 - mse: 124.6886 - mae: 9.1466 - val_loss: 122.7479 - val_mse: 122.7479 - val_mae: 9.1137\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8071 - mse: 124.8071 - mae: 9.1590 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 220/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6842 - mse: 124.6842 - mae: 9.1480 - val_loss: 122.7133 - val_mse: 122.7133 - val_mae: 9.0924\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6058 - mse: 124.6058 - mae: 9.1467 - val_loss: 122.8902 - val_mse: 122.8902 - val_mae: 9.1317\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5974 - mse: 124.5974 - mae: 9.1440 - val_loss: 122.6840 - val_mse: 122.6840 - val_mae: 9.0967\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.7327 - mse: 124.7327 - mae: 9.1466 - val_loss: 122.7513 - val_mse: 122.7513 - val_mae: 9.1143\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7309 - mse: 124.7309 - mae: 9.1566 - val_loss: 122.6836 - val_mse: 122.6836 - val_mae: 9.0964\n",
      "Epoch 225/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5997 - mse: 124.5997 - mae: 9.1363 - val_loss: 122.7638 - val_mse: 122.7638 - val_mae: 9.1163\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6801 - mse: 124.6801 - mae: 9.1521 - val_loss: 123.7406 - val_mse: 123.7406 - val_mae: 9.0873\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6774 - mse: 124.6774 - mae: 9.1459 - val_loss: 123.4512 - val_mse: 123.4512 - val_mae: 9.1702\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7465 - mse: 124.7465 - mae: 9.1551 - val_loss: 122.8852 - val_mse: 122.8852 - val_mae: 9.1312\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5723 - mse: 124.5723 - mae: 9.1485 - val_loss: 122.6845 - val_mse: 122.6845 - val_mae: 9.0970\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6692 - mse: 124.6692 - mae: 9.1512 - val_loss: 122.7242 - val_mse: 122.7242 - val_mae: 9.0922\n",
      "Epoch 231/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.7530 - mse: 124.7530 - mae: 9.1512 - val_loss: 122.7264 - val_mse: 122.7264 - val_mae: 9.0921\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7339 - mse: 124.7339 - mae: 9.1491 - val_loss: 122.6916 - val_mse: 122.6916 - val_mae: 9.1002\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7882 - mse: 124.7882 - mae: 9.1488 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4488 - mse: 124.4488 - mae: 9.1374 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6632 - mse: 124.6632 - mae: 9.1484 - val_loss: 122.9383 - val_mse: 122.9383 - val_mae: 9.0894\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5630 - mse: 124.5630 - mae: 9.1522 - val_loss: 122.9429 - val_mse: 122.9429 - val_mae: 9.0894\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7461 - mse: 124.7461 - mae: 9.1517 - val_loss: 122.6821 - val_mse: 122.6821 - val_mae: 9.0935\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7348 - mse: 124.7348 - mae: 9.1438 - val_loss: 122.7065 - val_mse: 122.7065 - val_mae: 9.1049\n",
      "Epoch 239/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7477 - mse: 124.7477 - mae: 9.1490 - val_loss: 122.8182 - val_mse: 122.8182 - val_mae: 9.1238\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5646 - mse: 124.5646 - mae: 9.1392 - val_loss: 122.7949 - val_mse: 122.7949 - val_mae: 9.0910\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5647 - mse: 124.5647 - mae: 9.1442 - val_loss: 122.6878 - val_mse: 122.6878 - val_mae: 9.0986\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7480 - mse: 124.7480 - mae: 9.1509 - val_loss: 122.8865 - val_mse: 122.8865 - val_mae: 9.1313\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6565 - mse: 124.6565 - mae: 9.1473 - val_loss: 122.8681 - val_mse: 122.8681 - val_mae: 9.0901\n",
      "Epoch 244/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7251 - mse: 124.7251 - mae: 9.1465 - val_loss: 122.7390 - val_mse: 122.7390 - val_mae: 9.0919\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6596 - mse: 124.6596 - mae: 9.1478 - val_loss: 122.8291 - val_mse: 122.8291 - val_mae: 9.0906\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.7237 - mse: 124.7237 - mae: 9.1502 - val_loss: 122.8141 - val_mse: 122.8141 - val_mae: 9.0907\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.8198 - mse: 124.8198 - mae: 9.1528 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0945\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6148 - mse: 124.6148 - mae: 9.1546 - val_loss: 123.2549 - val_mse: 123.2549 - val_mae: 9.0872\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6591 - mse: 124.6591 - mae: 9.1477 - val_loss: 122.7115 - val_mse: 122.7115 - val_mae: 9.0924\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.5333 - mse: 124.5333 - mae: 9.1382 - val_loss: 123.1231 - val_mse: 123.1231 - val_mae: 9.0880\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 108s 797ms/step - loss: 124.6108 - mse: 124.6108 - mae: 9.1463 - val_loss: 123.6629 - val_mse: 123.6629 - val_mae: 9.1806\n",
      "Epoch 252/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 50s 362ms/step - loss: 124.7280 - mse: 124.7280 - mae: 9.1493 - val_loss: 122.8171 - val_mse: 122.8171 - val_mae: 9.1237\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 30s 223ms/step - loss: 124.6598 - mse: 124.6598 - mae: 9.1509 - val_loss: 123.4731 - val_mse: 123.4731 - val_mae: 9.0859\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 31s 226ms/step - loss: 124.7312 - mse: 124.7312 - mae: 9.1549 - val_loss: 123.1375 - val_mse: 123.1375 - val_mae: 9.0879\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 30s 222ms/step - loss: 124.8077 - mse: 124.8077 - mae: 9.1507 - val_loss: 122.7037 - val_mse: 122.7037 - val_mae: 9.1042\n",
      "Epoch 256/300\n",
      "136/136 [==============================] - 30s 222ms/step - loss: 124.6773 - mse: 124.6773 - mae: 9.1487 - val_loss: 122.7386 - val_mse: 122.7386 - val_mae: 9.0919\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 30s 223ms/step - loss: 124.6179 - mse: 124.6179 - mae: 9.1377 - val_loss: 122.8626 - val_mse: 122.8626 - val_mae: 9.1289\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 124.7226 - mse: 124.7226 - mae: 9.1477 - val_loss: 122.7421 - val_mse: 122.7421 - val_mae: 9.1127\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 124.7151 - mse: 124.7151 - mae: 9.1491 - val_loss: 122.8391 - val_mse: 122.8391 - val_mae: 9.1263\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.6670 - mse: 124.6670 - mae: 9.1524 - val_loss: 122.6945 - val_mse: 122.6945 - val_mae: 9.0929\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 25s 185ms/step - loss: 124.7972 - mse: 124.7972 - mae: 9.1453 - val_loss: 122.6919 - val_mse: 122.6919 - val_mae: 9.1003\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 33s 246ms/step - loss: 124.6968 - mse: 124.6968 - mae: 9.1430 - val_loss: 122.7385 - val_mse: 122.7385 - val_mae: 9.0919\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 33s 241ms/step - loss: 124.6612 - mse: 124.6612 - mae: 9.1435 - val_loss: 123.3100 - val_mse: 123.3100 - val_mae: 9.1624\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.6091 - mse: 124.6091 - mae: 9.1551 - val_loss: 122.8245 - val_mse: 122.8245 - val_mae: 9.0906\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6256 - mse: 124.6256 - mae: 9.1476 - val_loss: 122.8028 - val_mse: 122.8028 - val_mae: 9.1219\n",
      "Epoch 266/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.6445 - mse: 124.6445 - mae: 9.1479 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6683 - mse: 124.6683 - mae: 9.1459 - val_loss: 122.7260 - val_mse: 122.7260 - val_mae: 9.0921\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7708 - mse: 124.7708 - mae: 9.1524 - val_loss: 122.7067 - val_mse: 122.7067 - val_mae: 9.1050\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7065 - mse: 124.7065 - mae: 9.1511 - val_loss: 122.6828 - val_mse: 122.6828 - val_mae: 9.0959\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6356 - mse: 124.6356 - mae: 9.1502 - val_loss: 123.2983 - val_mse: 123.2983 - val_mae: 9.0869\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6469 - mse: 124.6469 - mae: 9.1488 - val_loss: 123.0301 - val_mse: 123.0301 - val_mae: 9.1439\n",
      "Epoch 272/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.7066 - mse: 124.7066 - mae: 9.1533 - val_loss: 122.8995 - val_mse: 122.8995 - val_mae: 9.0898\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6513 - mse: 124.6513 - mae: 9.1421 - val_loss: 123.0321 - val_mse: 123.0321 - val_mae: 9.1441\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7772 - mse: 124.7772 - mae: 9.1573 - val_loss: 122.7438 - val_mse: 122.7438 - val_mae: 9.1130\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5436 - mse: 124.5436 - mae: 9.1472 - val_loss: 123.6329 - val_mse: 123.6329 - val_mae: 9.1792\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6622 - mse: 124.6622 - mae: 9.1513 - val_loss: 122.6869 - val_mse: 122.6869 - val_mae: 9.0933\n",
      "Epoch 277/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7324 - mse: 124.7324 - mae: 9.1600 - val_loss: 123.6232 - val_mse: 123.6232 - val_mae: 9.0861\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6693 - mse: 124.6693 - mae: 9.1414 - val_loss: 122.6881 - val_mse: 122.6881 - val_mae: 9.0987\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5869 - mse: 124.5869 - mae: 9.1513 - val_loss: 123.0162 - val_mse: 123.0162 - val_mae: 9.0888\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6174 - mse: 124.6174 - mae: 9.1454 - val_loss: 122.7114 - val_mse: 122.7114 - val_mae: 9.1062\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6941 - mse: 124.6941 - mae: 9.1512 - val_loss: 122.8377 - val_mse: 122.8377 - val_mae: 9.1261\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5817 - mse: 124.5817 - mae: 9.1452 - val_loss: 122.7049 - val_mse: 122.7049 - val_mae: 9.1045\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7043 - mse: 124.7043 - mae: 9.1497 - val_loss: 122.7058 - val_mse: 122.7058 - val_mae: 9.0926\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6955 - mse: 124.6955 - mae: 9.1437 - val_loss: 122.9918 - val_mse: 122.9918 - val_mae: 9.0890\n",
      "Epoch 285/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6758 - mse: 124.6758 - mae: 9.1545 - val_loss: 123.0813 - val_mse: 123.0813 - val_mae: 9.0883\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.6352 - mse: 124.6352 - mae: 9.1497 - val_loss: 122.6933 - val_mse: 122.6933 - val_mae: 9.1008\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6460 - mse: 124.6460 - mae: 9.1418 - val_loss: 122.7003 - val_mse: 122.7003 - val_mae: 9.0928\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.6968 - mse: 124.6968 - mae: 9.1505 - val_loss: 122.7925 - val_mse: 122.7925 - val_mae: 9.0910\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7159 - mse: 124.7159 - mae: 9.1553 - val_loss: 122.6998 - val_mse: 122.6998 - val_mae: 9.1030\n",
      "Epoch 290/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6085 - mse: 124.6085 - mae: 9.1474 - val_loss: 123.3525 - val_mse: 123.3525 - val_mae: 9.0866\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7399 - mse: 124.7399 - mae: 9.1514 - val_loss: 122.6892 - val_mse: 122.6892 - val_mae: 9.0992\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7394 - mse: 124.7394 - mae: 9.1526 - val_loss: 122.6839 - val_mse: 122.6839 - val_mae: 9.0966\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5834 - mse: 124.5834 - mae: 9.1389 - val_loss: 122.7128 - val_mse: 122.7128 - val_mae: 9.1066\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 124.7676 - mse: 124.7676 - mae: 9.1512 - val_loss: 122.6813 - val_mse: 122.6813 - val_mae: 9.0936\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6156 - mse: 124.6156 - mae: 9.1465 - val_loss: 123.4630 - val_mse: 123.4630 - val_mae: 9.0860\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7394 - mse: 124.7394 - mae: 9.1542 - val_loss: 122.7702 - val_mse: 122.7702 - val_mae: 9.0913\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4034 - mse: 124.4034 - mae: 9.1327 - val_loss: 122.9705 - val_mse: 122.9705 - val_mae: 9.1391\n",
      "Epoch 298/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6541 - mse: 124.6541 - mae: 9.1490 - val_loss: 122.6823 - val_mse: 122.6823 - val_mae: 9.0956\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6296 - mse: 124.6296 - mae: 9.1510 - val_loss: 122.9508 - val_mse: 122.9508 - val_mae: 9.1374\n",
      "Epoch 300/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.7113 - mse: 124.7113 - mae: 9.1518 - val_loss: 122.7121 - val_mse: 122.7121 - val_mae: 9.1064\n",
      "57/57 [==============================] - 4s 38ms/step - loss: 134.0080 - mse: 134.0080 - mae: 9.4613\n",
      "57/57 [==============================] - 3s 38ms/step\n",
      "MAE: 9.461255115102945\n",
      "MSE: 134.00806106536191\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 60, 240)          173760    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 120)              144480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 28s 159ms/step - loss: 5060.4668 - mse: 5060.4668 - mae: 70.1186 - val_loss: 4365.7002 - val_mse: 4365.7002 - val_mae: 65.1385\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 4210.0273 - mse: 4210.0273 - mae: 63.9146 - val_loss: 4078.7808 - val_mse: 4078.7808 - val_mae: 62.8976\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 4018.0745 - mse: 4018.0745 - mae: 62.3984 - val_loss: 3947.0989 - val_mse: 3947.0989 - val_mae: 61.8419\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3912.4390 - mse: 3912.4390 - mae: 61.5479 - val_loss: 3863.0229 - val_mse: 3863.0229 - val_mae: 61.1584\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3839.3262 - mse: 3839.3262 - mae: 60.9498 - val_loss: 3799.5493 - val_mse: 3799.5493 - val_mae: 60.6372\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3781.1196 - mse: 3781.1196 - mae: 60.4702 - val_loss: 3747.5190 - val_mse: 3747.5190 - val_mae: 60.2067\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3733.0117 - mse: 3733.0117 - mae: 60.0716 - val_loss: 3703.3464 - val_mse: 3703.3464 - val_mae: 59.8387\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3690.9785 - mse: 3690.9785 - mae: 59.7205 - val_loss: 3663.7075 - val_mse: 3663.7075 - val_mae: 59.5065\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3653.1902 - mse: 3653.1902 - mae: 59.4032 - val_loss: 3628.5271 - val_mse: 3628.5271 - val_mae: 59.2102\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3619.6975 - mse: 3619.6975 - mae: 59.1212 - val_loss: 3596.9341 - val_mse: 3596.9341 - val_mae: 58.9428\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3589.2808 - mse: 3589.2808 - mae: 58.8628 - val_loss: 3567.8706 - val_mse: 3567.8706 - val_mae: 58.6958\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3560.9878 - mse: 3560.9878 - mae: 58.6224 - val_loss: 3540.8340 - val_mse: 3540.8340 - val_mae: 58.4650\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3534.7324 - mse: 3534.7324 - mae: 58.3983 - val_loss: 3515.4607 - val_mse: 3515.4607 - val_mae: 58.2476\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3509.8960 - mse: 3509.8960 - mae: 58.1851 - val_loss: 3491.5083 - val_mse: 3491.5083 - val_mae: 58.0416\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3486.4961 - mse: 3486.4961 - mae: 57.9834 - val_loss: 3468.7593 - val_mse: 3468.7593 - val_mae: 57.8454\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3464.1436 - mse: 3464.1436 - mae: 57.7907 - val_loss: 3446.8032 - val_mse: 3446.8032 - val_mae: 57.6552\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 3440.9324 - mse: 3440.9324 - mae: 57.5894 - val_loss: 3422.7825 - val_mse: 3422.7825 - val_mae: 57.4466\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3416.9709 - mse: 3416.9709 - mae: 57.3810 - val_loss: 3398.6777 - val_mse: 3398.6777 - val_mae: 57.2364\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3393.5916 - mse: 3393.5916 - mae: 57.1766 - val_loss: 3376.5999 - val_mse: 3376.5999 - val_mae: 57.0431\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3372.4458 - mse: 3372.4458 - mae: 56.9918 - val_loss: 3356.3743 - val_mse: 3356.3743 - val_mae: 56.8656\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3352.8254 - mse: 3352.8254 - mae: 56.8191 - val_loss: 3337.3323 - val_mse: 3337.3323 - val_mae: 56.6979\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3334.1519 - mse: 3334.1519 - mae: 56.6545 - val_loss: 3319.1562 - val_mse: 3319.1562 - val_mae: 56.5374\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 3316.2515 - mse: 3316.2515 - mae: 56.4962 - val_loss: 3301.6787 - val_mse: 3301.6787 - val_mae: 56.3826\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3299.0635 - mse: 3299.0635 - mae: 56.3439 - val_loss: 3284.7915 - val_mse: 3284.7915 - val_mae: 56.2327\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3282.3875 - mse: 3282.3875 - mae: 56.1958 - val_loss: 3268.4255 - val_mse: 3268.4255 - val_mae: 56.0870\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3266.2249 - mse: 3266.2249 - mae: 56.0517 - val_loss: 3252.5195 - val_mse: 3252.5195 - val_mae: 55.9450\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3250.4490 - mse: 3250.4490 - mae: 55.9111 - val_loss: 3236.9963 - val_mse: 3236.9963 - val_mae: 55.8061\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3234.9548 - mse: 3234.9548 - mae: 55.7722 - val_loss: 3221.5222 - val_mse: 3221.5222 - val_mae: 55.6673\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3219.2512 - mse: 3219.2512 - mae: 55.6313 - val_loss: 3205.6616 - val_mse: 3205.6616 - val_mae: 55.5246\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3203.6206 - mse: 3203.6206 - mae: 55.4903 - val_loss: 3190.3250 - val_mse: 3190.3250 - val_mae: 55.3864\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3188.5549 - mse: 3188.5549 - mae: 55.3549 - val_loss: 3175.5776 - val_mse: 3175.5776 - val_mae: 55.2531\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3173.9802 - mse: 3173.9802 - mae: 55.2230 - val_loss: 3161.3101 - val_mse: 3161.3101 - val_mae: 55.1238\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3159.9207 - mse: 3159.9207 - mae: 55.0951 - val_loss: 3147.4329 - val_mse: 3147.4329 - val_mae: 54.9978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3146.1934 - mse: 3146.1934 - mae: 54.9708 - val_loss: 3133.8936 - val_mse: 3133.8936 - val_mae: 54.8745\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3132.7844 - mse: 3132.7844 - mae: 54.8487 - val_loss: 3120.6577 - val_mse: 3120.6577 - val_mae: 54.7538\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3119.6221 - mse: 3119.6221 - mae: 54.7287 - val_loss: 3107.7000 - val_mse: 3107.7000 - val_mae: 54.6353\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3106.7607 - mse: 3106.7607 - mae: 54.6108 - val_loss: 3095.0012 - val_mse: 3095.0012 - val_mae: 54.5190\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3094.1660 - mse: 3094.1660 - mae: 54.4956 - val_loss: 3082.5435 - val_mse: 3082.5435 - val_mae: 54.4046\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3081.8105 - mse: 3081.8105 - mae: 54.3819 - val_loss: 3070.3130 - val_mse: 3070.3130 - val_mae: 54.2921\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3069.6892 - mse: 3069.6892 - mae: 54.2705 - val_loss: 3058.2983 - val_mse: 3058.2983 - val_mae: 54.1814\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3057.7358 - mse: 3057.7358 - mae: 54.1603 - val_loss: 3046.4871 - val_mse: 3046.4871 - val_mae: 54.0723\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3046.0122 - mse: 3046.0122 - mae: 54.0517 - val_loss: 3034.8728 - val_mse: 3034.8728 - val_mae: 53.9648\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 3034.4883 - mse: 3034.4883 - mae: 53.9450 - val_loss: 3023.4443 - val_mse: 3023.4443 - val_mae: 53.8588\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3023.1138 - mse: 3023.1138 - mae: 53.8397 - val_loss: 3012.1941 - val_mse: 3012.1941 - val_mae: 53.7542\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3011.9343 - mse: 3011.9343 - mae: 53.7358 - val_loss: 3001.1152 - val_mse: 3001.1152 - val_mae: 53.6511\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3000.9075 - mse: 3000.9075 - mae: 53.6330 - val_loss: 2990.2012 - val_mse: 2990.2012 - val_mae: 53.5492\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2990.0759 - mse: 2990.0759 - mae: 53.5321 - val_loss: 2979.4448 - val_mse: 2979.4448 - val_mae: 53.4487\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2979.3882 - mse: 2979.3882 - mae: 53.4319 - val_loss: 2968.8418 - val_mse: 2968.8418 - val_mae: 53.3494\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2968.8391 - mse: 2968.8391 - mae: 53.3333 - val_loss: 2958.3845 - val_mse: 2958.3845 - val_mae: 53.2514\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2958.4436 - mse: 2958.4436 - mae: 53.2358 - val_loss: 2948.0693 - val_mse: 2948.0693 - val_mae: 53.1544\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2948.1594 - mse: 2948.1594 - mae: 53.1390 - val_loss: 2937.8916 - val_mse: 2937.8916 - val_mae: 53.0586\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2938.0564 - mse: 2938.0564 - mae: 53.0439 - val_loss: 2927.8457 - val_mse: 2927.8457 - val_mae: 52.9638\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 2928.0464 - mse: 2928.0464 - mae: 52.9494 - val_loss: 2917.9285 - val_mse: 2917.9285 - val_mae: 52.8701\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 2918.1970 - mse: 2918.1970 - mae: 52.8564 - val_loss: 2908.1343 - val_mse: 2908.1343 - val_mae: 52.7774\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2908.4478 - mse: 2908.4478 - mae: 52.7641 - val_loss: 2898.4614 - val_mse: 2898.4614 - val_mae: 52.6857\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2898.8230 - mse: 2898.8230 - mae: 52.6727 - val_loss: 2888.9043 - val_mse: 2888.9043 - val_mae: 52.5949\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2889.2930 - mse: 2889.2930 - mae: 52.5822 - val_loss: 2879.4614 - val_mse: 2879.4614 - val_mae: 52.5051\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2879.9192 - mse: 2879.9192 - mae: 52.4930 - val_loss: 2870.1282 - val_mse: 2870.1282 - val_mae: 52.4161\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2870.6272 - mse: 2870.6272 - mae: 52.4044 - val_loss: 2860.9006 - val_mse: 2860.9006 - val_mae: 52.3280\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2861.4275 - mse: 2861.4275 - mae: 52.3166 - val_loss: 2851.7786 - val_mse: 2851.7786 - val_mae: 52.2408\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2852.3611 - mse: 2852.3611 - mae: 52.2296 - val_loss: 2842.7578 - val_mse: 2842.7578 - val_mae: 52.1544\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2843.3625 - mse: 2843.3625 - mae: 52.1437 - val_loss: 2833.8340 - val_mse: 2833.8340 - val_mae: 52.0688\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2834.4985 - mse: 2834.4985 - mae: 52.0584 - val_loss: 2825.0071 - val_mse: 2825.0071 - val_mae: 51.9839\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2825.6968 - mse: 2825.6968 - mae: 51.9739 - val_loss: 2816.2729 - val_mse: 2816.2729 - val_mae: 51.8999\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2816.9980 - mse: 2816.9980 - mae: 51.8902 - val_loss: 2807.6304 - val_mse: 2807.6304 - val_mae: 51.8165\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2808.3892 - mse: 2808.3892 - mae: 51.8071 - val_loss: 2799.0762 - val_mse: 2799.0762 - val_mae: 51.7339\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2799.8545 - mse: 2799.8545 - mae: 51.7246 - val_loss: 2790.6096 - val_mse: 2790.6096 - val_mae: 51.6520\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2791.4373 - mse: 2791.4373 - mae: 51.6433 - val_loss: 2782.2268 - val_mse: 2782.2268 - val_mae: 51.5708\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2783.0911 - mse: 2783.0911 - mae: 51.5622 - val_loss: 2773.9265 - val_mse: 2773.9265 - val_mae: 51.4903\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2774.8196 - mse: 2774.8196 - mae: 51.4823 - val_loss: 2765.7068 - val_mse: 2765.7068 - val_mae: 51.4104\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2766.6321 - mse: 2766.6321 - mae: 51.4024 - val_loss: 2757.5664 - val_mse: 2757.5664 - val_mae: 51.3312\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2758.5198 - mse: 2758.5198 - mae: 51.3236 - val_loss: 2749.5024 - val_mse: 2749.5024 - val_mae: 51.2526\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2750.4966 - mse: 2750.4966 - mae: 51.2453 - val_loss: 2741.5137 - val_mse: 2741.5137 - val_mae: 51.1746\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2742.5156 - mse: 2742.5156 - mae: 51.1675 - val_loss: 2733.5989 - val_mse: 2733.5989 - val_mae: 51.0972\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2734.6399 - mse: 2734.6399 - mae: 51.0905 - val_loss: 2725.7561 - val_mse: 2725.7561 - val_mae: 51.0204\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2726.8174 - mse: 2726.8174 - mae: 51.0137 - val_loss: 2717.9841 - val_mse: 2717.9841 - val_mae: 50.9441\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2719.0852 - mse: 2719.0852 - mae: 50.9379 - val_loss: 2710.2803 - val_mse: 2710.2803 - val_mae: 50.8685\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2711.3982 - mse: 2711.3982 - mae: 50.8623 - val_loss: 2702.6458 - val_mse: 2702.6458 - val_mae: 50.7934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2703.7871 - mse: 2703.7871 - mae: 50.7876 - val_loss: 2695.0759 - val_mse: 2695.0759 - val_mae: 50.7188\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2696.2432 - mse: 2696.2432 - mae: 50.7133 - val_loss: 2687.5718 - val_mse: 2687.5718 - val_mae: 50.6448\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2688.7603 - mse: 2688.7603 - mae: 50.6395 - val_loss: 2680.1306 - val_mse: 2680.1306 - val_mae: 50.5712\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 2681.3562 - mse: 2681.3562 - mae: 50.5664 - val_loss: 2672.7527 - val_mse: 2672.7527 - val_mae: 50.4982\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2674.0029 - mse: 2674.0029 - mae: 50.4935 - val_loss: 2665.4353 - val_mse: 2665.4353 - val_mae: 50.4258\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2666.6975 - mse: 2666.6975 - mae: 50.4212 - val_loss: 2658.1785 - val_mse: 2658.1785 - val_mae: 50.3538\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2659.4604 - mse: 2659.4604 - mae: 50.3494 - val_loss: 2650.9795 - val_mse: 2650.9795 - val_mae: 50.2822\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2652.2974 - mse: 2652.2974 - mae: 50.2781 - val_loss: 2643.8394 - val_mse: 2643.8394 - val_mae: 50.2112\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2645.1699 - mse: 2645.1699 - mae: 50.2073 - val_loss: 2636.7559 - val_mse: 2636.7559 - val_mae: 50.1406\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2638.1082 - mse: 2638.1082 - mae: 50.1367 - val_loss: 2629.7283 - val_mse: 2629.7283 - val_mae: 50.0705\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 2631.1050 - mse: 2631.1050 - mae: 50.0670 - val_loss: 2622.7551 - val_mse: 2622.7551 - val_mae: 50.0008\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2624.1509 - mse: 2624.1509 - mae: 49.9976 - val_loss: 2615.8342 - val_mse: 2615.8342 - val_mae: 49.9315\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2617.2483 - mse: 2617.2483 - mae: 49.9284 - val_loss: 2608.9688 - val_mse: 2608.9688 - val_mae: 49.8627\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2610.4026 - mse: 2610.4026 - mae: 49.8597 - val_loss: 2602.1541 - val_mse: 2602.1541 - val_mae: 49.7943\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2603.6060 - mse: 2603.6060 - mae: 49.7915 - val_loss: 2595.3901 - val_mse: 2595.3901 - val_mae: 49.7264\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2596.8552 - mse: 2596.8552 - mae: 49.7238 - val_loss: 2588.6768 - val_mse: 2588.6768 - val_mae: 49.6588\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2590.1750 - mse: 2590.1750 - mae: 49.6565 - val_loss: 2582.0127 - val_mse: 2582.0127 - val_mae: 49.5917\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2583.5215 - mse: 2583.5215 - mae: 49.5895 - val_loss: 2575.3965 - val_mse: 2575.3965 - val_mae: 49.5249\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2576.9290 - mse: 2576.9290 - mae: 49.5230 - val_loss: 2568.8293 - val_mse: 2568.8293 - val_mae: 49.4586\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 39s 285ms/step - loss: 2570.3733 - mse: 2570.3733 - mae: 49.4568 - val_loss: 2562.3081 - val_mse: 2562.3081 - val_mae: 49.3926\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 20s 144ms/step - loss: 2563.8640 - mse: 2563.8640 - mae: 49.3908 - val_loss: 2555.8335 - val_mse: 2555.8335 - val_mae: 49.3270\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2557.4114 - mse: 2557.4114 - mae: 49.3253 - val_loss: 2549.4043 - val_mse: 2549.4043 - val_mae: 49.2618\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2550.9980 - mse: 2550.9980 - mae: 49.2605 - val_loss: 2543.0198 - val_mse: 2543.0198 - val_mae: 49.1970\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 30s 223ms/step - loss: 2544.6379 - mse: 2544.6379 - mae: 49.1958 - val_loss: 2536.6792 - val_mse: 2536.6792 - val_mae: 49.1325\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 2538.3142 - mse: 2538.3142 - mae: 49.1315 - val_loss: 2530.3823 - val_mse: 2530.3823 - val_mae: 49.0684\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2532.0242 - mse: 2532.0242 - mae: 49.0674 - val_loss: 2524.1282 - val_mse: 2524.1282 - val_mae: 49.0046\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2525.7837 - mse: 2525.7837 - mae: 49.0038 - val_loss: 2517.9158 - val_mse: 2517.9158 - val_mae: 48.9412\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2519.5908 - mse: 2519.5908 - mae: 48.9405 - val_loss: 2511.7449 - val_mse: 2511.7449 - val_mae: 48.8781\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2513.4321 - mse: 2513.4321 - mae: 48.8776 - val_loss: 2505.6147 - val_mse: 2505.6147 - val_mae: 48.8153\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2507.3140 - mse: 2507.3140 - mae: 48.8150 - val_loss: 2499.5249 - val_mse: 2499.5249 - val_mae: 48.7529\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2501.2505 - mse: 2501.2505 - mae: 48.7528 - val_loss: 2493.4746 - val_mse: 2493.4746 - val_mae: 48.6908\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2495.2168 - mse: 2495.2168 - mae: 48.6910 - val_loss: 2487.4634 - val_mse: 2487.4634 - val_mae: 48.6291\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2489.2148 - mse: 2489.2148 - mae: 48.6294 - val_loss: 2481.4905 - val_mse: 2481.4905 - val_mae: 48.5676\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2483.2646 - mse: 2483.2646 - mae: 48.5680 - val_loss: 2475.5564 - val_mse: 2475.5564 - val_mae: 48.5065\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2477.3384 - mse: 2477.3384 - mae: 48.5069 - val_loss: 2469.6589 - val_mse: 2469.6589 - val_mae: 48.4456\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2471.4551 - mse: 2471.4551 - mae: 48.4463 - val_loss: 2463.7988 - val_mse: 2463.7988 - val_mae: 48.3851\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2465.5959 - mse: 2465.5959 - mae: 48.3859 - val_loss: 2457.9749 - val_mse: 2457.9749 - val_mae: 48.3249\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2459.7834 - mse: 2459.7834 - mae: 48.3258 - val_loss: 2452.1873 - val_mse: 2452.1873 - val_mae: 48.2650\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2454.0159 - mse: 2454.0159 - mae: 48.2660 - val_loss: 2446.4351 - val_mse: 2446.4351 - val_mae: 48.2054\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2448.2759 - mse: 2448.2759 - mae: 48.2065 - val_loss: 2440.7173 - val_mse: 2440.7173 - val_mae: 48.1460\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2442.5627 - mse: 2442.5627 - mae: 48.1472 - val_loss: 2435.0344 - val_mse: 2435.0344 - val_mae: 48.0870\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2436.9128 - mse: 2436.9128 - mae: 48.0885 - val_loss: 2429.3855 - val_mse: 2429.3855 - val_mae: 48.0282\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2431.2590 - mse: 2431.2590 - mae: 48.0298 - val_loss: 2423.7700 - val_mse: 2423.7700 - val_mae: 47.9697\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2425.6631 - mse: 2425.6631 - mae: 47.9714 - val_loss: 2418.1875 - val_mse: 2418.1875 - val_mae: 47.9115\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2420.0803 - mse: 2420.0803 - mae: 47.9133 - val_loss: 2412.6392 - val_mse: 2412.6392 - val_mae: 47.8535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/300\n",
      "136/136 [==============================] - 54s 396ms/step - loss: 2414.5476 - mse: 2414.5476 - mae: 47.8555 - val_loss: 2407.1213 - val_mse: 2407.1213 - val_mae: 47.7958\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 28s 202ms/step - loss: 2409.0537 - mse: 2409.0537 - mae: 47.7979 - val_loss: 2401.6362 - val_mse: 2401.6362 - val_mae: 47.7384\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 2403.5823 - mse: 2403.5823 - mae: 47.7407 - val_loss: 2396.1831 - val_mse: 2396.1831 - val_mae: 47.6813\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 2398.1265 - mse: 2398.1265 - mae: 47.6834 - val_loss: 2390.7605 - val_mse: 2390.7605 - val_mae: 47.6244\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 27s 194ms/step - loss: 2392.7241 - mse: 2392.7241 - mae: 47.6268 - val_loss: 2385.3691 - val_mse: 2385.3691 - val_mae: 47.5678\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 2387.3289 - mse: 2387.3289 - mae: 47.5702 - val_loss: 2380.0076 - val_mse: 2380.0076 - val_mae: 47.5114\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2381.9875 - mse: 2381.9875 - mae: 47.5140 - val_loss: 2374.6768 - val_mse: 2374.6768 - val_mae: 47.4552\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 2376.6655 - mse: 2376.6655 - mae: 47.4579 - val_loss: 2369.3748 - val_mse: 2369.3748 - val_mae: 47.3993\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2371.3750 - mse: 2371.3750 - mae: 47.4022 - val_loss: 2364.1028 - val_mse: 2364.1028 - val_mae: 47.3437\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 2366.1016 - mse: 2366.1016 - mae: 47.3466 - val_loss: 2358.8591 - val_mse: 2358.8591 - val_mae: 47.2883\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2360.8882 - mse: 2360.8882 - mae: 47.2915 - val_loss: 2353.6448 - val_mse: 2353.6448 - val_mae: 47.2331\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 76s 563ms/step - loss: 2355.6648 - mse: 2355.6648 - mae: 47.2361 - val_loss: 2348.4578 - val_mse: 2348.4578 - val_mae: 47.1782\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 85s 630ms/step - loss: 2350.4932 - mse: 2350.4932 - mae: 47.1815 - val_loss: 2343.3000 - val_mse: 2343.3000 - val_mae: 47.1235\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 96s 703ms/step - loss: 2345.3486 - mse: 2345.3486 - mae: 47.1269 - val_loss: 2338.1687 - val_mse: 2338.1687 - val_mae: 47.0690\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 60s 444ms/step - loss: 2340.2290 - mse: 2340.2290 - mae: 47.0725 - val_loss: 2333.0649 - val_mse: 2333.0649 - val_mae: 47.0147\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 83s 615ms/step - loss: 2335.1443 - mse: 2335.1443 - mae: 47.0185 - val_loss: 2327.9885 - val_mse: 2327.9885 - val_mae: 46.9607\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 69s 507ms/step - loss: 2330.0657 - mse: 2330.0657 - mae: 46.9643 - val_loss: 2322.9380 - val_mse: 2322.9380 - val_mae: 46.9069\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 34s 250ms/step - loss: 2325.0273 - mse: 2325.0273 - mae: 46.9108 - val_loss: 2317.9150 - val_mse: 2317.9150 - val_mae: 46.8534\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 30s 222ms/step - loss: 2320.0107 - mse: 2320.0107 - mae: 46.8572 - val_loss: 2312.9177 - val_mse: 2312.9177 - val_mae: 46.8000\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 53s 388ms/step - loss: 2315.0269 - mse: 2315.0269 - mae: 46.8041 - val_loss: 2307.9465 - val_mse: 2307.9465 - val_mae: 46.7469\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 32s 232ms/step - loss: 2310.0576 - mse: 2310.0576 - mae: 46.7510 - val_loss: 2303.0007 - val_mse: 2303.0007 - val_mae: 46.6939\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 2305.1230 - mse: 2305.1230 - mae: 46.6981 - val_loss: 2298.0798 - val_mse: 2298.0798 - val_mae: 46.6412\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2300.2122 - mse: 2300.2122 - mae: 46.6455 - val_loss: 2293.1838 - val_mse: 2293.1838 - val_mae: 46.5887\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2295.3223 - mse: 2295.3223 - mae: 46.5931 - val_loss: 2288.3135 - val_mse: 2288.3135 - val_mae: 46.5364\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 2290.4683 - mse: 2290.4683 - mae: 46.5410 - val_loss: 2283.4668 - val_mse: 2283.4668 - val_mae: 46.4843\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 24s 173ms/step - loss: 2285.6238 - mse: 2285.6238 - mae: 46.4889 - val_loss: 2278.6448 - val_mse: 2278.6448 - val_mae: 46.4324\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 35s 255ms/step - loss: 2280.8091 - mse: 2280.8091 - mae: 46.4371 - val_loss: 2273.8467 - val_mse: 2273.8467 - val_mae: 46.3807\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 74s 549ms/step - loss: 2276.0164 - mse: 2276.0164 - mae: 46.3854 - val_loss: 2269.0713 - val_mse: 2269.0713 - val_mae: 46.3292\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 58s 427ms/step - loss: 2271.2515 - mse: 2271.2515 - mae: 46.3341 - val_loss: 2264.3201 - val_mse: 2264.3201 - val_mae: 46.2779\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 56s 408ms/step - loss: 2266.5027 - mse: 2266.5027 - mae: 46.2828 - val_loss: 2259.5918 - val_mse: 2259.5918 - val_mae: 46.2268\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 43s 320ms/step - loss: 2261.7871 - mse: 2261.7871 - mae: 46.2319 - val_loss: 2254.8877 - val_mse: 2254.8877 - val_mae: 46.1759\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 42s 310ms/step - loss: 2257.0837 - mse: 2257.0837 - mae: 46.1809 - val_loss: 2250.2058 - val_mse: 2250.2058 - val_mae: 46.1252\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 20s 145ms/step - loss: 2252.4219 - mse: 2252.4219 - mae: 46.1305 - val_loss: 2245.5464 - val_mse: 2245.5464 - val_mae: 46.0746\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 24s 179ms/step - loss: 2247.7590 - mse: 2247.7590 - mae: 46.0798 - val_loss: 2240.9089 - val_mse: 2240.9089 - val_mae: 46.0242\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 41s 306ms/step - loss: 2243.1230 - mse: 2243.1230 - mae: 46.0294 - val_loss: 2236.2942 - val_mse: 2236.2942 - val_mae: 45.9741\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 50s 366ms/step - loss: 2238.5188 - mse: 2238.5188 - mae: 45.9794 - val_loss: 2231.7009 - val_mse: 2231.7009 - val_mae: 45.9241\n",
      "Epoch 160/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 2233.9429 - mse: 2233.9429 - mae: 45.9296 - val_loss: 2227.1287 - val_mse: 2227.1287 - val_mae: 45.8743\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 2229.3706 - mse: 2229.3706 - mae: 45.8799 - val_loss: 2222.5784 - val_mse: 2222.5784 - val_mae: 45.8247\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 2224.8281 - mse: 2224.8281 - mae: 45.8304 - val_loss: 2218.0493 - val_mse: 2218.0493 - val_mae: 45.7752\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2220.3108 - mse: 2220.3108 - mae: 45.7810 - val_loss: 2213.5415 - val_mse: 2213.5415 - val_mae: 45.7260\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2215.8108 - mse: 2215.8108 - mae: 45.7318 - val_loss: 2209.0542 - val_mse: 2209.0542 - val_mae: 45.6769\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2211.3267 - mse: 2211.3267 - mae: 45.6828 - val_loss: 2204.5874 - val_mse: 2204.5874 - val_mae: 45.6280\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2206.8723 - mse: 2206.8723 - mae: 45.6341 - val_loss: 2200.1414 - val_mse: 2200.1414 - val_mae: 45.5792\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2202.4285 - mse: 2202.4285 - mae: 45.5852 - val_loss: 2195.7156 - val_mse: 2195.7156 - val_mae: 45.5306\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 149ms/step - loss: 2198.0151 - mse: 2198.0151 - mae: 45.5369 - val_loss: 2191.3093 - val_mse: 2191.3093 - val_mae: 45.4822\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2193.6060 - mse: 2193.6060 - mae: 45.4885 - val_loss: 2186.9233 - val_mse: 2186.9233 - val_mae: 45.4340\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2189.2346 - mse: 2189.2346 - mae: 45.4403 - val_loss: 2182.5569 - val_mse: 2182.5569 - val_mae: 45.3859\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2184.8726 - mse: 2184.8726 - mae: 45.3923 - val_loss: 2178.2102 - val_mse: 2178.2102 - val_mae: 45.3380\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2180.5234 - mse: 2180.5234 - mae: 45.3444 - val_loss: 2173.8826 - val_mse: 2173.8826 - val_mae: 45.2902\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2176.2043 - mse: 2176.2043 - mae: 45.2968 - val_loss: 2169.5737 - val_mse: 2169.5737 - val_mae: 45.2426\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2171.9043 - mse: 2171.9043 - mae: 45.2493 - val_loss: 2165.2847 - val_mse: 2165.2847 - val_mae: 45.1952\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2167.6223 - mse: 2167.6223 - mae: 45.2019 - val_loss: 2161.0137 - val_mse: 2161.0137 - val_mae: 45.1479\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2163.3579 - mse: 2163.3579 - mae: 45.1547 - val_loss: 2156.7625 - val_mse: 2156.7625 - val_mae: 45.1008\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2159.1074 - mse: 2159.1074 - mae: 45.1077 - val_loss: 2152.5286 - val_mse: 2152.5286 - val_mae: 45.0539\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 2154.8892 - mse: 2154.8892 - mae: 45.0609 - val_loss: 2148.3135 - val_mse: 2148.3135 - val_mae: 45.0071\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2150.6719 - mse: 2150.6719 - mae: 45.0141 - val_loss: 2144.1169 - val_mse: 2144.1169 - val_mae: 44.9604\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2146.4856 - mse: 2146.4856 - mae: 44.9674 - val_loss: 2139.9377 - val_mse: 2139.9377 - val_mae: 44.9139\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2142.3101 - mse: 2142.3101 - mae: 44.9210 - val_loss: 2135.7764 - val_mse: 2135.7764 - val_mae: 44.8676\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2138.1514 - mse: 2138.1514 - mae: 44.8748 - val_loss: 2131.6321 - val_mse: 2131.6321 - val_mae: 44.8213\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2134.0122 - mse: 2134.0122 - mae: 44.8286 - val_loss: 2127.5066 - val_mse: 2127.5066 - val_mae: 44.7753\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2129.8928 - mse: 2129.8928 - mae: 44.7826 - val_loss: 2123.3982 - val_mse: 2123.3982 - val_mae: 44.7294\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2125.7898 - mse: 2125.7898 - mae: 44.7369 - val_loss: 2119.3062 - val_mse: 2119.3062 - val_mae: 44.6836\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2121.7021 - mse: 2121.7021 - mae: 44.6912 - val_loss: 2115.2319 - val_mse: 2115.2319 - val_mae: 44.6380\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2117.6333 - mse: 2117.6333 - mae: 44.6455 - val_loss: 2111.1743 - val_mse: 2111.1743 - val_mae: 44.5926\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2113.5847 - mse: 2113.5847 - mae: 44.6003 - val_loss: 2107.1331 - val_mse: 2107.1331 - val_mae: 44.5472\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2109.5432 - mse: 2109.5432 - mae: 44.5547 - val_loss: 2103.1091 - val_mse: 2103.1091 - val_mae: 44.5020\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2105.5344 - mse: 2105.5344 - mae: 44.5098 - val_loss: 2099.1021 - val_mse: 2099.1021 - val_mae: 44.4570\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2101.5286 - mse: 2101.5286 - mae: 44.4648 - val_loss: 2095.1111 - val_mse: 2095.1111 - val_mae: 44.4121\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2097.5493 - mse: 2097.5493 - mae: 44.4200 - val_loss: 2091.1360 - val_mse: 2091.1360 - val_mae: 44.3673\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2093.5623 - mse: 2093.5623 - mae: 44.3752 - val_loss: 2087.1772 - val_mse: 2087.1772 - val_mae: 44.3227\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2089.6194 - mse: 2089.6194 - mae: 44.3306 - val_loss: 2083.2344 - val_mse: 2083.2344 - val_mae: 44.2782\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2085.6790 - mse: 2085.6790 - mae: 44.2862 - val_loss: 2079.3074 - val_mse: 2079.3074 - val_mae: 44.2338\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2081.7529 - mse: 2081.7529 - mae: 44.2420 - val_loss: 2075.3960 - val_mse: 2075.3960 - val_mae: 44.1896\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2077.8484 - mse: 2077.8484 - mae: 44.1978 - val_loss: 2071.5000 - val_mse: 2071.5000 - val_mae: 44.1455\n",
      "Epoch 198/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2073.9585 - mse: 2073.9585 - mae: 44.1538 - val_loss: 2067.6196 - val_mse: 2067.6196 - val_mae: 44.1015\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2070.0864 - mse: 2070.0864 - mae: 44.1099 - val_loss: 2063.7546 - val_mse: 2063.7546 - val_mae: 44.0577\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2066.2209 - mse: 2066.2209 - mae: 44.0659 - val_loss: 2059.9043 - val_mse: 2059.9043 - val_mae: 44.0139\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2062.3762 - mse: 2062.3762 - mae: 44.0224 - val_loss: 2056.0703 - val_mse: 2056.0703 - val_mae: 43.9704\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2058.5422 - mse: 2058.5422 - mae: 43.9787 - val_loss: 2052.2505 - val_mse: 2052.2505 - val_mae: 43.9269\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2054.7314 - mse: 2054.7314 - mae: 43.9354 - val_loss: 2048.4463 - val_mse: 2048.4463 - val_mae: 43.8836\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2050.9314 - mse: 2050.9314 - mae: 43.8922 - val_loss: 2044.6555 - val_mse: 2044.6555 - val_mae: 43.8404\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2047.1469 - mse: 2047.1469 - mae: 43.8490 - val_loss: 2040.8805 - val_mse: 2040.8805 - val_mae: 43.7973\n",
      "Epoch 206/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2043.3694 - mse: 2043.3694 - mae: 43.8059 - val_loss: 2037.1195 - val_mse: 2037.1195 - val_mae: 43.7543\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2039.6215 - mse: 2039.6215 - mae: 43.7631 - val_loss: 2033.3740 - val_mse: 2033.3740 - val_mae: 43.7115\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2035.8784 - mse: 2035.8784 - mae: 43.7203 - val_loss: 2029.6422 - val_mse: 2029.6422 - val_mae: 43.6688\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2032.1473 - mse: 2032.1473 - mae: 43.6777 - val_loss: 2025.9244 - val_mse: 2025.9244 - val_mae: 43.6262\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2028.4335 - mse: 2028.4335 - mae: 43.6351 - val_loss: 2022.2211 - val_mse: 2022.2211 - val_mae: 43.5838\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2024.7373 - mse: 2024.7373 - mae: 43.5927 - val_loss: 2018.5312 - val_mse: 2018.5312 - val_mae: 43.5414\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 149ms/step - loss: 2021.0474 - mse: 2021.0474 - mae: 43.5506 - val_loss: 2014.8547 - val_mse: 2014.8547 - val_mae: 43.4992\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2017.3749 - mse: 2017.3749 - mae: 43.5082 - val_loss: 2011.1919 - val_mse: 2011.1919 - val_mae: 43.4571\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2013.7196 - mse: 2013.7196 - mae: 43.4662 - val_loss: 2007.5437 - val_mse: 2007.5437 - val_mae: 43.4151\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2010.0781 - mse: 2010.0781 - mae: 43.4244 - val_loss: 2003.9092 - val_mse: 2003.9092 - val_mae: 43.3732\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2006.4406 - mse: 2006.4406 - mae: 43.3824 - val_loss: 2000.2875 - val_mse: 2000.2875 - val_mae: 43.3314\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2002.8235 - mse: 2002.8235 - mae: 43.3407 - val_loss: 1996.6792 - val_mse: 1996.6792 - val_mae: 43.2897\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1999.2163 - mse: 1999.2163 - mae: 43.2990 - val_loss: 1993.0836 - val_mse: 1993.0836 - val_mae: 43.2482\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1995.6323 - mse: 1995.6323 - mae: 43.2576 - val_loss: 1989.5018 - val_mse: 1989.5018 - val_mae: 43.2067\n",
      "Epoch 220/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1992.0575 - mse: 1992.0575 - mae: 43.2164 - val_loss: 1985.9331 - val_mse: 1985.9331 - val_mae: 43.1654\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 1988.4822 - mse: 1988.4822 - mae: 43.1748 - val_loss: 1982.3771 - val_mse: 1982.3771 - val_mae: 43.1242\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 1984.9308 - mse: 1984.9308 - mae: 43.1338 - val_loss: 1978.8341 - val_mse: 1978.8341 - val_mae: 43.0831\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1981.3916 - mse: 1981.3916 - mae: 43.0927 - val_loss: 1975.3040 - val_mse: 1975.3040 - val_mae: 43.0422\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1977.8651 - mse: 1977.8651 - mae: 43.0518 - val_loss: 1971.7852 - val_mse: 1971.7852 - val_mae: 43.0013\n",
      "Epoch 225/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1974.3468 - mse: 1974.3468 - mae: 43.0109 - val_loss: 1968.2797 - val_mse: 1968.2797 - val_mae: 42.9605\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1970.8518 - mse: 1970.8518 - mae: 42.9704 - val_loss: 1964.7852 - val_mse: 1964.7852 - val_mae: 42.9198\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1967.3647 - mse: 1967.3647 - mae: 42.9297 - val_loss: 1961.3031 - val_mse: 1961.3031 - val_mae: 42.8792\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1963.8828 - mse: 1963.8828 - mae: 42.8890 - val_loss: 1957.8325 - val_mse: 1957.8325 - val_mae: 42.8387\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1960.4109 - mse: 1960.4109 - mae: 42.8486 - val_loss: 1954.3722 - val_mse: 1954.3722 - val_mae: 42.7983\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 29s 213ms/step - loss: 1956.9545 - mse: 1956.9545 - mae: 42.8083 - val_loss: 1950.9229 - val_mse: 1950.9229 - val_mae: 42.7580\n",
      "Epoch 231/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 1953.5017 - mse: 1953.5017 - mae: 42.7680 - val_loss: 1947.4830 - val_mse: 1947.4830 - val_mae: 42.7177\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 1950.0641 - mse: 1950.0641 - mae: 42.7277 - val_loss: 1944.0474 - val_mse: 1944.0474 - val_mae: 42.6775\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 1946.6230 - mse: 1946.6230 - mae: 42.6874 - val_loss: 1940.6075 - val_mse: 1940.6075 - val_mae: 42.6372\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1943.1648 - mse: 1943.1648 - mae: 42.6469 - val_loss: 1937.1307 - val_mse: 1937.1307 - val_mae: 42.5964\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 1939.5923 - mse: 1939.5923 - mae: 42.6050 - val_loss: 1933.3427 - val_mse: 1933.3427 - val_mae: 42.5519\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 1934.7072 - mse: 1934.7072 - mae: 42.5475 - val_loss: 1927.5199 - val_mse: 1927.5199 - val_mae: 42.4834\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 1929.7872 - mse: 1929.7872 - mae: 42.4898 - val_loss: 1923.4991 - val_mse: 1923.4991 - val_mae: 42.4361\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1925.9061 - mse: 1925.9061 - mae: 42.4441 - val_loss: 1919.7285 - val_mse: 1919.7285 - val_mae: 42.3916\n",
      "Epoch 239/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1922.1870 - mse: 1922.1870 - mae: 42.4002 - val_loss: 1916.0662 - val_mse: 1916.0662 - val_mae: 42.3484\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 1918.5588 - mse: 1918.5588 - mae: 42.3575 - val_loss: 1912.4694 - val_mse: 1912.4694 - val_mae: 42.3059\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 1914.9777 - mse: 1914.9777 - mae: 42.3151 - val_loss: 1908.9202 - val_mse: 1908.9202 - val_mae: 42.2640\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 1911.4413 - mse: 1911.4413 - mae: 42.2733 - val_loss: 1905.4072 - val_mse: 1905.4072 - val_mae: 42.2224\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1907.9427 - mse: 1907.9427 - mae: 42.2320 - val_loss: 1901.9260 - val_mse: 1901.9260 - val_mae: 42.1812\n",
      "Epoch 244/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 1904.4747 - mse: 1904.4747 - mae: 42.1908 - val_loss: 1898.4698 - val_mse: 1898.4698 - val_mae: 42.1402\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 1901.0266 - mse: 1901.0266 - mae: 42.1500 - val_loss: 1895.0370 - val_mse: 1895.0370 - val_mae: 42.0994\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1897.5997 - mse: 1897.5997 - mae: 42.1094 - val_loss: 1891.6260 - val_mse: 1891.6260 - val_mae: 42.0589\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1894.1884 - mse: 1894.1884 - mae: 42.0688 - val_loss: 1888.2349 - val_mse: 1888.2349 - val_mae: 42.0185\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1890.8112 - mse: 1890.8112 - mae: 42.0286 - val_loss: 1884.8623 - val_mse: 1884.8623 - val_mae: 41.9784\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1887.4343 - mse: 1887.4343 - mae: 41.9884 - val_loss: 1881.5068 - val_mse: 1881.5068 - val_mae: 41.9384\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1884.0903 - mse: 1884.0903 - mae: 41.9486 - val_loss: 1878.1677 - val_mse: 1878.1677 - val_mae: 41.8986\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1880.7545 - mse: 1880.7545 - mae: 41.9087 - val_loss: 1874.8450 - val_mse: 1874.8450 - val_mae: 41.8589\n",
      "Epoch 252/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 1877.4373 - mse: 1877.4373 - mae: 41.8692 - val_loss: 1871.5380 - val_mse: 1871.5380 - val_mae: 41.8194\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1874.1323 - mse: 1874.1323 - mae: 41.8298 - val_loss: 1868.2458 - val_mse: 1868.2458 - val_mae: 41.7800\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1870.8458 - mse: 1870.8458 - mae: 41.7905 - val_loss: 1864.9685 - val_mse: 1864.9685 - val_mae: 41.7408\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1867.5745 - mse: 1867.5745 - mae: 41.7513 - val_loss: 1861.7047 - val_mse: 1861.7047 - val_mae: 41.7017\n",
      "Epoch 256/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 150ms/step - loss: 1864.3212 - mse: 1864.3212 - mae: 41.7122 - val_loss: 1858.4547 - val_mse: 1858.4547 - val_mae: 41.6626\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1861.0736 - mse: 1861.0736 - mae: 41.6732 - val_loss: 1855.2181 - val_mse: 1855.2181 - val_mae: 41.6238\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1857.8411 - mse: 1857.8411 - mae: 41.6345 - val_loss: 1851.9944 - val_mse: 1851.9944 - val_mae: 41.5850\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1854.6176 - mse: 1854.6176 - mae: 41.5958 - val_loss: 1848.7832 - val_mse: 1848.7832 - val_mae: 41.5464\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1851.4115 - mse: 1851.4115 - mae: 41.5573 - val_loss: 1845.5859 - val_mse: 1845.5859 - val_mae: 41.5079\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 1848.2173 - mse: 1848.2173 - mae: 41.5187 - val_loss: 1842.3998 - val_mse: 1842.3998 - val_mae: 41.4695\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1845.0332 - mse: 1845.0332 - mae: 41.4804 - val_loss: 1839.2257 - val_mse: 1839.2257 - val_mae: 41.4312\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1841.8597 - mse: 1841.8597 - mae: 41.4421 - val_loss: 1836.0649 - val_mse: 1836.0649 - val_mae: 41.3931\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 1838.7040 - mse: 1838.7040 - mae: 41.4041 - val_loss: 1832.9153 - val_mse: 1832.9153 - val_mae: 41.3550\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1835.5593 - mse: 1835.5593 - mae: 41.3660 - val_loss: 1829.7771 - val_mse: 1829.7771 - val_mae: 41.3171\n",
      "Epoch 266/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1832.4249 - mse: 1832.4249 - mae: 41.3282 - val_loss: 1826.6509 - val_mse: 1826.6509 - val_mae: 41.2792\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 1829.3016 - mse: 1829.3016 - mae: 41.2904 - val_loss: 1823.5366 - val_mse: 1823.5366 - val_mae: 41.2415\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1826.1879 - mse: 1826.1879 - mae: 41.2527 - val_loss: 1820.4332 - val_mse: 1820.4332 - val_mae: 41.2038\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1823.0966 - mse: 1823.0966 - mae: 41.2152 - val_loss: 1817.3397 - val_mse: 1817.3397 - val_mae: 41.1663\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1819.9952 - mse: 1819.9952 - mae: 41.1776 - val_loss: 1814.2583 - val_mse: 1814.2583 - val_mae: 41.1288\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1816.9205 - mse: 1816.9205 - mae: 41.1401 - val_loss: 1811.1873 - val_mse: 1811.1873 - val_mae: 41.0915\n",
      "Epoch 272/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1813.8551 - mse: 1813.8551 - mae: 41.1029 - val_loss: 1808.1270 - val_mse: 1808.1270 - val_mae: 41.0542\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 1810.7911 - mse: 1810.7911 - mae: 41.0656 - val_loss: 1805.0771 - val_mse: 1805.0771 - val_mae: 41.0171\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1807.7474 - mse: 1807.7474 - mae: 41.0285 - val_loss: 1802.0397 - val_mse: 1802.0397 - val_mae: 40.9800\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1804.7155 - mse: 1804.7155 - mae: 40.9915 - val_loss: 1799.0105 - val_mse: 1799.0105 - val_mae: 40.9430\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 1801.6846 - mse: 1801.6846 - mae: 40.9546 - val_loss: 1795.9926 - val_mse: 1795.9926 - val_mae: 40.9062\n",
      "Epoch 277/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1798.6746 - mse: 1798.6746 - mae: 40.9178 - val_loss: 1792.9854 - val_mse: 1792.9854 - val_mae: 40.8694\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1795.6691 - mse: 1795.6691 - mae: 40.8810 - val_loss: 1789.9862 - val_mse: 1789.9862 - val_mae: 40.8327\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 1792.6659 - mse: 1792.6659 - mae: 40.8444 - val_loss: 1786.9985 - val_mse: 1786.9985 - val_mae: 40.7961\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1789.6874 - mse: 1789.6874 - mae: 40.8078 - val_loss: 1784.0209 - val_mse: 1784.0209 - val_mae: 40.7596\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1786.7109 - mse: 1786.7109 - mae: 40.7714 - val_loss: 1781.0525 - val_mse: 1781.0525 - val_mae: 40.7232\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 34s 248ms/step - loss: 1783.7457 - mse: 1783.7457 - mae: 40.7350 - val_loss: 1778.0950 - val_mse: 1778.0950 - val_mae: 40.6868\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 1780.7905 - mse: 1780.7905 - mae: 40.6987 - val_loss: 1775.1466 - val_mse: 1775.1466 - val_mae: 40.6506\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 1777.8409 - mse: 1777.8409 - mae: 40.6625 - val_loss: 1772.2075 - val_mse: 1772.2075 - val_mae: 40.6144\n",
      "Epoch 285/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 1774.9108 - mse: 1774.9108 - mae: 40.6265 - val_loss: 1769.2783 - val_mse: 1769.2783 - val_mae: 40.5783\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 23s 166ms/step - loss: 1771.9824 - mse: 1771.9824 - mae: 40.5903 - val_loss: 1766.3578 - val_mse: 1766.3578 - val_mae: 40.5423\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1769.0652 - mse: 1769.0652 - mae: 40.5543 - val_loss: 1763.4473 - val_mse: 1763.4473 - val_mae: 40.5064\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1766.1511 - mse: 1766.1511 - mae: 40.5184 - val_loss: 1760.5454 - val_mse: 1760.5454 - val_mae: 40.4706\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 1763.2574 - mse: 1763.2574 - mae: 40.4827 - val_loss: 1757.6543 - val_mse: 1757.6543 - val_mae: 40.4349\n",
      "Epoch 290/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 1760.3650 - mse: 1760.3650 - mae: 40.4470 - val_loss: 1754.7715 - val_mse: 1754.7715 - val_mae: 40.3992\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1757.4846 - mse: 1757.4846 - mae: 40.4113 - val_loss: 1751.8994 - val_mse: 1751.8994 - val_mae: 40.3636\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1754.6187 - mse: 1754.6187 - mae: 40.3758 - val_loss: 1749.0342 - val_mse: 1749.0342 - val_mae: 40.3281\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1751.7518 - mse: 1751.7518 - mae: 40.3404 - val_loss: 1746.1785 - val_mse: 1746.1785 - val_mae: 40.2927\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1748.8999 - mse: 1748.8999 - mae: 40.3050 - val_loss: 1743.3314 - val_mse: 1743.3314 - val_mae: 40.2574\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1746.0532 - mse: 1746.0532 - mae: 40.2697 - val_loss: 1740.4950 - val_mse: 1740.4950 - val_mae: 40.2221\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1743.2167 - mse: 1743.2167 - mae: 40.2345 - val_loss: 1737.6650 - val_mse: 1737.6650 - val_mae: 40.1869\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1740.3901 - mse: 1740.3901 - mae: 40.1992 - val_loss: 1734.8450 - val_mse: 1734.8450 - val_mae: 40.1518\n",
      "Epoch 298/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1737.5704 - mse: 1737.5704 - mae: 40.1642 - val_loss: 1732.0333 - val_mse: 1732.0333 - val_mae: 40.1168\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1734.7612 - mse: 1734.7612 - mae: 40.1292 - val_loss: 1729.2303 - val_mse: 1729.2303 - val_mae: 40.0818\n",
      "Epoch 300/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 149ms/step - loss: 1731.9628 - mse: 1731.9628 - mae: 40.0943 - val_loss: 1726.4362 - val_mse: 1726.4362 - val_mae: 40.0469\n",
      "57/57 [==============================] - 4s 37ms/step - loss: 1754.6624 - mse: 1754.6624 - mae: 40.2593\n",
      "57/57 [==============================] - 4s 34ms/step\n",
      "MAE: 40.25931498363935\n",
      "MSE: 1754.6621776413522\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 60, 240)          173760    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 120)              144480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 28s 160ms/step - loss: 5450.2881 - mse: 5450.2881 - mae: 72.8795 - val_loss: 4709.6997 - val_mse: 4709.6997 - val_mae: 67.7276\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 4488.0020 - mse: 4488.0020 - mae: 66.0492 - val_loss: 4272.5239 - val_mse: 4272.5239 - val_mae: 64.4193\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 4155.5234 - mse: 4155.5234 - mae: 63.4891 - val_loss: 4040.2937 - val_mse: 4040.2937 - val_mae: 62.5909\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3971.8479 - mse: 3971.8479 - mae: 62.0289 - val_loss: 3896.8777 - val_mse: 3896.8777 - val_mae: 61.4345\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3846.4377 - mse: 3846.4377 - mae: 61.0072 - val_loss: 3785.5425 - val_mse: 3785.5425 - val_mae: 60.5216\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 3740.0613 - mse: 3740.0613 - mae: 60.1288 - val_loss: 3683.6775 - val_mse: 3683.6775 - val_mae: 59.6741\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3639.8142 - mse: 3639.8142 - mae: 59.2906 - val_loss: 3585.1758 - val_mse: 3585.1758 - val_mae: 58.8430\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 3542.4946 - mse: 3542.4946 - mae: 58.4637 - val_loss: 3488.9902 - val_mse: 3488.9902 - val_mae: 58.0199\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 3446.9834 - mse: 3446.9834 - mae: 57.6406 - val_loss: 3394.2698 - val_mse: 3394.2698 - val_mae: 57.1978\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3352.7380 - mse: 3352.7380 - mae: 56.8186 - val_loss: 3300.8679 - val_mse: 3300.8679 - val_mae: 56.3754\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3260.1909 - mse: 3260.1909 - mae: 55.9968 - val_loss: 3208.9351 - val_mse: 3208.9351 - val_mae: 55.5541\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3168.7207 - mse: 3168.7207 - mae: 55.1753 - val_loss: 3118.2578 - val_mse: 3118.2578 - val_mae: 54.7319\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3078.6082 - mse: 3078.6082 - mae: 54.3538 - val_loss: 3029.0527 - val_mse: 3029.0527 - val_mae: 53.9108\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2990.1182 - mse: 2990.1182 - mae: 53.5324 - val_loss: 2941.1897 - val_mse: 2941.1897 - val_mae: 53.0896\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2902.9556 - mse: 2902.9556 - mae: 52.7110 - val_loss: 2854.6738 - val_mse: 2854.6738 - val_mae: 52.2685\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2816.8489 - mse: 2816.8489 - mae: 51.8897 - val_loss: 2769.4536 - val_mse: 2769.4536 - val_mae: 51.4468\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2732.4312 - mse: 2732.4312 - mae: 51.0685 - val_loss: 2685.6277 - val_mse: 2685.6277 - val_mae: 50.6256\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 2649.1670 - mse: 2649.1670 - mae: 50.2472 - val_loss: 2603.1414 - val_mse: 2603.1414 - val_mae: 49.8042\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2567.4507 - mse: 2567.4507 - mae: 49.4259 - val_loss: 2522.0576 - val_mse: 2522.0576 - val_mae: 48.9835\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2486.7659 - mse: 2486.7659 - mae: 48.6048 - val_loss: 2442.2441 - val_mse: 2442.2441 - val_mae: 48.1619\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 2407.7942 - mse: 2407.7942 - mae: 47.7835 - val_loss: 2363.8772 - val_mse: 2363.8772 - val_mae: 47.3413\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 2329.9553 - mse: 2329.9553 - mae: 46.9625 - val_loss: 2286.7441 - val_mse: 2286.7441 - val_mae: 46.5195\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2253.5630 - mse: 2253.5630 - mae: 46.1414 - val_loss: 2211.1174 - val_mse: 2211.1174 - val_mae: 45.6995\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2178.5085 - mse: 2178.5085 - mae: 45.3206 - val_loss: 2136.7263 - val_mse: 2136.7263 - val_mae: 44.8781\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2104.7175 - mse: 2104.7175 - mae: 44.4998 - val_loss: 2063.6667 - val_mse: 2063.6667 - val_mae: 44.0567\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2032.3951 - mse: 2032.3951 - mae: 43.6787 - val_loss: 1992.0771 - val_mse: 1992.0771 - val_mae: 43.2366\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1961.2305 - mse: 1961.2305 - mae: 42.8578 - val_loss: 1921.7408 - val_mse: 1921.7408 - val_mae: 42.4154\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1891.5775 - mse: 1891.5775 - mae: 42.0370 - val_loss: 1852.7405 - val_mse: 1852.7405 - val_mae: 41.5940\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1823.2245 - mse: 1823.2245 - mae: 41.2160 - val_loss: 1785.2195 - val_mse: 1785.2195 - val_mae: 40.7743\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1756.4371 - mse: 1756.4371 - mae: 40.3954 - val_loss: 1718.9204 - val_mse: 1718.9204 - val_mae: 39.9530\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1690.5206 - mse: 1690.5206 - mae: 39.5747 - val_loss: 1654.0115 - val_mse: 1654.0115 - val_mae: 39.1322\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1626.3291 - mse: 1626.3291 - mae: 38.7541 - val_loss: 1590.4816 - val_mse: 1590.4816 - val_mae: 38.3119\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1563.6139 - mse: 1563.6139 - mae: 37.9338 - val_loss: 1528.4249 - val_mse: 1528.4249 - val_mae: 37.4933\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1501.8373 - mse: 1501.8373 - mae: 37.1137 - val_loss: 1467.4362 - val_mse: 1467.4362 - val_mae: 36.6709\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1441.6304 - mse: 1441.6304 - mae: 36.2934 - val_loss: 1407.9404 - val_mse: 1407.9404 - val_mae: 35.8506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1382.7631 - mse: 1382.7631 - mae: 35.4732 - val_loss: 1349.8466 - val_mse: 1349.8466 - val_mae: 35.0310\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1325.3665 - mse: 1325.3665 - mae: 34.6529 - val_loss: 1293.0806 - val_mse: 1293.0806 - val_mae: 34.2112\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 1269.0361 - mse: 1269.0361 - mae: 33.8331 - val_loss: 1237.5518 - val_mse: 1237.5518 - val_mae: 33.3897\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 1214.4104 - mse: 1214.4104 - mae: 33.0129 - val_loss: 1183.5444 - val_mse: 1183.5444 - val_mae: 32.5710\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 1160.7794 - mse: 1160.7794 - mae: 32.1929 - val_loss: 1130.7639 - val_mse: 1130.7639 - val_mae: 31.7504\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1108.6558 - mse: 1108.6558 - mae: 31.3729 - val_loss: 1079.4072 - val_mse: 1079.4072 - val_mae: 30.9311\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1058.0975 - mse: 1058.0975 - mae: 30.5531 - val_loss: 1029.4272 - val_mse: 1029.4272 - val_mae: 30.1123\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1008.6426 - mse: 1008.6426 - mae: 29.7337 - val_loss: 980.7072 - val_mse: 980.7072 - val_mae: 29.2922\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 960.4067 - mse: 960.4067 - mae: 28.9144 - val_loss: 933.3366 - val_mse: 933.3366 - val_mae: 28.4721\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 913.6570 - mse: 913.6570 - mae: 28.0949 - val_loss: 887.3743 - val_mse: 887.3743 - val_mae: 27.6531\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 868.4338 - mse: 868.4338 - mae: 27.2760 - val_loss: 842.7475 - val_mse: 842.7475 - val_mae: 26.8341\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 824.2881 - mse: 824.2881 - mae: 26.4571 - val_loss: 799.4934 - val_mse: 799.4934 - val_mae: 26.0157\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 781.9083 - mse: 781.9083 - mae: 25.6382 - val_loss: 757.5945 - val_mse: 757.5945 - val_mae: 25.1976\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 740.4164 - mse: 740.4164 - mae: 24.8218 - val_loss: 716.8954 - val_mse: 716.8954 - val_mae: 24.3816\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 700.4266 - mse: 700.4266 - mae: 24.0170 - val_loss: 677.7893 - val_mse: 677.7893 - val_mae: 23.5983\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 661.9522 - mse: 661.9522 - mae: 23.2458 - val_loss: 639.8628 - val_mse: 639.8628 - val_mae: 22.8289\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 624.6643 - mse: 624.6643 - mae: 22.4823 - val_loss: 603.3541 - val_mse: 603.3541 - val_mae: 22.0611\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 588.8452 - mse: 588.8452 - mae: 21.7156 - val_loss: 568.2828 - val_mse: 568.2828 - val_mae: 21.2956\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 554.2134 - mse: 554.2134 - mae: 20.9522 - val_loss: 534.2993 - val_mse: 534.2993 - val_mae: 20.5372\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 520.9877 - mse: 520.9877 - mae: 20.2052 - val_loss: 501.8948 - val_mse: 501.8948 - val_mae: 19.8013\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 489.1998 - mse: 489.1998 - mae: 19.4789 - val_loss: 470.8167 - val_mse: 470.8167 - val_mae: 19.0805\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 458.7292 - mse: 458.7292 - mae: 18.7699 - val_loss: 441.0272 - val_mse: 441.0272 - val_mae: 18.3750\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 429.5371 - mse: 429.5371 - mae: 18.0795 - val_loss: 412.6127 - val_mse: 412.6127 - val_mae: 17.6907\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 401.7493 - mse: 401.7493 - mae: 17.4040 - val_loss: 385.5672 - val_mse: 385.5672 - val_mae: 17.0147\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 375.2423 - mse: 375.2423 - mae: 16.7507 - val_loss: 359.7641 - val_mse: 359.7641 - val_mae: 16.3923\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 350.3821 - mse: 350.3821 - mae: 16.1421 - val_loss: 335.4911 - val_mse: 335.4911 - val_mae: 15.7882\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 326.3636 - mse: 326.3636 - mae: 15.5287 - val_loss: 312.3835 - val_mse: 312.3835 - val_mae: 15.1832\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 304.1849 - mse: 304.1849 - mae: 14.9311 - val_loss: 290.8297 - val_mse: 290.8297 - val_mae: 14.5982\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 283.0450 - mse: 283.0450 - mae: 14.3710 - val_loss: 270.4801 - val_mse: 270.4801 - val_mae: 14.0712\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 263.2581 - mse: 263.2581 - mae: 13.8370 - val_loss: 251.4929 - val_mse: 251.4929 - val_mae: 13.5456\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 245.0371 - mse: 245.0371 - mae: 13.3131 - val_loss: 233.8415 - val_mse: 233.8415 - val_mae: 13.0256\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 228.0855 - mse: 228.0855 - mae: 12.7968 - val_loss: 217.5831 - val_mse: 217.5831 - val_mae: 12.5114\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 27s 195ms/step - loss: 212.3766 - mse: 212.3766 - mae: 12.2773 - val_loss: 202.6772 - val_mse: 202.6772 - val_mae: 12.0049\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 198.1870 - mse: 198.1870 - mae: 11.7916 - val_loss: 189.0805 - val_mse: 189.0805 - val_mae: 11.5312\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 24s 173ms/step - loss: 184.9440 - mse: 184.9440 - mae: 11.3374 - val_loss: 176.7810 - val_mse: 176.7810 - val_mae: 11.1194\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 173.5283 - mse: 173.5283 - mae: 10.9496 - val_loss: 165.9433 - val_mse: 165.9433 - val_mae: 10.7418\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 163.0252 - mse: 163.0252 - mae: 10.5929 - val_loss: 156.1116 - val_mse: 156.1116 - val_mae: 10.3949\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 154.0628 - mse: 154.0628 - mae: 10.2873 - val_loss: 147.9014 - val_mse: 147.9014 - val_mae: 10.1141\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 35s 261ms/step - loss: 146.2787 - mse: 146.2787 - mae: 10.0156 - val_loss: 140.8580 - val_mse: 140.8580 - val_mae: 9.8632\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 23s 165ms/step - loss: 139.7779 - mse: 139.7779 - mae: 9.7749 - val_loss: 135.0907 - val_mse: 135.0907 - val_mae: 9.6525\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 134.7670 - mse: 134.7670 - mae: 9.5797 - val_loss: 130.7452 - val_mse: 130.7452 - val_mae: 9.4781\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 130.9763 - mse: 130.9763 - mae: 9.4196 - val_loss: 127.4961 - val_mse: 127.4961 - val_mae: 9.3286\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 128.2320 - mse: 128.2320 - mae: 9.3006 - val_loss: 125.4130 - val_mse: 125.4130 - val_mae: 9.2523\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 126.4731 - mse: 126.4731 - mae: 9.2436 - val_loss: 124.0513 - val_mse: 124.0513 - val_mae: 9.1992\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 125.4594 - mse: 125.4594 - mae: 9.2068 - val_loss: 123.3656 - val_mse: 123.3656 - val_mae: 9.1656\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.9065 - mse: 124.9065 - mae: 9.1823 - val_loss: 122.9658 - val_mse: 122.9658 - val_mae: 9.1387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.6338 - mse: 124.6338 - mae: 9.1646 - val_loss: 122.7969 - val_mse: 122.7969 - val_mae: 9.1211\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.5107 - mse: 124.5107 - mae: 9.1509 - val_loss: 122.7222 - val_mse: 122.7222 - val_mae: 9.1087\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4565 - mse: 124.4565 - mae: 9.1428 - val_loss: 122.6951 - val_mse: 122.6951 - val_mae: 9.1015\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4341 - mse: 124.4341 - mae: 9.1389 - val_loss: 122.6822 - val_mse: 122.6822 - val_mae: 9.0955\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4236 - mse: 124.4236 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0945\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 20s 145ms/step - loss: 124.4202 - mse: 124.4202 - mae: 9.1342 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 29s 217ms/step - loss: 124.4187 - mse: 124.4187 - mae: 9.1345 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1344 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 39s 290ms/step - loss: 124.4198 - mse: 124.4198 - mae: 9.1347 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.4205 - mse: 124.4205 - mae: 9.1346 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4182 - mse: 124.4182 - mae: 9.1345 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4219 - mse: 124.4219 - mae: 9.1347 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4201 - mse: 124.4201 - mae: 9.1342 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4205 - mse: 124.4205 - mae: 9.1347 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1345 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1347 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4203 - mse: 124.4203 - mae: 9.1349 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4164 - mse: 124.4164 - mae: 9.1341 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4211 - mse: 124.4211 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 37s 270ms/step - loss: 124.4186 - mse: 124.4186 - mae: 9.1348 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 19s 142ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1345 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 124.4143 - mse: 124.4143 - mae: 9.1341 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 45s 331ms/step - loss: 124.4188 - mse: 124.4188 - mae: 9.1346 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 42s 309ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1347 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 25s 185ms/step - loss: 124.4190 - mse: 124.4190 - mae: 9.1349 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 41s 301ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1346 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 33s 244ms/step - loss: 124.4221 - mse: 124.4221 - mae: 9.1349 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0937\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 20s 144ms/step - loss: 124.4163 - mse: 124.4163 - mae: 9.1348 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4189 - mse: 124.4189 - mae: 9.1347 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 124.4186 - mse: 124.4186 - mae: 9.1350 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1347 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1348 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1348 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1346 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4188 - mse: 124.4188 - mae: 9.1348 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1345 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0937\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1345 - val_loss: 122.6805 - val_mse: 122.6805 - val_mae: 9.0937\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4188 - mse: 124.4188 - mae: 9.1347 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1350 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1349 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1345 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 124.4187 - mse: 124.4187 - mae: 9.1345 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 128/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1348 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1346 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.4190 - mse: 124.4190 - mae: 9.1348 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1344 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 23s 172ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1345 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4163 - mse: 124.4163 - mae: 9.1348 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1345 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1348 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 124.4207 - mse: 124.4207 - mae: 9.1346 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 35s 259ms/step - loss: 124.4117 - mse: 124.4117 - mae: 9.1342 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 124.4202 - mse: 124.4202 - mae: 9.1342 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1350 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 74s 544ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1344 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 49s 354ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1344 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4160 - mse: 124.4160 - mae: 9.1349 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1346 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0939\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4197 - mse: 124.4197 - mae: 9.1347 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 50s 373ms/step - loss: 124.4190 - mse: 124.4190 - mae: 9.1349 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4182 - mse: 124.4182 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1345 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 35s 258ms/step - loss: 124.4249 - mse: 124.4249 - mae: 9.1352 - val_loss: 122.6807 - val_mse: 122.6807 - val_mae: 9.0936\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 120s 885ms/step - loss: 124.4173 - mse: 124.4173 - mae: 9.1344 - val_loss: 122.6815 - val_mse: 122.6815 - val_mae: 9.0936\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 54s 395ms/step - loss: 124.4190 - mse: 124.4190 - mae: 9.1343 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 87s 642ms/step - loss: 124.4158 - mse: 124.4158 - mae: 9.1345 - val_loss: 122.6825 - val_mse: 122.6825 - val_mae: 9.0935\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 34s 248ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1349 - val_loss: 122.6806 - val_mse: 122.6806 - val_mae: 9.0936\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 42s 307ms/step - loss: 124.4140 - mse: 124.4140 - mae: 9.1340 - val_loss: 122.6829 - val_mse: 122.6829 - val_mae: 9.0935\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 31s 229ms/step - loss: 124.4202 - mse: 124.4202 - mae: 9.1347 - val_loss: 122.6815 - val_mse: 122.6815 - val_mae: 9.0936\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1348 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 160/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1348 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1342 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4168 - mse: 124.4168 - mae: 9.1348 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4156 - mse: 124.4156 - mae: 9.1350 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1345 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1347 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 124.4213 - mse: 124.4213 - mae: 9.1347 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 124.4153 - mse: 124.4153 - mae: 9.1338 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4206 - mse: 124.4206 - mae: 9.1345 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1348 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 124.4169 - mse: 124.4169 - mae: 9.1348 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1347 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 174/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1349 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1347 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4199 - mse: 124.4199 - mae: 9.1347 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4186 - mse: 124.4186 - mae: 9.1346 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1346 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1347 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 39s 286ms/step - loss: 124.4158 - mse: 124.4158 - mae: 9.1347 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 50s 366ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1349 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 124.4199 - mse: 124.4199 - mae: 9.1347 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1348 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4212 - mse: 124.4212 - mae: 9.1349 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.4190 - mse: 124.4190 - mae: 9.1346 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 30s 219ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1346 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 39s 290ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1348 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 34s 254ms/step - loss: 124.4142 - mse: 124.4142 - mae: 9.1341 - val_loss: 122.6804 - val_mse: 122.6804 - val_mae: 9.0937\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1350 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 53s 390ms/step - loss: 124.4186 - mse: 124.4186 - mae: 9.1348 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4187 - mse: 124.4187 - mae: 9.1345 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4172 - mse: 124.4172 - mae: 9.1347 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4148 - mse: 124.4148 - mae: 9.1344 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1345 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 23s 172ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1347 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1343 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1346 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 198/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1346 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1346 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1348 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 39s 289ms/step - loss: 124.4168 - mse: 124.4168 - mae: 9.1345 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 50s 361ms/step - loss: 124.4189 - mse: 124.4189 - mae: 9.1349 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 40s 297ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4169 - mse: 124.4169 - mae: 9.1349 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1347 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 206/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1348 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1350 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 37s 272ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1346 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1347 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1347 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1348 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 212/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4144 - mse: 124.4144 - mae: 9.1344 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0938\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1349 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1348 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4138 - mse: 124.4138 - mae: 9.1346 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1347 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4202 - mse: 124.4202 - mae: 9.1346 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4170 - mse: 124.4170 - mae: 9.1347 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4208 - mse: 124.4208 - mae: 9.1349 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 220/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1345 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4173 - mse: 124.4173 - mae: 9.1347 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1341 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 24s 175ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 225/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4182 - mse: 124.4182 - mae: 9.1347 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1344 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4173 - mse: 124.4173 - mae: 9.1347 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4202 - mse: 124.4202 - mae: 9.1351 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1343 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1349 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 231/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4200 - mse: 124.4200 - mae: 9.1347 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1349 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1346 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4208 - mse: 124.4208 - mae: 9.1347 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1348 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.4170 - mse: 124.4170 - mae: 9.1354 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4189 - mse: 124.4189 - mae: 9.1346 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 239/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1347 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1344 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 20s 145ms/step - loss: 124.4170 - mse: 124.4170 - mae: 9.1346 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4197 - mse: 124.4197 - mae: 9.1347 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1347 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 244/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1345 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1348 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4242 - mse: 124.4242 - mae: 9.1346 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1348 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4205 - mse: 124.4205 - mae: 9.1352 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0939\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1349 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 124.4201 - mse: 124.4201 - mae: 9.1343 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1350 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 252/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4170 - mse: 124.4170 - mae: 9.1347 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4158 - mse: 124.4158 - mae: 9.1343 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4172 - mse: 124.4172 - mae: 9.1349 - val_loss: 122.6812 - val_mse: 122.6812 - val_mae: 9.0936\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 124.4214 - mse: 124.4214 - mae: 9.1348 - val_loss: 122.6807 - val_mse: 122.6807 - val_mae: 9.0936\n",
      "Epoch 256/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1349 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1347 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 31s 229ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1349 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.4207 - mse: 124.4207 - mae: 9.1346 - val_loss: 122.6799 - val_mse: 122.6799 - val_mae: 9.0937\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1346 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1344 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.4187 - mse: 124.4187 - mae: 9.1345 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1349 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4160 - mse: 124.4160 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4165 - mse: 124.4165 - mae: 9.1349 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 266/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 54s 400ms/step - loss: 124.4155 - mse: 124.4155 - mae: 9.1342 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4186 - mse: 124.4186 - mae: 9.1349 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1350 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1347 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0939\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.4148 - mse: 124.4148 - mae: 9.1346 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 272/300\n",
      "136/136 [==============================] - 20s 145ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1347 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1349 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 24s 173ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1349 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.4122 - mse: 124.4122 - mae: 9.1347 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4163 - mse: 124.4163 - mae: 9.1343 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 277/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1349 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1347 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1349 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4187 - mse: 124.4187 - mae: 9.1350 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1348 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1348 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1347 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 285/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4210 - mse: 124.4210 - mae: 9.1348 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1346 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 35s 256ms/step - loss: 124.4219 - mse: 124.4219 - mae: 9.1349 - val_loss: 122.6802 - val_mse: 122.6802 - val_mae: 9.0937\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1348 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1350 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 290/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4141 - mse: 124.4141 - mae: 9.1347 - val_loss: 122.6825 - val_mse: 122.6825 - val_mae: 9.0935\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4190 - mse: 124.4190 - mae: 9.1349 - val_loss: 122.6810 - val_mse: 122.6810 - val_mae: 9.0936\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4182 - mse: 124.4182 - mae: 9.1348 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1341 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 21s 151ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1348 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0938\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1348 - val_loss: 122.6802 - val_mse: 122.6802 - val_mae: 9.0937\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1348 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4220 - mse: 124.4220 - mae: 9.1345 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 298/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1350 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 300/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1346 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "57/57 [==============================] - 9s 29ms/step - loss: 133.8866 - mse: 133.8866 - mae: 9.4419\n",
      "57/57 [==============================] - 6s 29ms/step\n",
      "MAE: 9.441896534334278\n",
      "MSE: 133.88654823556297\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_15 (LSTM)              (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 60, 240)          173760    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirecti  (None, 120)              144480    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 61s 135ms/step - loss: 5605.9585 - mse: 5605.9585 - mae: 73.9403 - val_loss: 4665.8037 - val_mse: 4665.8037 - val_mae: 67.4027\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 21s 153ms/step - loss: 4370.6582 - mse: 4370.6582 - mae: 65.1511 - val_loss: 4144.7153 - val_mse: 4144.7153 - val_mae: 63.4195\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 4031.1711 - mse: 4031.1711 - mae: 62.5022 - val_loss: 3912.7849 - val_mse: 3912.7849 - val_mae: 61.5639\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3836.4211 - mse: 3836.4211 - mae: 60.9274 - val_loss: 3749.2122 - val_mse: 3749.2122 - val_mae: 60.2207\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 25s 185ms/step - loss: 3691.7715 - mse: 3691.7715 - mae: 59.7259 - val_loss: 3622.4905 - val_mse: 3622.4905 - val_mae: 59.1592\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 3571.7314 - mse: 3571.7314 - mae: 58.7122 - val_loss: 3507.2544 - val_mse: 3507.2544 - val_mae: 58.1771\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 3461.2566 - mse: 3461.2566 - mae: 57.7652 - val_loss: 3403.3188 - val_mse: 3403.3188 - val_mae: 57.2769\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 3360.9990 - mse: 3360.9990 - mae: 56.8902 - val_loss: 3306.6040 - val_mse: 3306.6040 - val_mae: 56.4263\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 3266.5081 - mse: 3266.5081 - mae: 56.0532 - val_loss: 3214.4514 - val_mse: 3214.4514 - val_mae: 55.6037\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 3175.9062 - mse: 3175.9062 - mae: 55.2406 - val_loss: 3125.7336 - val_mse: 3125.7336 - val_mae: 54.8001\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 3088.3794 - mse: 3088.3794 - mae: 54.4411 - val_loss: 3039.5530 - val_mse: 3039.5530 - val_mae: 54.0081\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 3001.8816 - mse: 3001.8816 - mae: 53.6421 - val_loss: 2954.1067 - val_mse: 2954.1067 - val_mae: 53.2112\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 2918.5833 - mse: 2918.5833 - mae: 52.8612 - val_loss: 2872.5793 - val_mse: 2872.5793 - val_mae: 52.4395\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2838.0959 - mse: 2838.0959 - mae: 52.0932 - val_loss: 2793.1785 - val_mse: 2793.1785 - val_mae: 51.6769\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2756.8650 - mse: 2756.8650 - mae: 51.3066 - val_loss: 2710.7849 - val_mse: 2710.7849 - val_mae: 50.8735\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 2677.1033 - mse: 2677.1033 - mae: 50.5252 - val_loss: 2633.4722 - val_mse: 2633.4722 - val_mae: 50.1078\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2600.8281 - mse: 2600.8281 - mae: 49.7634 - val_loss: 2558.3459 - val_mse: 2558.3459 - val_mae: 49.3525\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2526.5432 - mse: 2526.5432 - mae: 49.0119 - val_loss: 2485.0051 - val_mse: 2485.0051 - val_mae: 48.6038\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2453.9526 - mse: 2453.9526 - mae: 48.2642 - val_loss: 2413.2693 - val_mse: 2413.2693 - val_mae: 47.8601\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2382.9397 - mse: 2382.9397 - mae: 47.5247 - val_loss: 2343.0923 - val_mse: 2343.0923 - val_mae: 47.1213\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 2312.8743 - mse: 2312.8743 - mae: 46.7799 - val_loss: 2271.3728 - val_mse: 2271.3728 - val_mae: 46.3540\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 2240.9343 - mse: 2240.9343 - mae: 46.0050 - val_loss: 2201.6399 - val_mse: 2201.6399 - val_mae: 45.5956\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 2172.6030 - mse: 2172.6030 - mae: 45.2558 - val_loss: 2134.4231 - val_mse: 2134.4231 - val_mae: 44.8525\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 2106.2659 - mse: 2106.2659 - mae: 44.5167 - val_loss: 2069.0020 - val_mse: 2069.0020 - val_mae: 44.1172\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 31s 227ms/step - loss: 2041.4905 - mse: 2041.4905 - mae: 43.7837 - val_loss: 2005.0576 - val_mse: 2005.0576 - val_mae: 43.3864\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 1978.1611 - mse: 1978.1611 - mae: 43.0537 - val_loss: 1942.4125 - val_mse: 1942.4125 - val_mae: 42.6584\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1916.1823 - mse: 1916.1823 - mae: 42.3291 - val_loss: 1881.1710 - val_mse: 1881.1710 - val_mae: 41.9344\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1855.5256 - mse: 1855.5256 - mae: 41.6061 - val_loss: 1821.2523 - val_mse: 1821.2523 - val_mae: 41.2138\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1796.1699 - mse: 1796.1699 - mae: 40.8866 - val_loss: 1762.4961 - val_mse: 1762.4961 - val_mae: 40.4947\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1738.0707 - mse: 1738.0707 - mae: 40.1679 - val_loss: 1705.1278 - val_mse: 1705.1278 - val_mae: 39.7800\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1681.2003 - mse: 1681.2003 - mae: 39.4568 - val_loss: 1648.9177 - val_mse: 1648.9177 - val_mae: 39.0671\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1625.5370 - mse: 1625.5370 - mae: 38.7441 - val_loss: 1593.9049 - val_mse: 1593.9049 - val_mae: 38.3566\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1571.1025 - mse: 1571.1025 - mae: 38.0328 - val_loss: 1540.0123 - val_mse: 1540.0123 - val_mae: 37.6475\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1517.8262 - mse: 1517.8262 - mae: 37.3286 - val_loss: 1487.4952 - val_mse: 1487.4952 - val_mae: 36.9434\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1465.7235 - mse: 1465.7235 - mae: 36.6239 - val_loss: 1436.0317 - val_mse: 1436.0317 - val_mae: 36.2402\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1414.7064 - mse: 1414.7064 - mae: 35.9207 - val_loss: 1385.2476 - val_mse: 1385.2476 - val_mae: 35.5326\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1362.9425 - mse: 1362.9425 - mae: 35.1912 - val_loss: 1333.2976 - val_mse: 1333.2976 - val_mae: 34.7940\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1312.5265 - mse: 1312.5265 - mae: 34.4698 - val_loss: 1284.2157 - val_mse: 1284.2157 - val_mae: 34.0813\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1264.1166 - mse: 1264.1166 - mae: 33.7577 - val_loss: 1236.5057 - val_mse: 1236.5057 - val_mae: 33.3741\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 1217.0746 - mse: 1217.0746 - mae: 33.0558 - val_loss: 1190.1777 - val_mse: 1190.1777 - val_mae: 32.6726\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 1171.2849 - mse: 1171.2849 - mae: 32.3557 - val_loss: 1145.0094 - val_mse: 1145.0094 - val_mae: 31.9739\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1126.7065 - mse: 1126.7065 - mae: 31.6565 - val_loss: 1101.0253 - val_mse: 1101.0253 - val_mae: 31.2785\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1083.2939 - mse: 1083.2939 - mae: 30.9641 - val_loss: 1058.2870 - val_mse: 1058.2870 - val_mae: 30.5877\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 1041.0132 - mse: 1041.0132 - mae: 30.2762 - val_loss: 1016.6221 - val_mse: 1016.6221 - val_mae: 29.8989\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 999.8472 - mse: 999.8472 - mae: 29.5890 - val_loss: 976.0513 - val_mse: 976.0513 - val_mae: 29.2126\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 959.7979 - mse: 959.7979 - mae: 28.9026 - val_loss: 936.6121 - val_mse: 936.6121 - val_mae: 28.5295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 920.8485 - mse: 920.8485 - mae: 28.2229 - val_loss: 898.2286 - val_mse: 898.2286 - val_mae: 27.8487\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 882.9980 - mse: 882.9980 - mae: 27.5399 - val_loss: 860.9433 - val_mse: 860.9433 - val_mae: 27.1711\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 846.2163 - mse: 846.2163 - mae: 26.8669 - val_loss: 824.8615 - val_mse: 824.8615 - val_mae: 26.4988\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 810.5145 - mse: 810.5145 - mae: 26.1948 - val_loss: 789.6284 - val_mse: 789.6284 - val_mae: 25.8254\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 775.8533 - mse: 775.8533 - mae: 25.5229 - val_loss: 755.6172 - val_mse: 755.6172 - val_mae: 25.1583\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 742.2647 - mse: 742.2647 - mae: 24.8588 - val_loss: 722.5822 - val_mse: 722.5822 - val_mae: 24.4971\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 709.7173 - mse: 709.7173 - mae: 24.2014 - val_loss: 690.5179 - val_mse: 690.5179 - val_mae: 23.8506\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 678.1898 - mse: 678.1898 - mae: 23.5748 - val_loss: 659.6704 - val_mse: 659.6704 - val_mae: 23.2341\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 647.7064 - mse: 647.7064 - mae: 22.9593 - val_loss: 629.6770 - val_mse: 629.6770 - val_mae: 22.6175\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 618.2286 - mse: 618.2286 - mae: 22.3458 - val_loss: 600.7290 - val_mse: 600.7290 - val_mae: 22.0048\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 36s 268ms/step - loss: 589.7678 - mse: 589.7678 - mae: 21.7335 - val_loss: 572.8051 - val_mse: 572.8051 - val_mae: 21.3959\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 562.3071 - mse: 562.3071 - mae: 21.1314 - val_loss: 545.8781 - val_mse: 545.8781 - val_mae: 20.7987\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 535.8373 - mse: 535.8373 - mae: 20.5389 - val_loss: 519.9462 - val_mse: 519.9462 - val_mae: 20.2114\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 510.3462 - mse: 510.3462 - mae: 19.9650 - val_loss: 495.0117 - val_mse: 495.0117 - val_mae: 19.6423\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 485.8498 - mse: 485.8498 - mae: 19.4004 - val_loss: 470.9730 - val_mse: 470.9730 - val_mae: 19.0841\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 29s 216ms/step - loss: 462.2941 - mse: 462.2941 - mae: 18.8580 - val_loss: 447.9988 - val_mse: 447.9988 - val_mae: 18.5382\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 439.7109 - mse: 439.7109 - mae: 18.3217 - val_loss: 425.8391 - val_mse: 425.8391 - val_mae: 18.0131\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 418.0591 - mse: 418.0591 - mae: 17.8031 - val_loss: 404.7307 - val_mse: 404.7307 - val_mae: 17.4971\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 397.3395 - mse: 397.3395 - mae: 17.2943 - val_loss: 384.5256 - val_mse: 384.5256 - val_mae: 16.9880\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 377.5435 - mse: 377.5435 - mae: 16.8005 - val_loss: 365.2561 - val_mse: 365.2561 - val_mae: 16.5247\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 29s 214ms/step - loss: 358.6694 - mse: 358.6694 - mae: 16.3490 - val_loss: 346.8595 - val_mse: 346.8595 - val_mae: 16.0752\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 340.6883 - mse: 340.6883 - mae: 15.8976 - val_loss: 329.3527 - val_mse: 329.3527 - val_mae: 15.6300\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 323.5968 - mse: 323.5968 - mae: 15.4514 - val_loss: 312.7053 - val_mse: 312.7053 - val_mae: 15.1918\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 307.3604 - mse: 307.3604 - mae: 15.0205 - val_loss: 296.9809 - val_mse: 296.9809 - val_mae: 14.7634\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 291.9962 - mse: 291.9962 - mae: 14.6036 - val_loss: 281.9969 - val_mse: 281.9969 - val_mae: 14.3736\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 23s 166ms/step - loss: 277.4586 - mse: 277.4586 - mae: 14.2239 - val_loss: 268.0360 - val_mse: 268.0360 - val_mae: 14.0055\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 263.7605 - mse: 263.7605 - mae: 13.8524 - val_loss: 254.7270 - val_mse: 254.7270 - val_mae: 13.6377\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 22s 166ms/step - loss: 250.8557 - mse: 250.8557 - mae: 13.4863 - val_loss: 242.2670 - val_mse: 242.2670 - val_mae: 13.2773\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 238.7411 - mse: 238.7411 - mae: 13.1256 - val_loss: 230.5996 - val_mse: 230.5996 - val_mae: 12.9263\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 227.4126 - mse: 227.4126 - mae: 12.7781 - val_loss: 219.6293 - val_mse: 219.6293 - val_mae: 12.5785\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 216.8412 - mse: 216.8412 - mae: 12.4300 - val_loss: 209.4900 - val_mse: 209.4900 - val_mae: 12.2390\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 207.0219 - mse: 207.0219 - mae: 12.0943 - val_loss: 199.9953 - val_mse: 199.9953 - val_mae: 11.9149\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 197.8965 - mse: 197.8965 - mae: 11.7834 - val_loss: 191.3209 - val_mse: 191.3209 - val_mae: 11.6123\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 189.4723 - mse: 189.4723 - mae: 11.4909 - val_loss: 183.2798 - val_mse: 183.2798 - val_mae: 11.3379\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 181.7189 - mse: 181.7189 - mae: 11.2323 - val_loss: 175.8710 - val_mse: 175.8710 - val_mae: 11.0877\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 174.6137 - mse: 174.6137 - mae: 10.9892 - val_loss: 169.1221 - val_mse: 169.1221 - val_mae: 10.8535\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 168.1387 - mse: 168.1387 - mae: 10.7672 - val_loss: 162.9784 - val_mse: 162.9784 - val_mae: 10.6339\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 162.2623 - mse: 162.2623 - mae: 10.5612 - val_loss: 157.4414 - val_mse: 157.4414 - val_mae: 10.4371\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 156.9589 - mse: 156.9589 - mae: 10.3866 - val_loss: 152.4271 - val_mse: 152.4271 - val_mae: 10.2731\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 152.1998 - mse: 152.1998 - mae: 10.2232 - val_loss: 147.9660 - val_mse: 147.9660 - val_mae: 10.1163\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 147.9580 - mse: 147.9580 - mae: 10.0752 - val_loss: 144.0028 - val_mse: 144.0028 - val_mae: 9.9803\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 144.2075 - mse: 144.2075 - mae: 9.9404 - val_loss: 140.4936 - val_mse: 140.4936 - val_mae: 9.8490\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 140.9065 - mse: 140.9065 - mae: 9.8126 - val_loss: 137.4554 - val_mse: 137.4554 - val_mae: 9.7381\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 138.0183 - mse: 138.0183 - mae: 9.7095 - val_loss: 134.8101 - val_mse: 134.8101 - val_mae: 9.6418\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 135.5282 - mse: 135.5282 - mae: 9.6127 - val_loss: 132.4807 - val_mse: 132.4807 - val_mae: 9.5480\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 133.3985 - mse: 133.3985 - mae: 9.5229 - val_loss: 130.5050 - val_mse: 130.5050 - val_mae: 9.4682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 131.5942 - mse: 131.5942 - mae: 9.4517 - val_loss: 128.8992 - val_mse: 128.8992 - val_mae: 9.3980\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 130.0849 - mse: 130.0849 - mae: 9.3805 - val_loss: 127.5389 - val_mse: 127.5389 - val_mae: 9.3309\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 128.8306 - mse: 128.8306 - mae: 9.3192 - val_loss: 126.4394 - val_mse: 126.4394 - val_mae: 9.2838\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 127.8079 - mse: 127.8079 - mae: 9.2880 - val_loss: 125.5366 - val_mse: 125.5366 - val_mae: 9.2564\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 126.9861 - mse: 126.9861 - mae: 9.2631 - val_loss: 124.8122 - val_mse: 124.8122 - val_mae: 9.2310\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 126.3296 - mse: 126.3296 - mae: 9.2423 - val_loss: 124.2485 - val_mse: 124.2485 - val_mae: 9.2081\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 125.8220 - mse: 125.8220 - mae: 9.2226 - val_loss: 123.7955 - val_mse: 123.7955 - val_mae: 9.1865\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 125.4354 - mse: 125.4354 - mae: 9.2047 - val_loss: 123.4655 - val_mse: 123.4655 - val_mae: 9.1709\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 125.1346 - mse: 125.1346 - mae: 9.1945 - val_loss: 123.2397 - val_mse: 123.2397 - val_mae: 9.1582\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.9143 - mse: 124.9143 - mae: 9.1834 - val_loss: 123.0527 - val_mse: 123.0527 - val_mae: 9.1456\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.7521 - mse: 124.7521 - mae: 9.1729 - val_loss: 122.9269 - val_mse: 122.9269 - val_mae: 9.1352\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.6410 - mse: 124.6410 - mae: 9.1654 - val_loss: 122.8375 - val_mse: 122.8375 - val_mae: 9.1261\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5621 - mse: 124.5621 - mae: 9.1575 - val_loss: 122.7742 - val_mse: 122.7742 - val_mae: 9.1179\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.5125 - mse: 124.5125 - mae: 9.1508 - val_loss: 122.7320 - val_mse: 122.7320 - val_mae: 9.1108\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4716 - mse: 124.4716 - mae: 9.1467 - val_loss: 122.7150 - val_mse: 122.7150 - val_mae: 9.1071\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4518 - mse: 124.4518 - mae: 9.1429 - val_loss: 122.6977 - val_mse: 122.6977 - val_mae: 9.1023\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4349 - mse: 124.4349 - mae: 9.1385 - val_loss: 122.6875 - val_mse: 122.6875 - val_mae: 9.0984\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4252 - mse: 124.4252 - mae: 9.1373 - val_loss: 122.6827 - val_mse: 122.6827 - val_mae: 9.0958\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4234 - mse: 124.4234 - mae: 9.1361 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0945\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4173 - mse: 124.4173 - mae: 9.1347 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4195 - mse: 124.4195 - mae: 9.1346 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4149 - mse: 124.4149 - mae: 9.1345 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4120 - mse: 124.4120 - mae: 9.1343 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4142 - mse: 124.4142 - mae: 9.1344 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4119 - mse: 124.4119 - mae: 9.1345 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1346 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 124.4139 - mse: 124.4139 - mae: 9.1345 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 124.4217 - mse: 124.4217 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1346 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4155 - mse: 124.4155 - mae: 9.1345 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4192 - mse: 124.4192 - mae: 9.1347 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1349 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4163 - mse: 124.4163 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1346 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 124.4239 - mse: 124.4239 - mae: 9.1347 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0939\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1346 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4141 - mse: 124.4141 - mae: 9.1346 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4168 - mse: 124.4168 - mae: 9.1347 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1348 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4116 - mse: 124.4116 - mae: 9.1344 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4149 - mse: 124.4149 - mae: 9.1344 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4201 - mse: 124.4201 - mae: 9.1347 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4163 - mse: 124.4163 - mae: 9.1345 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4209 - mse: 124.4209 - mae: 9.1346 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4139 - mse: 124.4139 - mae: 9.1346 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1345 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 139/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 24s 174ms/step - loss: 124.4150 - mse: 124.4150 - mae: 9.1346 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 124.4208 - mse: 124.4208 - mae: 9.1346 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1343 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1349 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1346 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 124.4135 - mse: 124.4135 - mae: 9.1343 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1347 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 23s 166ms/step - loss: 124.4162 - mse: 124.4162 - mae: 9.1346 - val_loss: 122.6804 - val_mse: 122.6804 - val_mae: 9.0937\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 21s 153ms/step - loss: 124.4129 - mse: 124.4129 - mae: 9.1345 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 124.4158 - mse: 124.4158 - mae: 9.1347 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1348 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 124.4137 - mse: 124.4137 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4255 - mse: 124.4255 - mae: 9.1352 - val_loss: 122.6802 - val_mse: 122.6802 - val_mae: 9.0937\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4178 - mse: 124.4178 - mae: 9.1342 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4198 - mse: 124.4198 - mae: 9.1346 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1345 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4129 - mse: 124.4129 - mae: 9.1346 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4285 - mse: 124.4285 - mae: 9.1345 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4160 - mse: 124.4160 - mae: 9.1346 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4215 - mse: 124.4215 - mae: 9.1348 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4121 - mse: 124.4121 - mae: 9.1345 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 160/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4161 - mse: 124.4161 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4201 - mse: 124.4201 - mae: 9.1341 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4225 - mse: 124.4225 - mae: 9.1351 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4169 - mse: 124.4169 - mae: 9.1349 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4153 - mse: 124.4153 - mae: 9.1347 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 124.4169 - mse: 124.4169 - mae: 9.1346 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1348 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4210 - mse: 124.4210 - mae: 9.1343 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4206 - mse: 124.4206 - mae: 9.1346 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4138 - mse: 124.4138 - mae: 9.1347 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4146 - mse: 124.4146 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1346 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4153 - mse: 124.4153 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4156 - mse: 124.4156 - mae: 9.1346 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4223 - mse: 124.4223 - mae: 9.1349 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4205 - mse: 124.4205 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4151 - mse: 124.4151 - mae: 9.1347 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 21s 155ms/step - loss: 124.4162 - mse: 124.4162 - mae: 9.1347 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 23s 173ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1348 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0938\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4198 - mse: 124.4198 - mae: 9.1348 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4186 - mse: 124.4186 - mae: 9.1345 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4137 - mse: 124.4137 - mae: 9.1347 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4218 - mse: 124.4218 - mae: 9.1349 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 185/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1344 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4133 - mse: 124.4133 - mae: 9.1344 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4139 - mse: 124.4139 - mae: 9.1345 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0938\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4196 - mse: 124.4196 - mae: 9.1344 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4300 - mse: 124.4300 - mae: 9.1354 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4176 - mse: 124.4176 - mae: 9.1348 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4182 - mse: 124.4182 - mae: 9.1346 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4233 - mse: 124.4233 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 21s 154ms/step - loss: 124.4345 - mse: 124.4345 - mae: 9.1351 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1347 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 124.4139 - mse: 124.4139 - mae: 9.1345 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4169 - mse: 124.4169 - mae: 9.1342 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 85s 629ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1345 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 198/300\n",
      "136/136 [==============================] - 32s 232ms/step - loss: 124.4161 - mse: 124.4161 - mae: 9.1346 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1346 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4160 - mse: 124.4160 - mae: 9.1348 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1348 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 124.4235 - mse: 124.4235 - mae: 9.1350 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1345 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.4177 - mse: 124.4177 - mae: 9.1349 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1348 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 206/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 124.4146 - mse: 124.4146 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 34s 253ms/step - loss: 124.4164 - mse: 124.4164 - mae: 9.1348 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 35s 254ms/step - loss: 124.4198 - mse: 124.4198 - mae: 9.1346 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 34s 252ms/step - loss: 124.4245 - mse: 124.4245 - mae: 9.1349 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 35s 255ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1347 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 124.4140 - mse: 124.4140 - mae: 9.1347 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 212/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 124.4206 - mse: 124.4206 - mae: 9.1344 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 124.4122 - mse: 124.4122 - mae: 9.1346 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1347 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4222 - mse: 124.4222 - mae: 9.1347 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4172 - mse: 124.4172 - mae: 9.1347 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1345 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4135 - mse: 124.4135 - mae: 9.1346 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 124.4159 - mse: 124.4159 - mae: 9.1346 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 220/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4131 - mse: 124.4131 - mae: 9.1344 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 57s 423ms/step - loss: 124.4203 - mse: 124.4203 - mae: 9.1348 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 61s 443ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1342 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 33s 241ms/step - loss: 124.4152 - mse: 124.4152 - mae: 9.1346 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1348 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 225/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4211 - mse: 124.4211 - mae: 9.1350 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4155 - mse: 124.4155 - mae: 9.1342 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4204 - mse: 124.4204 - mae: 9.1347 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0936\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4140 - mse: 124.4140 - mae: 9.1347 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4219 - mse: 124.4219 - mae: 9.1345 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4198 - mse: 124.4198 - mae: 9.1348 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 231/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 24s 180ms/step - loss: 124.4138 - mse: 124.4138 - mae: 9.1345 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4151 - mse: 124.4151 - mae: 9.1347 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 124.4130 - mse: 124.4130 - mae: 9.1345 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 124.4262 - mse: 124.4262 - mae: 9.1349 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 124.4138 - mse: 124.4138 - mae: 9.1345 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 25s 185ms/step - loss: 124.4205 - mse: 124.4205 - mae: 9.1354 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 124.4131 - mse: 124.4131 - mae: 9.1344 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 124.4147 - mse: 124.4147 - mae: 9.1346 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 239/300\n",
      "136/136 [==============================] - 36s 262ms/step - loss: 124.4142 - mse: 124.4142 - mae: 9.1346 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 44s 321ms/step - loss: 124.4188 - mse: 124.4188 - mae: 9.1347 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 32s 232ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1347 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0938\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 33s 239ms/step - loss: 124.4146 - mse: 124.4146 - mae: 9.1346 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 39s 287ms/step - loss: 124.4145 - mse: 124.4145 - mae: 9.1347 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 244/300\n",
      "136/136 [==============================] - 31s 229ms/step - loss: 124.4151 - mse: 124.4151 - mae: 9.1344 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4159 - mse: 124.4159 - mae: 9.1347 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 124.4198 - mse: 124.4198 - mae: 9.1344 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 31s 226ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1347 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 39s 283ms/step - loss: 124.4187 - mse: 124.4187 - mae: 9.1349 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 37s 270ms/step - loss: 124.4165 - mse: 124.4165 - mae: 9.1349 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 35s 256ms/step - loss: 124.4181 - mse: 124.4181 - mae: 9.1344 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 38s 279ms/step - loss: 124.4197 - mse: 124.4197 - mae: 9.1350 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 252/300\n",
      "136/136 [==============================] - 37s 270ms/step - loss: 124.4160 - mse: 124.4160 - mae: 9.1346 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 33s 241ms/step - loss: 124.4162 - mse: 124.4162 - mae: 9.1344 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 43s 320ms/step - loss: 124.4135 - mse: 124.4135 - mae: 9.1346 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 43s 317ms/step - loss: 124.4179 - mse: 124.4179 - mae: 9.1347 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 256/300\n",
      "136/136 [==============================] - 35s 259ms/step - loss: 124.4167 - mse: 124.4167 - mae: 9.1348 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 34s 250ms/step - loss: 124.4215 - mse: 124.4215 - mae: 9.1350 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 33s 245ms/step - loss: 124.4172 - mse: 124.4172 - mae: 9.1348 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1345 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 77s 567ms/step - loss: 124.4136 - mse: 124.4136 - mae: 9.1344 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 147s 1s/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1346 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4173 - mse: 124.4173 - mae: 9.1346 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1350 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 40s 293ms/step - loss: 124.4194 - mse: 124.4194 - mae: 9.1346 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 29s 213ms/step - loss: 124.4188 - mse: 124.4188 - mae: 9.1349 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 266/300\n",
      "136/136 [==============================] - 31s 226ms/step - loss: 124.4153 - mse: 124.4153 - mae: 9.1343 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0939\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 31s 227ms/step - loss: 124.4116 - mse: 124.4116 - mae: 9.1346 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 33s 239ms/step - loss: 124.4145 - mse: 124.4145 - mae: 9.1345 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1348 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4184 - mse: 124.4184 - mae: 9.1346 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 39s 283ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1346 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 272/300\n",
      "136/136 [==============================] - 37s 273ms/step - loss: 124.4243 - mse: 124.4243 - mae: 9.1350 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 33s 242ms/step - loss: 124.4175 - mse: 124.4175 - mae: 9.1349 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0938\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4161 - mse: 124.4161 - mae: 9.1347 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0939\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 151s 1s/step - loss: 124.4215 - mse: 124.4215 - mae: 9.1351 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 115s 847ms/step - loss: 124.4189 - mse: 124.4189 - mae: 9.1344 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 277/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 30s 218ms/step - loss: 124.4212 - mse: 124.4212 - mae: 9.1349 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0941\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 124.4180 - mse: 124.4180 - mae: 9.1347 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 34s 253ms/step - loss: 124.4137 - mse: 124.4137 - mae: 9.1346 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 36s 261ms/step - loss: 124.4203 - mse: 124.4203 - mae: 9.1350 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4140 - mse: 124.4140 - mae: 9.1346 - val_loss: 122.6803 - val_mse: 122.6803 - val_mae: 9.0937\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.4153 - mse: 124.4153 - mae: 9.1346 - val_loss: 122.6797 - val_mse: 122.6797 - val_mae: 9.0937\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4139 - mse: 124.4139 - mae: 9.1346 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 32s 234ms/step - loss: 124.4168 - mse: 124.4168 - mae: 9.1347 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 285/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4153 - mse: 124.4153 - mae: 9.1344 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4173 - mse: 124.4173 - mae: 9.1345 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0938\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.4231 - mse: 124.4231 - mae: 9.1350 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 43s 320ms/step - loss: 124.4126 - mse: 124.4126 - mae: 9.1345 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0940\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4171 - mse: 124.4171 - mae: 9.1347 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "Epoch 290/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4161 - mse: 124.4161 - mae: 9.1345 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 69s 511ms/step - loss: 124.4111 - mse: 124.4111 - mae: 9.1344 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0938\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 36s 264ms/step - loss: 124.4145 - mse: 124.4145 - mae: 9.1345 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4157 - mse: 124.4157 - mae: 9.1342 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 32s 238ms/step - loss: 124.4160 - mse: 124.4160 - mae: 9.1347 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 35s 261ms/step - loss: 124.4166 - mse: 124.4166 - mae: 9.1347 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.4147 - mse: 124.4147 - mae: 9.1346 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 124.4252 - mse: 124.4252 - mae: 9.1348 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 298/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 124.4154 - mse: 124.4154 - mae: 9.1347 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 124.4229 - mse: 124.4229 - mae: 9.1347 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0938\n",
      "Epoch 300/300\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 124.4174 - mse: 124.4174 - mae: 9.1346 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0939\n",
      "57/57 [==============================] - 23s 35ms/step - loss: 133.8839 - mse: 133.8839 - mae: 9.4417\n",
      "57/57 [==============================] - 13s 29ms/step\n",
      "MAE: 9.441705180871441\n",
      "MSE: 133.88388144522258\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_12 (Bidirecti  (None, 60, 240)          173760    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_13 (Bidirecti  (None, 120)              144480    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 102s 135ms/step - loss: 4736.3408 - mse: 4736.3408 - mae: 67.3113 - val_loss: 2307.3447 - val_mse: 2307.3447 - val_mae: 46.7404\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 20s 144ms/step - loss: 839.0250 - mse: 839.0250 - mae: 24.9760 - val_loss: 217.1615 - val_mse: 217.1615 - val_mae: 12.4976\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 21s 152ms/step - loss: 153.0816 - mse: 153.0816 - mae: 10.2360 - val_loss: 125.6804 - val_mse: 125.6804 - val_mae: 9.2610\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 125.3400 - mse: 125.3400 - mae: 9.1965 - val_loss: 122.6857 - val_mse: 122.6857 - val_mae: 9.0976\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 20s 147ms/step - loss: 124.4100 - mse: 124.4100 - mae: 9.1377 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0939\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 124.4558 - mse: 124.4558 - mae: 9.1367 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0943\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 98s 725ms/step - loss: 124.4885 - mse: 124.4885 - mae: 9.1427 - val_loss: 122.6912 - val_mse: 122.6912 - val_mae: 9.0931\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 109s 803ms/step - loss: 124.5492 - mse: 124.5492 - mae: 9.1405 - val_loss: 122.6868 - val_mse: 122.6868 - val_mae: 9.0933\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 53s 388ms/step - loss: 124.4539 - mse: 124.4539 - mae: 9.1400 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 41s 299ms/step - loss: 124.4982 - mse: 124.4982 - mae: 9.1419 - val_loss: 122.7003 - val_mse: 122.7003 - val_mae: 9.0927\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 44s 318ms/step - loss: 124.4928 - mse: 124.4928 - mae: 9.1384 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 124.5274 - mse: 124.5274 - mae: 9.1420 - val_loss: 122.6984 - val_mse: 122.6984 - val_mae: 9.0928\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 124.4419 - mse: 124.4419 - mae: 9.1381 - val_loss: 122.7410 - val_mse: 122.7410 - val_mae: 9.0918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300\n",
      "136/136 [==============================] - 30s 219ms/step - loss: 124.4410 - mse: 124.4410 - mae: 9.1377 - val_loss: 122.7111 - val_mse: 122.7111 - val_mae: 9.0925\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 42s 313ms/step - loss: 124.4501 - mse: 124.4501 - mae: 9.1357 - val_loss: 122.6851 - val_mse: 122.6851 - val_mae: 9.0933\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.5434 - mse: 124.5434 - mae: 9.1429 - val_loss: 122.7256 - val_mse: 122.7256 - val_mae: 9.0921\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 124.4519 - mse: 124.4519 - mae: 9.1360 - val_loss: 122.6995 - val_mse: 122.6995 - val_mae: 9.0928\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 124.5070 - mse: 124.5070 - mae: 9.1409 - val_loss: 122.7027 - val_mse: 122.7027 - val_mae: 9.0927\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 41s 299ms/step - loss: 124.5247 - mse: 124.5247 - mae: 9.1401 - val_loss: 122.6810 - val_mse: 122.6810 - val_mae: 9.0936\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 143s 1s/step - loss: 124.4052 - mse: 124.4052 - mae: 9.1377 - val_loss: 122.7086 - val_mse: 122.7086 - val_mae: 9.0925\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 88s 642ms/step - loss: 124.4853 - mse: 124.4853 - mae: 9.1366 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0935\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 32s 238ms/step - loss: 124.5994 - mse: 124.5994 - mae: 9.1453 - val_loss: 122.6872 - val_mse: 122.6872 - val_mae: 9.0932\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 34s 249ms/step - loss: 124.4687 - mse: 124.4687 - mae: 9.1388 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 34s 248ms/step - loss: 124.4419 - mse: 124.4419 - mae: 9.1370 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 33s 245ms/step - loss: 124.4608 - mse: 124.4608 - mae: 9.1393 - val_loss: 122.6868 - val_mse: 122.6868 - val_mae: 9.0933\n",
      "Epoch 26/300\n",
      "136/136 [==============================] - 33s 244ms/step - loss: 124.4525 - mse: 124.4525 - mae: 9.1363 - val_loss: 122.6779 - val_mse: 122.6779 - val_mae: 9.0940\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 33s 245ms/step - loss: 124.4534 - mse: 124.4534 - mae: 9.1387 - val_loss: 122.6870 - val_mse: 122.6870 - val_mae: 9.0932\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 37s 272ms/step - loss: 124.5639 - mse: 124.5639 - mae: 9.1398 - val_loss: 122.6893 - val_mse: 122.6893 - val_mae: 9.0931\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 36s 267ms/step - loss: 124.4391 - mse: 124.4391 - mae: 9.1368 - val_loss: 122.6834 - val_mse: 122.6834 - val_mae: 9.0934\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 33s 246ms/step - loss: 124.5121 - mse: 124.5121 - mae: 9.1380 - val_loss: 122.6785 - val_mse: 122.6785 - val_mae: 9.0944\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 39s 289ms/step - loss: 124.4133 - mse: 124.4133 - mae: 9.1422 - val_loss: 122.6982 - val_mse: 122.6982 - val_mae: 9.0928\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 37s 270ms/step - loss: 124.4947 - mse: 124.4947 - mae: 9.1368 - val_loss: 122.6916 - val_mse: 122.6916 - val_mae: 9.0930\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 33s 243ms/step - loss: 124.5518 - mse: 124.5518 - mae: 9.1406 - val_loss: 122.6809 - val_mse: 122.6809 - val_mae: 9.0947\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 33s 243ms/step - loss: 124.5400 - mse: 124.5400 - mae: 9.1446 - val_loss: 122.6887 - val_mse: 122.6887 - val_mae: 9.0932\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 33s 244ms/step - loss: 124.5157 - mse: 124.5157 - mae: 9.1429 - val_loss: 122.6956 - val_mse: 122.6956 - val_mae: 9.0929\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 33s 243ms/step - loss: 124.3218 - mse: 124.3218 - mae: 9.1325 - val_loss: 122.6991 - val_mse: 122.6991 - val_mae: 9.0928\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 40s 292ms/step - loss: 124.4161 - mse: 124.4161 - mae: 9.1368 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 21s 156ms/step - loss: 124.4777 - mse: 124.4777 - mae: 9.1391 - val_loss: 122.7138 - val_mse: 122.7138 - val_mae: 9.0924\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4997 - mse: 124.4997 - mae: 9.1370 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5266 - mse: 124.5266 - mae: 9.1399 - val_loss: 122.7005 - val_mse: 122.7005 - val_mae: 9.0927\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5247 - mse: 124.5247 - mae: 9.1406 - val_loss: 122.6962 - val_mse: 122.6962 - val_mae: 9.0929\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4532 - mse: 124.4532 - mae: 9.1366 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5425 - mse: 124.5425 - mae: 9.1414 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0939\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5226 - mse: 124.5226 - mae: 9.1454 - val_loss: 122.7011 - val_mse: 122.7011 - val_mae: 9.0927\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5434 - mse: 124.5434 - mae: 9.1404 - val_loss: 122.7181 - val_mse: 122.7181 - val_mae: 9.0923\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4784 - mse: 124.4784 - mae: 9.1370 - val_loss: 122.6955 - val_mse: 122.6955 - val_mae: 9.0929\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.3908 - mse: 124.3908 - mae: 9.1398 - val_loss: 122.7199 - val_mse: 122.7199 - val_mae: 9.0922\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4158 - mse: 124.4158 - mae: 9.1349 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0939\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5214 - mse: 124.5214 - mae: 9.1423 - val_loss: 122.7031 - val_mse: 122.7031 - val_mae: 9.0927\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.5284 - mse: 124.5284 - mae: 9.1402 - val_loss: 122.7195 - val_mse: 122.7195 - val_mae: 9.0923\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 54s 396ms/step - loss: 124.4738 - mse: 124.4738 - mae: 9.1385 - val_loss: 122.7008 - val_mse: 122.7008 - val_mae: 9.0927\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 19s 142ms/step - loss: 124.5188 - mse: 124.5188 - mae: 9.1453 - val_loss: 122.7133 - val_mse: 122.7133 - val_mae: 9.0924\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5056 - mse: 124.5056 - mae: 9.1394 - val_loss: 122.6886 - val_mse: 122.6886 - val_mae: 9.0932\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.3823 - mse: 124.3823 - mae: 9.1347 - val_loss: 122.7107 - val_mse: 122.7107 - val_mae: 9.0925\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4894 - mse: 124.4894 - mae: 9.1397 - val_loss: 122.7172 - val_mse: 122.7172 - val_mae: 9.0923\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4606 - mse: 124.4606 - mae: 9.1386 - val_loss: 122.6945 - val_mse: 122.6945 - val_mae: 9.0929\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4795 - mse: 124.4795 - mae: 9.1362 - val_loss: 122.6872 - val_mse: 122.6872 - val_mae: 9.0932\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.4819 - mse: 124.4819 - mae: 9.1384 - val_loss: 122.6890 - val_mse: 122.6890 - val_mae: 9.0932\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5952 - mse: 124.5952 - mae: 9.1411 - val_loss: 122.6908 - val_mse: 122.6908 - val_mae: 9.0931\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 20s 150ms/step - loss: 124.5260 - mse: 124.5260 - mae: 9.1407 - val_loss: 122.7084 - val_mse: 122.7084 - val_mae: 9.0925\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 20s 151ms/step - loss: 124.4056 - mse: 124.4056 - mae: 9.1353 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0943\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4795 - mse: 124.4795 - mae: 9.1426 - val_loss: 122.7091 - val_mse: 122.7091 - val_mae: 9.0925\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 20s 149ms/step - loss: 124.4022 - mse: 124.4022 - mae: 9.1359 - val_loss: 122.6776 - val_mse: 122.6776 - val_mae: 9.0942\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.5390 - mse: 124.5390 - mae: 9.1410 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0936\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 37s 269ms/step - loss: 124.5014 - mse: 124.5014 - mae: 9.1393 - val_loss: 122.6930 - val_mse: 122.6930 - val_mae: 9.0930\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.4191 - mse: 124.4191 - mae: 9.1357 - val_loss: 122.6820 - val_mse: 122.6820 - val_mae: 9.0935\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.3661 - mse: 124.3661 - mae: 9.1367 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4533 - mse: 124.4533 - mae: 9.1376 - val_loss: 122.6802 - val_mse: 122.6802 - val_mae: 9.0937\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 124.4397 - mse: 124.4397 - mae: 9.1329 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0951\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 24s 175ms/step - loss: 124.4279 - mse: 124.4279 - mae: 9.1414 - val_loss: 122.6964 - val_mse: 122.6964 - val_mae: 9.0929\n",
      "Epoch 71/300\n",
      "136/136 [==============================] - 25s 185ms/step - loss: 124.4290 - mse: 124.4290 - mae: 9.1360 - val_loss: 122.6819 - val_mse: 122.6819 - val_mae: 9.0953\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4661 - mse: 124.4661 - mae: 9.1397 - val_loss: 122.6858 - val_mse: 122.6858 - val_mae: 9.0933\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 27s 195ms/step - loss: 124.4878 - mse: 124.4878 - mae: 9.1371 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0937\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 124.5033 - mse: 124.5033 - mae: 9.1392 - val_loss: 122.6953 - val_mse: 122.6953 - val_mae: 9.0929\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 23s 166ms/step - loss: 124.5023 - mse: 124.5023 - mae: 9.1398 - val_loss: 122.7261 - val_mse: 122.7261 - val_mae: 9.0921\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 124.3765 - mse: 124.3765 - mae: 9.1320 - val_loss: 122.6815 - val_mse: 122.6815 - val_mae: 9.0936\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 124.4807 - mse: 124.4807 - mae: 9.1376 - val_loss: 122.6776 - val_mse: 122.6776 - val_mae: 9.0941\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4783 - mse: 124.4783 - mae: 9.1393 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0945\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4423 - mse: 124.4423 - mae: 9.1388 - val_loss: 122.6889 - val_mse: 122.6889 - val_mae: 9.0932\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5146 - mse: 124.5146 - mae: 9.1401 - val_loss: 122.6997 - val_mse: 122.6997 - val_mae: 9.0928\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 34s 247ms/step - loss: 124.4899 - mse: 124.4899 - mae: 9.1393 - val_loss: 122.6990 - val_mse: 122.6990 - val_mae: 9.0928\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 44s 322ms/step - loss: 124.4328 - mse: 124.4328 - mae: 9.1369 - val_loss: 122.6980 - val_mse: 122.6980 - val_mae: 9.0928\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.4193 - mse: 124.4193 - mae: 9.1385 - val_loss: 122.6895 - val_mse: 122.6895 - val_mae: 9.0931\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4332 - mse: 124.4332 - mae: 9.1341 - val_loss: 122.6898 - val_mse: 122.6898 - val_mae: 9.0931\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.5661 - mse: 124.5661 - mae: 9.1419 - val_loss: 122.7009 - val_mse: 122.7009 - val_mae: 9.0927\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.5188 - mse: 124.5188 - mae: 9.1391 - val_loss: 122.6849 - val_mse: 122.6849 - val_mae: 9.0933\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 124.3978 - mse: 124.3978 - mae: 9.1340 - val_loss: 122.7023 - val_mse: 122.7023 - val_mae: 9.0927\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.4050 - mse: 124.4050 - mae: 9.1334 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.5487 - mse: 124.5487 - mae: 9.1453 - val_loss: 122.7152 - val_mse: 122.7152 - val_mae: 9.0924\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 27s 195ms/step - loss: 124.5491 - mse: 124.5491 - mae: 9.1423 - val_loss: 122.7448 - val_mse: 122.7448 - val_mae: 9.0917\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4887 - mse: 124.4887 - mae: 9.1377 - val_loss: 122.7093 - val_mse: 122.7093 - val_mae: 9.0925\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 124.5265 - mse: 124.5265 - mae: 9.1404 - val_loss: 122.6839 - val_mse: 122.6839 - val_mae: 9.0934\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 124.5185 - mse: 124.5185 - mae: 9.1381 - val_loss: 122.6802 - val_mse: 122.6802 - val_mae: 9.0937\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.4454 - mse: 124.4454 - mae: 9.1393 - val_loss: 122.6899 - val_mse: 122.6899 - val_mae: 9.0931\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.5348 - mse: 124.5348 - mae: 9.1410 - val_loss: 122.6999 - val_mse: 122.6999 - val_mae: 9.0928\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4924 - mse: 124.4924 - mae: 9.1380 - val_loss: 122.6874 - val_mse: 122.6874 - val_mae: 9.0932\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.5276 - mse: 124.5276 - mae: 9.1375 - val_loss: 122.6885 - val_mse: 122.6885 - val_mae: 9.0932\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 23s 173ms/step - loss: 124.4996 - mse: 124.4996 - mae: 9.1424 - val_loss: 122.6985 - val_mse: 122.6985 - val_mae: 9.0928\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 124.5737 - mse: 124.5737 - mae: 9.1400 - val_loss: 122.6776 - val_mse: 122.6776 - val_mae: 9.0941\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.5363 - mse: 124.5363 - mae: 9.1396 - val_loss: 122.6811 - val_mse: 122.6811 - val_mae: 9.0948\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 124.4121 - mse: 124.4121 - mae: 9.1391 - val_loss: 122.6872 - val_mse: 122.6872 - val_mae: 9.0932\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 124.4306 - mse: 124.4306 - mae: 9.1351 - val_loss: 122.6943 - val_mse: 122.6943 - val_mae: 9.0929\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4809 - mse: 124.4809 - mae: 9.1389 - val_loss: 122.6874 - val_mse: 122.6874 - val_mae: 9.0932\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.5023 - mse: 124.5023 - mae: 9.1389 - val_loss: 122.6873 - val_mse: 122.6873 - val_mae: 9.0932\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4119 - mse: 124.4119 - mae: 9.1339 - val_loss: 122.6808 - val_mse: 122.6808 - val_mae: 9.0936\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 22s 162ms/step - loss: 124.4577 - mse: 124.4577 - mae: 9.1358 - val_loss: 122.6788 - val_mse: 122.6788 - val_mae: 9.0945\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 124.5243 - mse: 124.5243 - mae: 9.1429 - val_loss: 122.6831 - val_mse: 122.6831 - val_mae: 9.0935\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 124.5032 - mse: 124.5032 - mae: 9.1416 - val_loss: 122.6879 - val_mse: 122.6879 - val_mae: 9.0932\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.4601 - mse: 124.4601 - mae: 9.1375 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 124.3864 - mse: 124.3864 - mae: 9.1367 - val_loss: 122.6989 - val_mse: 122.6989 - val_mae: 9.0928\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5160 - mse: 124.5160 - mae: 9.1407 - val_loss: 122.7382 - val_mse: 122.7382 - val_mae: 9.0919\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4134 - mse: 124.4134 - mae: 9.1322 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4836 - mse: 124.4836 - mae: 9.1347 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 124.4528 - mse: 124.4528 - mae: 9.1397 - val_loss: 122.6898 - val_mse: 122.6898 - val_mae: 9.0931\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4790 - mse: 124.4790 - mae: 9.1379 - val_loss: 122.6929 - val_mse: 122.6929 - val_mae: 9.0930\n",
      "Epoch 116/300\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 124.3532 - mse: 124.3532 - mae: 9.1356 - val_loss: 122.6997 - val_mse: 122.6997 - val_mae: 9.0928\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.5083 - mse: 124.5083 - mae: 9.1412 - val_loss: 122.6865 - val_mse: 122.6865 - val_mae: 9.0933\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5231 - mse: 124.5231 - mae: 9.1383 - val_loss: 122.6840 - val_mse: 122.6840 - val_mae: 9.0934\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4659 - mse: 124.4659 - mae: 9.1369 - val_loss: 122.6824 - val_mse: 122.6824 - val_mae: 9.0935\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5440 - mse: 124.5440 - mae: 9.1408 - val_loss: 122.6807 - val_mse: 122.6807 - val_mae: 9.0936\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4744 - mse: 124.4744 - mae: 9.1402 - val_loss: 122.7020 - val_mse: 122.7020 - val_mae: 9.0927\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.5166 - mse: 124.5166 - mae: 9.1408 - val_loss: 122.6920 - val_mse: 122.6920 - val_mae: 9.0930\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.5823 - mse: 124.5823 - mae: 9.1455 - val_loss: 122.7092 - val_mse: 122.7092 - val_mae: 9.0925\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.4797 - mse: 124.4797 - mae: 9.1408 - val_loss: 122.7341 - val_mse: 122.7341 - val_mae: 9.0919\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.5187 - mse: 124.5187 - mae: 9.1414 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5704 - mse: 124.5704 - mae: 9.1455 - val_loss: 122.6993 - val_mse: 122.6993 - val_mae: 9.0928\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.4803 - mse: 124.4803 - mae: 9.1381 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.5514 - mse: 124.5514 - mae: 9.1424 - val_loss: 122.6813 - val_mse: 122.6813 - val_mae: 9.0936\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 22s 158ms/step - loss: 124.4689 - mse: 124.4689 - mae: 9.1388 - val_loss: 122.6971 - val_mse: 122.6971 - val_mae: 9.0928\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4989 - mse: 124.4989 - mae: 9.1372 - val_loss: 122.6903 - val_mse: 122.6903 - val_mae: 9.0931\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.5377 - mse: 124.5377 - mae: 9.1402 - val_loss: 122.6834 - val_mse: 122.6834 - val_mae: 9.0934\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.4409 - mse: 124.4409 - mae: 9.1384 - val_loss: 122.6908 - val_mse: 122.6908 - val_mae: 9.0931\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 124.4510 - mse: 124.4510 - mae: 9.1365 - val_loss: 122.7056 - val_mse: 122.7056 - val_mae: 9.0926\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 124.5391 - mse: 124.5391 - mae: 9.1397 - val_loss: 122.6983 - val_mse: 122.6983 - val_mae: 9.0928\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 46s 338ms/step - loss: 124.5541 - mse: 124.5541 - mae: 9.1415 - val_loss: 122.6793 - val_mse: 122.6793 - val_mae: 9.0938\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.4183 - mse: 124.4183 - mae: 9.1398 - val_loss: 122.7067 - val_mse: 122.7067 - val_mae: 9.0926\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 45s 329ms/step - loss: 124.4671 - mse: 124.4671 - mae: 9.1379 - val_loss: 122.6861 - val_mse: 122.6861 - val_mae: 9.0933\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.4605 - mse: 124.4605 - mae: 9.1368 - val_loss: 122.6825 - val_mse: 122.6825 - val_mae: 9.0935\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4378 - mse: 124.4378 - mae: 9.1397 - val_loss: 122.6863 - val_mse: 122.6863 - val_mae: 9.0933\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 124.4063 - mse: 124.4063 - mae: 9.1332 - val_loss: 122.6790 - val_mse: 122.6790 - val_mae: 9.0945\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 61s 448ms/step - loss: 124.5253 - mse: 124.5253 - mae: 9.1428 - val_loss: 122.6929 - val_mse: 122.6929 - val_mae: 9.0930\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 38s 278ms/step - loss: 124.3649 - mse: 124.3649 - mae: 9.1350 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0939\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 38s 278ms/step - loss: 124.5208 - mse: 124.5208 - mae: 9.1401 - val_loss: 122.6996 - val_mse: 122.6996 - val_mae: 9.0928\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.3776 - mse: 124.3776 - mae: 9.1341 - val_loss: 122.6919 - val_mse: 122.6919 - val_mae: 9.0930\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 124.5088 - mse: 124.5088 - mae: 9.1392 - val_loss: 122.6983 - val_mse: 122.6983 - val_mae: 9.0928\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 124.5632 - mse: 124.5632 - mae: 9.1415 - val_loss: 122.6918 - val_mse: 122.6918 - val_mae: 9.0930\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 29s 217ms/step - loss: 124.4023 - mse: 124.4023 - mae: 9.1340 - val_loss: 122.6892 - val_mse: 122.6892 - val_mae: 9.0931\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 124.4594 - mse: 124.4594 - mae: 9.1393 - val_loss: 122.7055 - val_mse: 122.7055 - val_mae: 9.0926\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 124.3986 - mse: 124.3986 - mae: 9.1327 - val_loss: 122.6833 - val_mse: 122.6833 - val_mae: 9.0934\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 29s 214ms/step - loss: 124.4655 - mse: 124.4655 - mae: 9.1388 - val_loss: 122.7028 - val_mse: 122.7028 - val_mae: 9.0927\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 30s 220ms/step - loss: 124.4376 - mse: 124.4376 - mae: 9.1354 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 152/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 28s 203ms/step - loss: 124.5279 - mse: 124.5279 - mae: 9.1400 - val_loss: 122.7133 - val_mse: 122.7133 - val_mae: 9.0924\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 124.4771 - mse: 124.4771 - mae: 9.1352 - val_loss: 122.6934 - val_mse: 122.6934 - val_mae: 9.0930\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 124.4560 - mse: 124.4560 - mae: 9.1376 - val_loss: 122.7260 - val_mse: 122.7260 - val_mae: 9.0921\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.5323 - mse: 124.5323 - mae: 9.1375 - val_loss: 122.6889 - val_mse: 122.6889 - val_mae: 9.0932\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 124.4433 - mse: 124.4433 - mae: 9.1386 - val_loss: 122.7130 - val_mse: 122.7130 - val_mae: 9.0924\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4689 - mse: 124.4689 - mae: 9.1366 - val_loss: 122.6914 - val_mse: 122.6914 - val_mae: 9.0931\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 27s 195ms/step - loss: 124.3829 - mse: 124.3829 - mae: 9.1350 - val_loss: 122.6805 - val_mse: 122.6805 - val_mae: 9.0936\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 124.4100 - mse: 124.4100 - mae: 9.1349 - val_loss: 122.6805 - val_mse: 122.6805 - val_mae: 9.0936\n",
      "Epoch 160/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4481 - mse: 124.4481 - mae: 9.1377 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0938\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 29s 213ms/step - loss: 124.4252 - mse: 124.4252 - mae: 9.1388 - val_loss: 122.6979 - val_mse: 122.6979 - val_mae: 9.0928\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.5508 - mse: 124.5508 - mae: 9.1440 - val_loss: 122.6838 - val_mse: 122.6838 - val_mae: 9.0934\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 29s 212ms/step - loss: 124.4700 - mse: 124.4700 - mae: 9.1399 - val_loss: 122.6811 - val_mse: 122.6811 - val_mae: 9.0936\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 124.5700 - mse: 124.5700 - mae: 9.1403 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0939\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 124.5092 - mse: 124.5092 - mae: 9.1399 - val_loss: 122.6935 - val_mse: 122.6935 - val_mae: 9.0930\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 33s 244ms/step - loss: 124.4293 - mse: 124.4293 - mae: 9.1377 - val_loss: 122.6905 - val_mse: 122.6905 - val_mae: 9.0931\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 31s 227ms/step - loss: 124.5308 - mse: 124.5308 - mae: 9.1399 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0941\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.4290 - mse: 124.4290 - mae: 9.1360 - val_loss: 122.6921 - val_mse: 122.6921 - val_mae: 9.0930\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 124.5228 - mse: 124.5228 - mae: 9.1415 - val_loss: 122.7039 - val_mse: 122.7039 - val_mae: 9.0926\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 124.4986 - mse: 124.4986 - mae: 9.1381 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0939\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.5579 - mse: 124.5579 - mae: 9.1427 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.3479 - mse: 124.3479 - mae: 9.1338 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4327 - mse: 124.4327 - mae: 9.1362 - val_loss: 122.6952 - val_mse: 122.6952 - val_mae: 9.0929\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.5095 - mse: 124.5095 - mae: 9.1395 - val_loss: 122.6946 - val_mse: 122.6946 - val_mae: 9.0929\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4496 - mse: 124.4496 - mae: 9.1362 - val_loss: 122.7001 - val_mse: 122.7001 - val_mae: 9.0928\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 30s 222ms/step - loss: 124.5054 - mse: 124.5054 - mae: 9.1375 - val_loss: 122.6862 - val_mse: 122.6862 - val_mae: 9.0933\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 30s 220ms/step - loss: 124.3969 - mse: 124.3969 - mae: 9.1382 - val_loss: 122.7150 - val_mse: 122.7150 - val_mae: 9.0924\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 37s 268ms/step - loss: 124.4325 - mse: 124.4325 - mae: 9.1370 - val_loss: 122.7192 - val_mse: 122.7192 - val_mae: 9.0923\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 28s 210ms/step - loss: 124.5416 - mse: 124.5416 - mae: 9.1396 - val_loss: 122.6919 - val_mse: 122.6919 - val_mae: 9.0930\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 124.4347 - mse: 124.4347 - mae: 9.1351 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 124.4065 - mse: 124.4065 - mae: 9.1347 - val_loss: 122.6801 - val_mse: 122.6801 - val_mae: 9.0946\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 33s 239ms/step - loss: 124.4185 - mse: 124.4185 - mae: 9.1432 - val_loss: 122.6956 - val_mse: 122.6956 - val_mae: 9.0929\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 29s 213ms/step - loss: 124.4403 - mse: 124.4403 - mae: 9.1354 - val_loss: 122.6818 - val_mse: 122.6818 - val_mae: 9.0935\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 31s 232ms/step - loss: 124.4437 - mse: 124.4437 - mae: 9.1362 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 31s 228ms/step - loss: 124.5060 - mse: 124.5060 - mae: 9.1416 - val_loss: 122.7078 - val_mse: 122.7078 - val_mae: 9.0925\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 32s 238ms/step - loss: 124.5090 - mse: 124.5090 - mae: 9.1396 - val_loss: 122.7073 - val_mse: 122.7073 - val_mae: 9.0926\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 124.4841 - mse: 124.4841 - mae: 9.1379 - val_loss: 122.6909 - val_mse: 122.6909 - val_mae: 9.0931\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.4500 - mse: 124.4500 - mae: 9.1407 - val_loss: 122.7382 - val_mse: 122.7382 - val_mae: 9.0919\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 30s 220ms/step - loss: 124.4244 - mse: 124.4244 - mae: 9.1344 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0943\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 43s 318ms/step - loss: 124.3858 - mse: 124.3858 - mae: 9.1359 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0940\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 42s 308ms/step - loss: 124.4490 - mse: 124.4490 - mae: 9.1391 - val_loss: 122.6865 - val_mse: 122.6865 - val_mae: 9.0933\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 40s 292ms/step - loss: 124.3881 - mse: 124.3881 - mae: 9.1342 - val_loss: 122.6833 - val_mse: 122.6833 - val_mae: 9.0934\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 30s 222ms/step - loss: 124.4575 - mse: 124.4575 - mae: 9.1392 - val_loss: 122.6834 - val_mse: 122.6834 - val_mae: 9.0934\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.4847 - mse: 124.4847 - mae: 9.1374 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0944\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 124.3954 - mse: 124.3954 - mae: 9.1340 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0943\n",
      "Epoch 196/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 124.4226 - mse: 124.4226 - mae: 9.1385 - val_loss: 122.6909 - val_mse: 122.6909 - val_mae: 9.0931\n",
      "Epoch 197/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.5119 - mse: 124.5119 - mae: 9.1397 - val_loss: 122.6965 - val_mse: 122.6965 - val_mae: 9.0929\n",
      "Epoch 198/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 26s 192ms/step - loss: 124.4236 - mse: 124.4236 - mae: 9.1389 - val_loss: 122.7069 - val_mse: 122.7069 - val_mae: 9.0926\n",
      "Epoch 199/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4991 - mse: 124.4991 - mae: 9.1399 - val_loss: 122.7110 - val_mse: 122.7110 - val_mae: 9.0925\n",
      "Epoch 200/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 124.4450 - mse: 124.4450 - mae: 9.1355 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 201/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4219 - mse: 124.4219 - mae: 9.1374 - val_loss: 122.6900 - val_mse: 122.6900 - val_mae: 9.0931\n",
      "Epoch 202/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4858 - mse: 124.4858 - mae: 9.1380 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 203/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.5037 - mse: 124.5037 - mae: 9.1361 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 204/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 124.4856 - mse: 124.4856 - mae: 9.1396 - val_loss: 122.6937 - val_mse: 122.6937 - val_mae: 9.0930\n",
      "Epoch 205/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 124.4600 - mse: 124.4600 - mae: 9.1394 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 206/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 124.4127 - mse: 124.4127 - mae: 9.1369 - val_loss: 122.6792 - val_mse: 122.6792 - val_mae: 9.0938\n",
      "Epoch 207/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4328 - mse: 124.4328 - mae: 9.1376 - val_loss: 122.6781 - val_mse: 122.6781 - val_mae: 9.0939\n",
      "Epoch 208/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 124.5196 - mse: 124.5196 - mae: 9.1395 - val_loss: 122.6795 - val_mse: 122.6795 - val_mae: 9.0937\n",
      "Epoch 209/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 124.4222 - mse: 124.4222 - mae: 9.1392 - val_loss: 122.6894 - val_mse: 122.6894 - val_mae: 9.0931\n",
      "Epoch 210/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4324 - mse: 124.4324 - mae: 9.1367 - val_loss: 122.6842 - val_mse: 122.6842 - val_mae: 9.0934\n",
      "Epoch 211/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4599 - mse: 124.4599 - mae: 9.1398 - val_loss: 122.6833 - val_mse: 122.6833 - val_mae: 9.0934\n",
      "Epoch 212/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4689 - mse: 124.4689 - mae: 9.1412 - val_loss: 122.7420 - val_mse: 122.7420 - val_mae: 9.0918\n",
      "Epoch 213/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.4999 - mse: 124.4999 - mae: 9.1385 - val_loss: 122.6906 - val_mse: 122.6906 - val_mae: 9.0931\n",
      "Epoch 214/300\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 124.4072 - mse: 124.4072 - mae: 9.1360 - val_loss: 122.6901 - val_mse: 122.6901 - val_mae: 9.0931\n",
      "Epoch 215/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4276 - mse: 124.4276 - mae: 9.1397 - val_loss: 122.7142 - val_mse: 122.7142 - val_mae: 9.0924\n",
      "Epoch 216/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.4258 - mse: 124.4258 - mae: 9.1350 - val_loss: 122.6903 - val_mse: 122.6903 - val_mae: 9.0931\n",
      "Epoch 217/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4164 - mse: 124.4164 - mae: 9.1361 - val_loss: 122.6809 - val_mse: 122.6809 - val_mae: 9.0936\n",
      "Epoch 218/300\n",
      "136/136 [==============================] - 22s 162ms/step - loss: 124.4810 - mse: 124.4810 - mae: 9.1403 - val_loss: 122.6800 - val_mse: 122.6800 - val_mae: 9.0937\n",
      "Epoch 219/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4887 - mse: 124.4887 - mae: 9.1402 - val_loss: 122.6794 - val_mse: 122.6794 - val_mae: 9.0937\n",
      "Epoch 220/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4566 - mse: 124.4566 - mae: 9.1396 - val_loss: 122.6940 - val_mse: 122.6940 - val_mae: 9.0930\n",
      "Epoch 221/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4617 - mse: 124.4617 - mae: 9.1364 - val_loss: 122.6786 - val_mse: 122.6786 - val_mae: 9.0944\n",
      "Epoch 222/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4926 - mse: 124.4926 - mae: 9.1437 - val_loss: 122.6929 - val_mse: 122.6929 - val_mae: 9.0930\n",
      "Epoch 223/300\n",
      "136/136 [==============================] - 22s 159ms/step - loss: 124.5103 - mse: 124.5103 - mae: 9.1384 - val_loss: 122.6780 - val_mse: 122.6780 - val_mae: 9.0940\n",
      "Epoch 224/300\n",
      "136/136 [==============================] - 21s 158ms/step - loss: 124.5261 - mse: 124.5261 - mae: 9.1428 - val_loss: 122.7053 - val_mse: 122.7053 - val_mae: 9.0926\n",
      "Epoch 225/300\n",
      "136/136 [==============================] - 22s 160ms/step - loss: 124.4359 - mse: 124.4359 - mae: 9.1360 - val_loss: 122.6837 - val_mse: 122.6837 - val_mae: 9.0934\n",
      "Epoch 226/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4900 - mse: 124.4900 - mae: 9.1418 - val_loss: 122.7272 - val_mse: 122.7272 - val_mae: 9.0921\n",
      "Epoch 227/300\n",
      "136/136 [==============================] - 31s 228ms/step - loss: 124.5000 - mse: 124.5000 - mae: 9.1376 - val_loss: 122.6872 - val_mse: 122.6872 - val_mae: 9.0932\n",
      "Epoch 228/300\n",
      "136/136 [==============================] - 31s 230ms/step - loss: 124.4920 - mse: 124.4920 - mae: 9.1386 - val_loss: 122.6776 - val_mse: 122.6776 - val_mae: 9.0941\n",
      "Epoch 229/300\n",
      "136/136 [==============================] - 30s 217ms/step - loss: 124.4601 - mse: 124.4601 - mae: 9.1406 - val_loss: 122.6939 - val_mse: 122.6939 - val_mae: 9.0930\n",
      "Epoch 230/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.5269 - mse: 124.5269 - mae: 9.1394 - val_loss: 122.6899 - val_mse: 122.6899 - val_mae: 9.0931\n",
      "Epoch 231/300\n",
      "136/136 [==============================] - 36s 266ms/step - loss: 124.4344 - mse: 124.4344 - mae: 9.1354 - val_loss: 122.7009 - val_mse: 122.7009 - val_mae: 9.0927\n",
      "Epoch 232/300\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 124.4675 - mse: 124.4675 - mae: 9.1375 - val_loss: 122.6816 - val_mse: 122.6816 - val_mae: 9.0936\n",
      "Epoch 233/300\n",
      "136/136 [==============================] - 30s 223ms/step - loss: 124.4969 - mse: 124.4969 - mae: 9.1393 - val_loss: 122.6807 - val_mse: 122.6807 - val_mae: 9.0936\n",
      "Epoch 234/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 124.5182 - mse: 124.5182 - mae: 9.1401 - val_loss: 122.6829 - val_mse: 122.6829 - val_mae: 9.0935\n",
      "Epoch 235/300\n",
      "136/136 [==============================] - 22s 161ms/step - loss: 124.4666 - mse: 124.4666 - mae: 9.1417 - val_loss: 122.7048 - val_mse: 122.7048 - val_mae: 9.0926\n",
      "Epoch 236/300\n",
      "136/136 [==============================] - 22s 164ms/step - loss: 124.4977 - mse: 124.4977 - mae: 9.1398 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 237/300\n",
      "136/136 [==============================] - 22s 165ms/step - loss: 124.3959 - mse: 124.3959 - mae: 9.1354 - val_loss: 122.6856 - val_mse: 122.6856 - val_mae: 9.0933\n",
      "Epoch 238/300\n",
      "136/136 [==============================] - 37s 274ms/step - loss: 124.3588 - mse: 124.3588 - mae: 9.1362 - val_loss: 122.6888 - val_mse: 122.6888 - val_mae: 9.0932\n",
      "Epoch 239/300\n",
      "136/136 [==============================] - 47s 343ms/step - loss: 124.5361 - mse: 124.5361 - mae: 9.1397 - val_loss: 122.6784 - val_mse: 122.6784 - val_mae: 9.0939\n",
      "Epoch 240/300\n",
      "136/136 [==============================] - 43s 318ms/step - loss: 124.4605 - mse: 124.4605 - mae: 9.1406 - val_loss: 122.7081 - val_mse: 122.7081 - val_mae: 9.0925\n",
      "Epoch 241/300\n",
      "136/136 [==============================] - 37s 275ms/step - loss: 124.4711 - mse: 124.4711 - mae: 9.1381 - val_loss: 122.6850 - val_mse: 122.6850 - val_mae: 9.0933\n",
      "Epoch 242/300\n",
      "136/136 [==============================] - 37s 273ms/step - loss: 124.4617 - mse: 124.4617 - mae: 9.1376 - val_loss: 122.6804 - val_mse: 122.6804 - val_mae: 9.0936\n",
      "Epoch 243/300\n",
      "136/136 [==============================] - 44s 323ms/step - loss: 124.5058 - mse: 124.5058 - mae: 9.1401 - val_loss: 122.6978 - val_mse: 122.6978 - val_mae: 9.0928\n",
      "Epoch 244/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 36s 264ms/step - loss: 124.5183 - mse: 124.5183 - mae: 9.1407 - val_loss: 122.6959 - val_mse: 122.6959 - val_mae: 9.0929\n",
      "Epoch 245/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4042 - mse: 124.4042 - mae: 9.1365 - val_loss: 122.6871 - val_mse: 122.6871 - val_mae: 9.0932\n",
      "Epoch 246/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 124.4864 - mse: 124.4864 - mae: 9.1393 - val_loss: 122.7211 - val_mse: 122.7211 - val_mae: 9.0922\n",
      "Epoch 247/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 124.5296 - mse: 124.5296 - mae: 9.1396 - val_loss: 122.7030 - val_mse: 122.7030 - val_mae: 9.0927\n",
      "Epoch 248/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.4852 - mse: 124.4852 - mae: 9.1428 - val_loss: 122.6953 - val_mse: 122.6953 - val_mae: 9.0929\n",
      "Epoch 249/300\n",
      "136/136 [==============================] - 24s 179ms/step - loss: 124.3895 - mse: 124.3895 - mae: 9.1351 - val_loss: 122.6789 - val_mse: 122.6789 - val_mae: 9.0938\n",
      "Epoch 250/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 124.4508 - mse: 124.4508 - mae: 9.1373 - val_loss: 122.6861 - val_mse: 122.6861 - val_mae: 9.0933\n",
      "Epoch 251/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 124.3828 - mse: 124.3828 - mae: 9.1345 - val_loss: 122.6822 - val_mse: 122.6822 - val_mae: 9.0955\n",
      "Epoch 252/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 124.5626 - mse: 124.5626 - mae: 9.1437 - val_loss: 122.6861 - val_mse: 122.6861 - val_mae: 9.0933\n",
      "Epoch 253/300\n",
      "136/136 [==============================] - 56s 412ms/step - loss: 124.5075 - mse: 124.5075 - mae: 9.1407 - val_loss: 122.7162 - val_mse: 122.7162 - val_mae: 9.0923\n",
      "Epoch 254/300\n",
      "136/136 [==============================] - 52s 387ms/step - loss: 124.5119 - mse: 124.5119 - mae: 9.1454 - val_loss: 122.7153 - val_mse: 122.7153 - val_mae: 9.0923\n",
      "Epoch 255/300\n",
      "136/136 [==============================] - 41s 303ms/step - loss: 124.5631 - mse: 124.5631 - mae: 9.1392 - val_loss: 122.6886 - val_mse: 122.6886 - val_mae: 9.0932\n",
      "Epoch 256/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.5972 - mse: 124.5972 - mae: 9.1426 - val_loss: 122.6868 - val_mse: 122.6868 - val_mae: 9.0933\n",
      "Epoch 257/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 124.4790 - mse: 124.4790 - mae: 9.1351 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0945\n",
      "Epoch 258/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 124.4860 - mse: 124.4860 - mae: 9.1383 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 259/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.4795 - mse: 124.4795 - mae: 9.1386 - val_loss: 122.6805 - val_mse: 122.6805 - val_mae: 9.0936\n",
      "Epoch 260/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4356 - mse: 124.4356 - mae: 9.1369 - val_loss: 122.7015 - val_mse: 122.7015 - val_mae: 9.0927\n",
      "Epoch 261/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 124.5669 - mse: 124.5669 - mae: 9.1403 - val_loss: 122.6791 - val_mse: 122.6791 - val_mae: 9.0938\n",
      "Epoch 262/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.5285 - mse: 124.5285 - mae: 9.1395 - val_loss: 122.6829 - val_mse: 122.6829 - val_mae: 9.0935\n",
      "Epoch 263/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 124.5075 - mse: 124.5075 - mae: 9.1370 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 264/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 124.4799 - mse: 124.4799 - mae: 9.1431 - val_loss: 122.7173 - val_mse: 122.7173 - val_mae: 9.0923\n",
      "Epoch 265/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 124.4334 - mse: 124.4334 - mae: 9.1347 - val_loss: 122.6777 - val_mse: 122.6777 - val_mae: 9.0942\n",
      "Epoch 266/300\n",
      "136/136 [==============================] - 29s 214ms/step - loss: 124.4296 - mse: 124.4296 - mae: 9.1404 - val_loss: 122.6964 - val_mse: 122.6964 - val_mae: 9.0929\n",
      "Epoch 267/300\n",
      "136/136 [==============================] - 50s 371ms/step - loss: 124.3991 - mse: 124.3991 - mae: 9.1327 - val_loss: 122.6856 - val_mse: 122.6856 - val_mae: 9.0933\n",
      "Epoch 268/300\n",
      "136/136 [==============================] - 33s 242ms/step - loss: 124.4693 - mse: 124.4693 - mae: 9.1393 - val_loss: 122.6906 - val_mse: 122.6906 - val_mae: 9.0931\n",
      "Epoch 269/300\n",
      "136/136 [==============================] - 31s 229ms/step - loss: 124.5091 - mse: 124.5091 - mae: 9.1413 - val_loss: 122.6890 - val_mse: 122.6890 - val_mae: 9.0932\n",
      "Epoch 270/300\n",
      "136/136 [==============================] - 23s 172ms/step - loss: 124.5628 - mse: 124.5628 - mae: 9.1453 - val_loss: 122.7282 - val_mse: 122.7282 - val_mae: 9.0921\n",
      "Epoch 271/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 124.4251 - mse: 124.4251 - mae: 9.1340 - val_loss: 122.6787 - val_mse: 122.6787 - val_mae: 9.0938\n",
      "Epoch 272/300\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 124.4931 - mse: 124.4931 - mae: 9.1381 - val_loss: 122.6851 - val_mse: 122.6851 - val_mae: 9.0933\n",
      "Epoch 273/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4351 - mse: 124.4351 - mae: 9.1350 - val_loss: 122.6796 - val_mse: 122.6796 - val_mae: 9.0937\n",
      "Epoch 274/300\n",
      "136/136 [==============================] - 25s 185ms/step - loss: 124.5148 - mse: 124.5148 - mae: 9.1416 - val_loss: 122.6798 - val_mse: 122.6798 - val_mae: 9.0937\n",
      "Epoch 275/300\n",
      "136/136 [==============================] - 24s 174ms/step - loss: 124.5313 - mse: 124.5313 - mae: 9.1420 - val_loss: 122.6783 - val_mse: 122.6783 - val_mae: 9.0944\n",
      "Epoch 276/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 124.4712 - mse: 124.4712 - mae: 9.1419 - val_loss: 122.7029 - val_mse: 122.7029 - val_mae: 9.0927\n",
      "Epoch 277/300\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 124.4159 - mse: 124.4159 - mae: 9.1373 - val_loss: 122.7038 - val_mse: 122.7038 - val_mae: 9.0926\n",
      "Epoch 278/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 124.5652 - mse: 124.5652 - mae: 9.1399 - val_loss: 122.6920 - val_mse: 122.6920 - val_mae: 9.0930\n",
      "Epoch 279/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 124.4520 - mse: 124.4520 - mae: 9.1375 - val_loss: 122.7063 - val_mse: 122.7063 - val_mae: 9.0926\n",
      "Epoch 280/300\n",
      "136/136 [==============================] - 30s 217ms/step - loss: 124.4750 - mse: 124.4750 - mae: 9.1386 - val_loss: 122.6956 - val_mse: 122.6956 - val_mae: 9.0929\n",
      "Epoch 281/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 124.5261 - mse: 124.5261 - mae: 9.1402 - val_loss: 122.6901 - val_mse: 122.6901 - val_mae: 9.0931\n",
      "Epoch 282/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 124.4660 - mse: 124.4660 - mae: 9.1377 - val_loss: 122.6930 - val_mse: 122.6930 - val_mae: 9.0930\n",
      "Epoch 283/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 124.4300 - mse: 124.4300 - mae: 9.1381 - val_loss: 122.6896 - val_mse: 122.6896 - val_mae: 9.0931\n",
      "Epoch 284/300\n",
      "136/136 [==============================] - 31s 230ms/step - loss: 124.4712 - mse: 124.4712 - mae: 9.1371 - val_loss: 122.7003 - val_mse: 122.7003 - val_mae: 9.0927\n",
      "Epoch 285/300\n",
      "136/136 [==============================] - 42s 312ms/step - loss: 124.4368 - mse: 124.4368 - mae: 9.1391 - val_loss: 122.7338 - val_mse: 122.7338 - val_mae: 9.0920\n",
      "Epoch 286/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4224 - mse: 124.4224 - mae: 9.1342 - val_loss: 122.6827 - val_mse: 122.6827 - val_mae: 9.0935\n",
      "Epoch 287/300\n",
      "136/136 [==============================] - 24s 173ms/step - loss: 124.4849 - mse: 124.4849 - mae: 9.1380 - val_loss: 122.6782 - val_mse: 122.6782 - val_mae: 9.0939\n",
      "Epoch 288/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 124.4387 - mse: 124.4387 - mae: 9.1377 - val_loss: 122.6838 - val_mse: 122.6838 - val_mae: 9.0934\n",
      "Epoch 289/300\n",
      "136/136 [==============================] - 23s 173ms/step - loss: 124.4796 - mse: 124.4796 - mae: 9.1411 - val_loss: 122.6807 - val_mse: 122.6807 - val_mae: 9.0936\n",
      "Epoch 290/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 24s 175ms/step - loss: 124.4138 - mse: 124.4138 - mae: 9.1399 - val_loss: 122.7106 - val_mse: 122.7106 - val_mae: 9.0925\n",
      "Epoch 291/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4896 - mse: 124.4896 - mae: 9.1385 - val_loss: 122.6928 - val_mse: 122.6928 - val_mae: 9.0930\n",
      "Epoch 292/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 124.4189 - mse: 124.4189 - mae: 9.1370 - val_loss: 122.6828 - val_mse: 122.6828 - val_mae: 9.0935\n",
      "Epoch 293/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 124.3945 - mse: 124.3945 - mae: 9.1344 - val_loss: 122.6981 - val_mse: 122.6981 - val_mae: 9.0928\n",
      "Epoch 294/300\n",
      "136/136 [==============================] - 73s 539ms/step - loss: 124.5032 - mse: 124.5032 - mae: 9.1381 - val_loss: 122.6863 - val_mse: 122.6863 - val_mae: 9.0933\n",
      "Epoch 295/300\n",
      "136/136 [==============================] - 31s 228ms/step - loss: 124.3808 - mse: 124.3808 - mae: 9.1364 - val_loss: 122.7088 - val_mse: 122.7088 - val_mae: 9.0925\n",
      "Epoch 296/300\n",
      "136/136 [==============================] - 35s 254ms/step - loss: 124.4987 - mse: 124.4987 - mae: 9.1393 - val_loss: 122.6981 - val_mse: 122.6981 - val_mae: 9.0928\n",
      "Epoch 297/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 124.4008 - mse: 124.4008 - mae: 9.1312 - val_loss: 122.6811 - val_mse: 122.6811 - val_mae: 9.0948\n",
      "Epoch 298/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 124.4857 - mse: 124.4857 - mae: 9.1398 - val_loss: 122.6818 - val_mse: 122.6818 - val_mae: 9.0935\n",
      "Epoch 299/300\n",
      "136/136 [==============================] - 24s 175ms/step - loss: 124.5187 - mse: 124.5187 - mae: 9.1412 - val_loss: 122.6778 - val_mse: 122.6778 - val_mae: 9.0940\n",
      "Epoch 300/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 124.4770 - mse: 124.4770 - mae: 9.1397 - val_loss: 122.6851 - val_mse: 122.6851 - val_mae: 9.0933\n",
      "57/57 [==============================] - 21s 37ms/step - loss: 133.8655 - mse: 133.8655 - mae: 9.4401\n",
      "57/57 [==============================] - 91s 57ms/step\n",
      "MAE: 9.440106706992736\n",
      "MSE: 133.86552560442638\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 60, 60)            14880     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 60, 60)            0         \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 60, 240)          173760    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 60, 240)           0         \n",
      "                                                                 \n",
      " bidirectional_15 (Bidirecti  (None, 120)              144480    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,241\n",
      "Trainable params: 333,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "136/136 [==============================] - 51s 217ms/step - loss: 6062.6748 - mse: 6062.6748 - mae: 77.0601 - val_loss: 6053.6772 - val_mse: 6053.6772 - val_mae: 77.0130\n",
      "Epoch 2/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 6057.2222 - mse: 6057.2222 - mae: 77.0248 - val_loss: 6048.1958 - val_mse: 6048.1958 - val_mae: 76.9774\n",
      "Epoch 3/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 6051.1182 - mse: 6051.1182 - mae: 76.9851 - val_loss: 6041.2954 - val_mse: 6041.2954 - val_mae: 76.9325\n",
      "Epoch 4/300\n",
      "136/136 [==============================] - 50s 370ms/step - loss: 6043.0674 - mse: 6043.0674 - mae: 76.9329 - val_loss: 6031.8999 - val_mse: 6031.8999 - val_mae: 76.8714\n",
      "Epoch 5/300\n",
      "136/136 [==============================] - 40s 293ms/step - loss: 6031.8340 - mse: 6031.8340 - mae: 76.8597 - val_loss: 6018.4731 - val_mse: 6018.4731 - val_mae: 76.7841\n",
      "Epoch 6/300\n",
      "136/136 [==============================] - 62s 457ms/step - loss: 6015.3062 - mse: 6015.3062 - mae: 76.7520 - val_loss: 5998.1826 - val_mse: 5998.1826 - val_mae: 76.6518\n",
      "Epoch 7/300\n",
      "136/136 [==============================] - 58s 429ms/step - loss: 5989.4565 - mse: 5989.4565 - mae: 76.5836 - val_loss: 5965.3062 - val_mse: 5965.3062 - val_mae: 76.4370\n",
      "Epoch 8/300\n",
      "136/136 [==============================] - 42s 310ms/step - loss: 5945.7061 - mse: 5945.7061 - mae: 76.2973 - val_loss: 5907.6875 - val_mse: 5907.6875 - val_mae: 76.0592\n",
      "Epoch 9/300\n",
      "136/136 [==============================] - 36s 262ms/step - loss: 5866.9233 - mse: 5866.9233 - mae: 75.7786 - val_loss: 5802.7144 - val_mse: 5802.7144 - val_mae: 75.3660\n",
      "Epoch 10/300\n",
      "136/136 [==============================] - 52s 381ms/step - loss: 5729.0498 - mse: 5729.0498 - mae: 74.8641 - val_loss: 5629.6577 - val_mse: 5629.6577 - val_mae: 74.2089\n",
      "Epoch 11/300\n",
      "136/136 [==============================] - 40s 293ms/step - loss: 5535.0317 - mse: 5535.0317 - mae: 73.5549 - val_loss: 5426.0845 - val_mse: 5426.0845 - val_mae: 72.8244\n",
      "Epoch 12/300\n",
      "136/136 [==============================] - 29s 212ms/step - loss: 5352.9609 - mse: 5352.9609 - mae: 72.3084 - val_loss: 5271.3462 - val_mse: 5271.3462 - val_mae: 71.7542\n",
      "Epoch 13/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 5229.0132 - mse: 5229.0132 - mae: 71.4474 - val_loss: 5172.0088 - val_mse: 5172.0088 - val_mae: 71.0586\n",
      "Epoch 14/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 5147.0806 - mse: 5147.0806 - mae: 70.8711 - val_loss: 5103.1689 - val_mse: 5103.1689 - val_mae: 70.5726\n",
      "Epoch 15/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 5087.6812 - mse: 5087.6812 - mae: 70.4502 - val_loss: 5050.7026 - val_mse: 5050.7026 - val_mae: 70.1999\n",
      "Epoch 16/300\n",
      "136/136 [==============================] - 31s 227ms/step - loss: 5040.3115 - mse: 5040.3115 - mae: 70.1139 - val_loss: 5007.8364 - val_mse: 5007.8364 - val_mae: 69.8939\n",
      "Epoch 17/300\n",
      "136/136 [==============================] - 27s 195ms/step - loss: 5000.7681 - mse: 5000.7681 - mae: 69.8311 - val_loss: 4971.2734 - val_mse: 4971.2734 - val_mae: 69.6318\n",
      "Epoch 18/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 4966.9170 - mse: 4966.9170 - mae: 69.5883 - val_loss: 4939.4570 - val_mse: 4939.4570 - val_mae: 69.4030\n",
      "Epoch 19/300\n",
      "136/136 [==============================] - 36s 268ms/step - loss: 4936.9946 - mse: 4936.9946 - mae: 69.3726 - val_loss: 4911.4038 - val_mse: 4911.4038 - val_mae: 69.2006\n",
      "Epoch 20/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 4910.3672 - mse: 4910.3672 - mae: 69.1807 - val_loss: 4886.3647 - val_mse: 4886.3647 - val_mae: 69.0195\n",
      "Epoch 21/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 4886.7085 - mse: 4886.7085 - mae: 69.0094 - val_loss: 4863.7236 - val_mse: 4863.7236 - val_mae: 68.8552\n",
      "Epoch 22/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 4864.6245 - mse: 4864.6245 - mae: 68.8494 - val_loss: 4842.9824 - val_mse: 4842.9824 - val_mae: 68.7045\n",
      "Epoch 23/300\n",
      "136/136 [==============================] - 24s 178ms/step - loss: 4844.5854 - mse: 4844.5854 - mae: 68.7033 - val_loss: 4823.8247 - val_mse: 4823.8247 - val_mae: 68.5649\n",
      "Epoch 24/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 4826.1387 - mse: 4826.1387 - mae: 68.5691 - val_loss: 4805.9995 - val_mse: 4805.9995 - val_mae: 68.4348\n",
      "Epoch 25/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 4809.1216 - mse: 4809.1216 - mae: 68.4449 - val_loss: 4789.3237 - val_mse: 4789.3237 - val_mae: 68.3128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "136/136 [==============================] - 35s 260ms/step - loss: 4792.7803 - mse: 4792.7803 - mae: 68.3253 - val_loss: 4773.6416 - val_mse: 4773.6416 - val_mae: 68.1980\n",
      "Epoch 27/300\n",
      "136/136 [==============================] - 27s 201ms/step - loss: 4777.5459 - mse: 4777.5459 - mae: 68.2139 - val_loss: 4758.8452 - val_mse: 4758.8452 - val_mae: 68.0894\n",
      "Epoch 28/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 4762.9097 - mse: 4762.9097 - mae: 68.1064 - val_loss: 4744.8311 - val_mse: 4744.8311 - val_mae: 67.9864\n",
      "Epoch 29/300\n",
      "136/136 [==============================] - 25s 182ms/step - loss: 4749.2505 - mse: 4749.2505 - mae: 68.0062 - val_loss: 4731.5244 - val_mse: 4731.5244 - val_mae: 67.8885\n",
      "Epoch 30/300\n",
      "136/136 [==============================] - 24s 175ms/step - loss: 4736.2954 - mse: 4736.2954 - mae: 67.9104 - val_loss: 4718.8525 - val_mse: 4718.8525 - val_mae: 67.7951\n",
      "Epoch 31/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 4723.8521 - mse: 4723.8521 - mae: 67.8193 - val_loss: 4706.7520 - val_mse: 4706.7520 - val_mae: 67.7058\n",
      "Epoch 32/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 4711.9424 - mse: 4711.9424 - mae: 67.7314 - val_loss: 4695.1699 - val_mse: 4695.1699 - val_mae: 67.6202\n",
      "Epoch 33/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 4700.6206 - mse: 4700.6206 - mae: 67.6474 - val_loss: 4684.0610 - val_mse: 4684.0610 - val_mae: 67.5380\n",
      "Epoch 34/300\n",
      "136/136 [==============================] - 35s 257ms/step - loss: 4689.6084 - mse: 4689.6084 - mae: 67.5661 - val_loss: 4673.3862 - val_mse: 4673.3862 - val_mae: 67.4589\n",
      "Epoch 35/300\n",
      "136/136 [==============================] - 26s 193ms/step - loss: 4679.4937 - mse: 4679.4937 - mae: 67.4912 - val_loss: 4663.1084 - val_mse: 4663.1084 - val_mae: 67.3827\n",
      "Epoch 36/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 4669.3530 - mse: 4669.3530 - mae: 67.4162 - val_loss: 4653.1958 - val_mse: 4653.1958 - val_mae: 67.3091\n",
      "Epoch 37/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 4659.4204 - mse: 4659.4204 - mae: 67.3426 - val_loss: 4643.6172 - val_mse: 4643.6172 - val_mae: 67.2379\n",
      "Epoch 38/300\n",
      "136/136 [==============================] - 24s 176ms/step - loss: 4649.8589 - mse: 4649.8589 - mae: 67.2717 - val_loss: 4634.3555 - val_mse: 4634.3555 - val_mae: 67.1690\n",
      "Epoch 39/300\n",
      "136/136 [==============================] - 24s 180ms/step - loss: 4640.7114 - mse: 4640.7114 - mae: 67.2032 - val_loss: 4625.3804 - val_mse: 4625.3804 - val_mae: 67.1022\n",
      "Epoch 40/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 4631.9253 - mse: 4631.9253 - mae: 67.1379 - val_loss: 4616.6753 - val_mse: 4616.6753 - val_mae: 67.0373\n",
      "Epoch 41/300\n",
      "136/136 [==============================] - 29s 215ms/step - loss: 4623.2905 - mse: 4623.2905 - mae: 67.0734 - val_loss: 4608.2197 - val_mse: 4608.2197 - val_mae: 66.9742\n",
      "Epoch 42/300\n",
      "136/136 [==============================] - 37s 276ms/step - loss: 4614.9507 - mse: 4614.9507 - mae: 67.0112 - val_loss: 4599.9990 - val_mse: 4599.9990 - val_mae: 66.9128\n",
      "Epoch 43/300\n",
      "136/136 [==============================] - 49s 362ms/step - loss: 4606.9736 - mse: 4606.9736 - mae: 66.9516 - val_loss: 4591.9941 - val_mse: 4591.9941 - val_mae: 66.8529\n",
      "Epoch 44/300\n",
      "136/136 [==============================] - 31s 226ms/step - loss: 4598.9648 - mse: 4598.9648 - mae: 66.8918 - val_loss: 4584.1895 - val_mse: 4584.1895 - val_mae: 66.7945\n",
      "Epoch 45/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 4591.1538 - mse: 4591.1538 - mae: 66.8335 - val_loss: 4576.5771 - val_mse: 4576.5771 - val_mae: 66.7375\n",
      "Epoch 46/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 4583.6431 - mse: 4583.6431 - mae: 66.7776 - val_loss: 4569.1450 - val_mse: 4569.1450 - val_mae: 66.6818\n",
      "Epoch 47/300\n",
      "136/136 [==============================] - 44s 324ms/step - loss: 4576.6108 - mse: 4576.6108 - mae: 66.7247 - val_loss: 4561.8818 - val_mse: 4561.8818 - val_mae: 66.6273\n",
      "Epoch 48/300\n",
      "136/136 [==============================] - 39s 287ms/step - loss: 4569.0791 - mse: 4569.0791 - mae: 66.6684 - val_loss: 4554.7783 - val_mse: 4554.7783 - val_mae: 66.5740\n",
      "Epoch 49/300\n",
      "136/136 [==============================] - 35s 254ms/step - loss: 4562.4194 - mse: 4562.4194 - mae: 66.6181 - val_loss: 4547.8218 - val_mse: 4547.8218 - val_mae: 66.5218\n",
      "Epoch 50/300\n",
      "136/136 [==============================] - 27s 202ms/step - loss: 4555.2178 - mse: 4555.2178 - mae: 66.5645 - val_loss: 4541.0112 - val_mse: 4541.0112 - val_mae: 66.4705\n",
      "Epoch 51/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 4548.3384 - mse: 4548.3384 - mae: 66.5127 - val_loss: 4534.3374 - val_mse: 4534.3374 - val_mae: 66.4203\n",
      "Epoch 52/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 4542.2354 - mse: 4542.2354 - mae: 66.4667 - val_loss: 4527.7930 - val_mse: 4527.7930 - val_mae: 66.3710\n",
      "Epoch 53/300\n",
      "136/136 [==============================] - 25s 180ms/step - loss: 4535.5845 - mse: 4535.5845 - mae: 66.4164 - val_loss: 4521.3760 - val_mse: 4521.3760 - val_mae: 66.3227\n",
      "Epoch 54/300\n",
      "136/136 [==============================] - 31s 232ms/step - loss: 4529.3760 - mse: 4529.3760 - mae: 66.3701 - val_loss: 4515.0776 - val_mse: 4515.0776 - val_mae: 66.2752\n",
      "Epoch 55/300\n",
      "136/136 [==============================] - 47s 344ms/step - loss: 4523.2842 - mse: 4523.2842 - mae: 66.3244 - val_loss: 4508.8989 - val_mse: 4508.8989 - val_mae: 66.2286\n",
      "Epoch 56/300\n",
      "136/136 [==============================] - 30s 221ms/step - loss: 4516.7539 - mse: 4516.7539 - mae: 66.2747 - val_loss: 4502.8320 - val_mse: 4502.8320 - val_mae: 66.1827\n",
      "Epoch 57/300\n",
      "136/136 [==============================] - 54s 401ms/step - loss: 4510.9541 - mse: 4510.9541 - mae: 66.2310 - val_loss: 4496.8682 - val_mse: 4496.8682 - val_mae: 66.1377\n",
      "Epoch 58/300\n",
      "136/136 [==============================] - 38s 277ms/step - loss: 4505.0840 - mse: 4505.0840 - mae: 66.1865 - val_loss: 4491.0151 - val_mse: 4491.0151 - val_mae: 66.0934\n",
      "Epoch 59/300\n",
      "136/136 [==============================] - 25s 181ms/step - loss: 4499.4512 - mse: 4499.4512 - mae: 66.1435 - val_loss: 4485.2617 - val_mse: 4485.2617 - val_mae: 66.0499\n",
      "Epoch 60/300\n",
      "136/136 [==============================] - 26s 189ms/step - loss: 4493.6675 - mse: 4493.6675 - mae: 66.1003 - val_loss: 4479.6050 - val_mse: 4479.6050 - val_mae: 66.0070\n",
      "Epoch 61/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 4488.1387 - mse: 4488.1387 - mae: 66.0581 - val_loss: 4474.0430 - val_mse: 4474.0430 - val_mae: 65.9649\n",
      "Epoch 62/300\n",
      "136/136 [==============================] - 23s 172ms/step - loss: 4482.2705 - mse: 4482.2705 - mae: 66.0140 - val_loss: 4468.5732 - val_mse: 4468.5732 - val_mae: 65.9234\n",
      "Epoch 63/300\n",
      "136/136 [==============================] - 23s 173ms/step - loss: 4477.5107 - mse: 4477.5107 - mae: 65.9778 - val_loss: 4463.1914 - val_mse: 4463.1914 - val_mae: 65.8826\n",
      "Epoch 64/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 4471.8198 - mse: 4471.8198 - mae: 65.9347 - val_loss: 4457.8975 - val_mse: 4457.8975 - val_mae: 65.8424\n",
      "Epoch 65/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 4466.3174 - mse: 4466.3174 - mae: 65.8928 - val_loss: 4452.6880 - val_mse: 4452.6880 - val_mae: 65.8028\n",
      "Epoch 66/300\n",
      "136/136 [==============================] - 26s 191ms/step - loss: 4461.3608 - mse: 4461.3608 - mae: 65.8557 - val_loss: 4447.5605 - val_mse: 4447.5605 - val_mae: 65.7638\n",
      "Epoch 67/300\n",
      "136/136 [==============================] - 33s 246ms/step - loss: 4456.0698 - mse: 4456.0698 - mae: 65.8157 - val_loss: 4442.5142 - val_mse: 4442.5142 - val_mae: 65.7255\n",
      "Epoch 68/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 4451.0854 - mse: 4451.0854 - mae: 65.7777 - val_loss: 4437.5464 - val_mse: 4437.5464 - val_mae: 65.6876\n",
      "Epoch 69/300\n",
      "136/136 [==============================] - 32s 238ms/step - loss: 4446.2407 - mse: 4446.2407 - mae: 65.7405 - val_loss: 4432.6567 - val_mse: 4432.6567 - val_mae: 65.6504\n",
      "Epoch 70/300\n",
      "136/136 [==============================] - 28s 204ms/step - loss: 4441.5015 - mse: 4441.5015 - mae: 65.7044 - val_loss: 4427.8433 - val_mse: 4427.8433 - val_mae: 65.6138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300\n",
      "136/136 [==============================] - 29s 212ms/step - loss: 4436.8511 - mse: 4436.8511 - mae: 65.6694 - val_loss: 4423.1055 - val_mse: 4423.1055 - val_mae: 65.5776\n",
      "Epoch 72/300\n",
      "136/136 [==============================] - 32s 234ms/step - loss: 4432.0210 - mse: 4432.0210 - mae: 65.6322 - val_loss: 4418.4395 - val_mse: 4418.4395 - val_mae: 65.5421\n",
      "Epoch 73/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 4427.2695 - mse: 4427.2695 - mae: 65.5963 - val_loss: 4413.8457 - val_mse: 4413.8457 - val_mae: 65.5070\n",
      "Epoch 74/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 4422.8071 - mse: 4422.8071 - mae: 65.5622 - val_loss: 4409.3188 - val_mse: 4409.3188 - val_mae: 65.4724\n",
      "Epoch 75/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 4418.4312 - mse: 4418.4312 - mae: 65.5288 - val_loss: 4404.8613 - val_mse: 4404.8613 - val_mae: 65.4384\n",
      "Epoch 76/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 4413.9604 - mse: 4413.9604 - mae: 65.4946 - val_loss: 4400.4683 - val_mse: 4400.4683 - val_mae: 65.4048\n",
      "Epoch 77/300\n",
      "136/136 [==============================] - 25s 187ms/step - loss: 4409.6133 - mse: 4409.6133 - mae: 65.4612 - val_loss: 4396.1396 - val_mse: 4396.1396 - val_mae: 65.3717\n",
      "Epoch 78/300\n",
      "136/136 [==============================] - 23s 170ms/step - loss: 4405.2251 - mse: 4405.2251 - mae: 65.4279 - val_loss: 4391.8726 - val_mse: 4391.8726 - val_mae: 65.3391\n",
      "Epoch 79/300\n",
      "136/136 [==============================] - 25s 180ms/step - loss: 4400.9854 - mse: 4400.9854 - mae: 65.3955 - val_loss: 4387.6660 - val_mse: 4387.6660 - val_mae: 65.3069\n",
      "Epoch 80/300\n",
      "136/136 [==============================] - 25s 184ms/step - loss: 4396.8159 - mse: 4396.8159 - mae: 65.3633 - val_loss: 4383.5186 - val_mse: 4383.5186 - val_mae: 65.2751\n",
      "Epoch 81/300\n",
      "136/136 [==============================] - 23s 171ms/step - loss: 4392.6841 - mse: 4392.6841 - mae: 65.3321 - val_loss: 4379.4272 - val_mse: 4379.4272 - val_mae: 65.2438\n",
      "Epoch 82/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 4388.7773 - mse: 4388.7773 - mae: 65.3023 - val_loss: 4375.3936 - val_mse: 4375.3936 - val_mae: 65.2128\n",
      "Epoch 83/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 4384.8228 - mse: 4384.8228 - mae: 65.2721 - val_loss: 4371.4111 - val_mse: 4371.4111 - val_mae: 65.1823\n",
      "Epoch 84/300\n",
      "136/136 [==============================] - 25s 188ms/step - loss: 4380.5425 - mse: 4380.5425 - mae: 65.2389 - val_loss: 4367.4834 - val_mse: 4367.4834 - val_mae: 65.1522\n",
      "Epoch 85/300\n",
      "136/136 [==============================] - 26s 194ms/step - loss: 4376.7153 - mse: 4376.7153 - mae: 65.2094 - val_loss: 4363.6060 - val_mse: 4363.6060 - val_mae: 65.1224\n",
      "Epoch 86/300\n",
      "136/136 [==============================] - 28s 205ms/step - loss: 4373.2480 - mse: 4373.2480 - mae: 65.1829 - val_loss: 4359.7769 - val_mse: 4359.7769 - val_mae: 65.0930\n",
      "Epoch 87/300\n",
      "136/136 [==============================] - 31s 232ms/step - loss: 4369.1187 - mse: 4369.1187 - mae: 65.1516 - val_loss: 4355.9961 - val_mse: 4355.9961 - val_mae: 65.0640\n",
      "Epoch 88/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 4365.3691 - mse: 4365.3691 - mae: 65.1230 - val_loss: 4352.2627 - val_mse: 4352.2627 - val_mae: 65.0353\n",
      "Epoch 89/300\n",
      "136/136 [==============================] - 43s 320ms/step - loss: 4361.5020 - mse: 4361.5020 - mae: 65.0928 - val_loss: 4348.5742 - val_mse: 4348.5742 - val_mae: 65.0069\n",
      "Epoch 90/300\n",
      "136/136 [==============================] - 33s 239ms/step - loss: 4357.9844 - mse: 4357.9844 - mae: 65.0658 - val_loss: 4344.9297 - val_mse: 4344.9297 - val_mae: 64.9789\n",
      "Epoch 91/300\n",
      "136/136 [==============================] - 28s 208ms/step - loss: 4354.4136 - mse: 4354.4136 - mae: 65.0385 - val_loss: 4341.3271 - val_mse: 4341.3271 - val_mae: 64.9511\n",
      "Epoch 92/300\n",
      "136/136 [==============================] - 29s 214ms/step - loss: 4350.8262 - mse: 4350.8262 - mae: 65.0110 - val_loss: 4337.7627 - val_mse: 4337.7627 - val_mae: 64.9237\n",
      "Epoch 93/300\n",
      "136/136 [==============================] - 29s 216ms/step - loss: 4347.1177 - mse: 4347.1177 - mae: 64.9823 - val_loss: 4334.2388 - val_mse: 4334.2388 - val_mae: 64.8965\n",
      "Epoch 94/300\n",
      "136/136 [==============================] - 35s 261ms/step - loss: 4343.6533 - mse: 4343.6533 - mae: 64.9558 - val_loss: 4330.7510 - val_mse: 4330.7510 - val_mae: 64.8697\n",
      "Epoch 95/300\n",
      "136/136 [==============================] - 45s 333ms/step - loss: 4340.5850 - mse: 4340.5850 - mae: 64.9320 - val_loss: 4327.2988 - val_mse: 4327.2988 - val_mae: 64.8430\n",
      "Epoch 96/300\n",
      "136/136 [==============================] - 33s 245ms/step - loss: 4337.0215 - mse: 4337.0215 - mae: 64.9047 - val_loss: 4323.8804 - val_mse: 4323.8804 - val_mae: 64.8167\n",
      "Epoch 97/300\n",
      "136/136 [==============================] - 27s 199ms/step - loss: 4333.6021 - mse: 4333.6021 - mae: 64.8782 - val_loss: 4320.4961 - val_mse: 4320.4961 - val_mae: 64.7906\n",
      "Epoch 98/300\n",
      "136/136 [==============================] - 27s 196ms/step - loss: 4330.1162 - mse: 4330.1162 - mae: 64.8517 - val_loss: 4317.1431 - val_mse: 4317.1431 - val_mae: 64.7647\n",
      "Epoch 99/300\n",
      "136/136 [==============================] - 34s 251ms/step - loss: 4326.7241 - mse: 4326.7241 - mae: 64.8251 - val_loss: 4313.8193 - val_mse: 4313.8193 - val_mae: 64.7390\n",
      "Epoch 100/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 4323.4829 - mse: 4323.4829 - mae: 64.7999 - val_loss: 4310.5269 - val_mse: 4310.5269 - val_mae: 64.7136\n",
      "Epoch 101/300\n",
      "136/136 [==============================] - 34s 251ms/step - loss: 4320.2046 - mse: 4320.2046 - mae: 64.7750 - val_loss: 4307.2612 - val_mse: 4307.2612 - val_mae: 64.6884\n",
      "Epoch 102/300\n",
      "136/136 [==============================] - 33s 241ms/step - loss: 4316.9741 - mse: 4316.9741 - mae: 64.7500 - val_loss: 4304.0220 - val_mse: 4304.0220 - val_mae: 64.6633\n",
      "Epoch 103/300\n",
      "136/136 [==============================] - 38s 281ms/step - loss: 4313.8760 - mse: 4313.8760 - mae: 64.7259 - val_loss: 4300.8115 - val_mse: 4300.8115 - val_mae: 64.6385\n",
      "Epoch 104/300\n",
      "136/136 [==============================] - 38s 276ms/step - loss: 4310.5381 - mse: 4310.5381 - mae: 64.7001 - val_loss: 4297.6279 - val_mse: 4297.6279 - val_mae: 64.6139\n",
      "Epoch 105/300\n",
      "136/136 [==============================] - 35s 258ms/step - loss: 4307.2056 - mse: 4307.2056 - mae: 64.6750 - val_loss: 4294.4717 - val_mse: 4294.4717 - val_mae: 64.5894\n",
      "Epoch 106/300\n",
      "136/136 [==============================] - 31s 228ms/step - loss: 4304.3525 - mse: 4304.3525 - mae: 64.6526 - val_loss: 4291.3398 - val_mse: 4291.3398 - val_mae: 64.5652\n",
      "Epoch 107/300\n",
      "136/136 [==============================] - 36s 264ms/step - loss: 4300.9390 - mse: 4300.9390 - mae: 64.6259 - val_loss: 4288.2383 - val_mse: 4288.2383 - val_mae: 64.5411\n",
      "Epoch 108/300\n",
      "136/136 [==============================] - 28s 206ms/step - loss: 4297.7573 - mse: 4297.7573 - mae: 64.6013 - val_loss: 4285.1616 - val_mse: 4285.1616 - val_mae: 64.5173\n",
      "Epoch 109/300\n",
      "136/136 [==============================] - 28s 203ms/step - loss: 4295.1538 - mse: 4295.1538 - mae: 64.5813 - val_loss: 4282.1128 - val_mse: 4282.1128 - val_mae: 64.4937\n",
      "Epoch 110/300\n",
      "136/136 [==============================] - 31s 232ms/step - loss: 4291.9839 - mse: 4291.9839 - mae: 64.5569 - val_loss: 4279.0908 - val_mse: 4279.0908 - val_mae: 64.4702\n",
      "Epoch 111/300\n",
      "136/136 [==============================] - 32s 237ms/step - loss: 4288.8926 - mse: 4288.8926 - mae: 64.5326 - val_loss: 4276.0981 - val_mse: 4276.0981 - val_mae: 64.4470\n",
      "Epoch 112/300\n",
      "136/136 [==============================] - 35s 258ms/step - loss: 4286.1045 - mse: 4286.1045 - mae: 64.5114 - val_loss: 4273.1328 - val_mse: 4273.1328 - val_mae: 64.4240\n",
      "Epoch 113/300\n",
      "136/136 [==============================] - 35s 257ms/step - loss: 4282.9482 - mse: 4282.9482 - mae: 64.4867 - val_loss: 4270.1948 - val_mse: 4270.1948 - val_mae: 64.4012\n",
      "Epoch 114/300\n",
      "136/136 [==============================] - 30s 224ms/step - loss: 4280.1313 - mse: 4280.1313 - mae: 64.4649 - val_loss: 4267.2852 - val_mse: 4267.2852 - val_mae: 64.3786\n",
      "Epoch 115/300\n",
      "136/136 [==============================] - 26s 190ms/step - loss: 4276.9751 - mse: 4276.9751 - mae: 64.4404 - val_loss: 4264.4009 - val_mse: 4264.4009 - val_mae: 64.3562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300\n",
      "136/136 [==============================] - 35s 255ms/step - loss: 4273.9346 - mse: 4273.9346 - mae: 64.4169 - val_loss: 4261.5449 - val_mse: 4261.5449 - val_mae: 64.3340\n",
      "Epoch 117/300\n",
      "136/136 [==============================] - 36s 264ms/step - loss: 4271.4736 - mse: 4271.4736 - mae: 64.3978 - val_loss: 4258.7153 - val_mse: 4258.7153 - val_mae: 64.3120\n",
      "Epoch 118/300\n",
      "136/136 [==============================] - 28s 209ms/step - loss: 4268.4727 - mse: 4268.4727 - mae: 64.3742 - val_loss: 4255.9106 - val_mse: 4255.9106 - val_mae: 64.2902\n",
      "Epoch 119/300\n",
      "136/136 [==============================] - 32s 233ms/step - loss: 4265.5508 - mse: 4265.5508 - mae: 64.3516 - val_loss: 4253.1318 - val_mse: 4253.1318 - val_mae: 64.2686\n",
      "Epoch 120/300\n",
      "136/136 [==============================] - 29s 211ms/step - loss: 4263.0391 - mse: 4263.0391 - mae: 64.3318 - val_loss: 4250.3789 - val_mse: 4250.3789 - val_mae: 64.2472\n",
      "Epoch 121/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 4260.1323 - mse: 4260.1323 - mae: 64.3096 - val_loss: 4247.6499 - val_mse: 4247.6499 - val_mae: 64.2260\n",
      "Epoch 122/300\n",
      "136/136 [==============================] - 26s 188ms/step - loss: 4257.5713 - mse: 4257.5713 - mae: 64.2896 - val_loss: 4244.9448 - val_mse: 4244.9448 - val_mae: 64.2049\n",
      "Epoch 123/300\n",
      "136/136 [==============================] - 30s 220ms/step - loss: 4254.5659 - mse: 4254.5659 - mae: 64.2662 - val_loss: 4242.2661 - val_mse: 4242.2661 - val_mae: 64.1840\n",
      "Epoch 124/300\n",
      "136/136 [==============================] - 37s 269ms/step - loss: 4252.4878 - mse: 4252.4878 - mae: 64.2500 - val_loss: 4239.6084 - val_mse: 4239.6084 - val_mae: 64.1633\n",
      "Epoch 125/300\n",
      "136/136 [==============================] - 27s 197ms/step - loss: 4249.6675 - mse: 4249.6675 - mae: 64.2280 - val_loss: 4236.9756 - val_mse: 4236.9756 - val_mae: 64.1428\n",
      "Epoch 126/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 4247.4316 - mse: 4247.4316 - mae: 64.2102 - val_loss: 4234.3643 - val_mse: 4234.3643 - val_mae: 64.1224\n",
      "Epoch 127/300\n",
      "136/136 [==============================] - 119s 881ms/step - loss: 4244.3613 - mse: 4244.3613 - mae: 64.1867 - val_loss: 4231.7769 - val_mse: 4231.7769 - val_mae: 64.1023\n",
      "Epoch 128/300\n",
      "136/136 [==============================] - 82s 608ms/step - loss: 4241.9990 - mse: 4241.9990 - mae: 64.1680 - val_loss: 4229.2114 - val_mse: 4229.2114 - val_mae: 64.0822\n",
      "Epoch 129/300\n",
      "136/136 [==============================] - 59s 435ms/step - loss: 4239.2344 - mse: 4239.2344 - mae: 64.1472 - val_loss: 4226.6685 - val_mse: 4226.6685 - val_mae: 64.0624\n",
      "Epoch 130/300\n",
      "136/136 [==============================] - 41s 299ms/step - loss: 4236.5801 - mse: 4236.5801 - mae: 64.1262 - val_loss: 4224.1479 - val_mse: 4224.1479 - val_mae: 64.0427\n",
      "Epoch 131/300\n",
      "136/136 [==============================] - 25s 186ms/step - loss: 4234.1309 - mse: 4234.1309 - mae: 64.1069 - val_loss: 4221.6509 - val_mse: 4221.6509 - val_mae: 64.0232\n",
      "Epoch 132/300\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 4231.6919 - mse: 4231.6919 - mae: 64.0886 - val_loss: 4219.1733 - val_mse: 4219.1733 - val_mae: 64.0039\n",
      "Epoch 133/300\n",
      "136/136 [==============================] - 24s 177ms/step - loss: 4229.1812 - mse: 4229.1812 - mae: 64.0685 - val_loss: 4216.7183 - val_mse: 4216.7183 - val_mae: 63.9847\n",
      "Epoch 134/300\n",
      "136/136 [==============================] - 26s 195ms/step - loss: 4226.8320 - mse: 4226.8320 - mae: 64.0500 - val_loss: 4214.2852 - val_mse: 4214.2852 - val_mae: 63.9657\n",
      "Epoch 135/300\n",
      "136/136 [==============================] - 27s 200ms/step - loss: 4224.0522 - mse: 4224.0522 - mae: 64.0284 - val_loss: 4211.8735 - val_mse: 4211.8735 - val_mae: 63.9468\n",
      "Epoch 136/300\n",
      "136/136 [==============================] - 40s 292ms/step - loss: 4221.8906 - mse: 4221.8906 - mae: 64.0115 - val_loss: 4209.4829 - val_mse: 4209.4829 - val_mae: 63.9281\n",
      "Epoch 137/300\n",
      "136/136 [==============================] - 38s 283ms/step - loss: 4219.4233 - mse: 4219.4233 - mae: 63.9926 - val_loss: 4207.1133 - val_mse: 4207.1133 - val_mae: 63.9096\n",
      "Epoch 138/300\n",
      "136/136 [==============================] - 36s 267ms/step - loss: 4217.0996 - mse: 4217.0996 - mae: 63.9742 - val_loss: 4204.7651 - val_mse: 4204.7651 - val_mae: 63.8912\n",
      "Epoch 139/300\n",
      "136/136 [==============================] - 45s 334ms/step - loss: 4215.0566 - mse: 4215.0566 - mae: 63.9584 - val_loss: 4202.4370 - val_mse: 4202.4370 - val_mae: 63.8730\n",
      "Epoch 140/300\n",
      "136/136 [==============================] - 36s 267ms/step - loss: 4212.2974 - mse: 4212.2974 - mae: 63.9368 - val_loss: 4200.1299 - val_mse: 4200.1299 - val_mae: 63.8549\n",
      "Epoch 141/300\n",
      "136/136 [==============================] - 68s 503ms/step - loss: 4210.2446 - mse: 4210.2446 - mae: 63.9202 - val_loss: 4197.8418 - val_mse: 4197.8418 - val_mae: 63.8370\n",
      "Epoch 142/300\n",
      "136/136 [==============================] - 48s 346ms/step - loss: 4207.8398 - mse: 4207.8398 - mae: 63.9017 - val_loss: 4195.5752 - val_mse: 4195.5752 - val_mae: 63.8192\n",
      "Epoch 143/300\n",
      "136/136 [==============================] - 38s 281ms/step - loss: 4205.7212 - mse: 4205.7212 - mae: 63.8853 - val_loss: 4193.3281 - val_mse: 4193.3281 - val_mae: 63.8017\n",
      "Epoch 144/300\n",
      "136/136 [==============================] - 41s 302ms/step - loss: 4203.2993 - mse: 4203.2993 - mae: 63.8664 - val_loss: 4191.1011 - val_mse: 4191.1011 - val_mae: 63.7842\n",
      "Epoch 145/300\n",
      "136/136 [==============================] - 52s 382ms/step - loss: 4201.0820 - mse: 4201.0820 - mae: 63.8488 - val_loss: 4188.8936 - val_mse: 4188.8936 - val_mae: 63.7669\n",
      "Epoch 146/300\n",
      "136/136 [==============================] - 86s 637ms/step - loss: 4199.0234 - mse: 4199.0234 - mae: 63.8325 - val_loss: 4186.7051 - val_mse: 4186.7051 - val_mae: 63.7497\n",
      "Epoch 147/300\n",
      "136/136 [==============================] - 126s 921ms/step - loss: 4196.7236 - mse: 4196.7236 - mae: 63.8152 - val_loss: 4184.5356 - val_mse: 4184.5356 - val_mae: 63.7327\n",
      "Epoch 148/300\n",
      "136/136 [==============================] - 51s 378ms/step - loss: 4194.5527 - mse: 4194.5527 - mae: 63.7976 - val_loss: 4182.3857 - val_mse: 4182.3857 - val_mae: 63.7158\n",
      "Epoch 149/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 4192.4561 - mse: 4192.4561 - mae: 63.7812 - val_loss: 4180.2539 - val_mse: 4180.2539 - val_mae: 63.6991\n",
      "Epoch 150/300\n",
      "136/136 [==============================] - 48s 358ms/step - loss: 4190.2627 - mse: 4190.2627 - mae: 63.7640 - val_loss: 4178.1406 - val_mse: 4178.1406 - val_mae: 63.6825\n",
      "Epoch 151/300\n",
      "136/136 [==============================] - 39s 281ms/step - loss: 4188.0801 - mse: 4188.0801 - mae: 63.7468 - val_loss: 4176.0464 - val_mse: 4176.0464 - val_mae: 63.6661\n",
      "Epoch 152/300\n",
      "136/136 [==============================] - 38s 281ms/step - loss: 4185.9956 - mse: 4185.9956 - mae: 63.7306 - val_loss: 4173.9688 - val_mse: 4173.9688 - val_mae: 63.6498\n",
      "Epoch 153/300\n",
      "136/136 [==============================] - 36s 268ms/step - loss: 4183.6685 - mse: 4183.6685 - mae: 63.7124 - val_loss: 4171.9102 - val_mse: 4171.9102 - val_mae: 63.6336\n",
      "Epoch 154/300\n",
      "136/136 [==============================] - 35s 258ms/step - loss: 4181.9678 - mse: 4181.9678 - mae: 63.6991 - val_loss: 4169.8672 - val_mse: 4169.8672 - val_mae: 63.6175\n",
      "Epoch 155/300\n",
      "136/136 [==============================] - 56s 416ms/step - loss: 4179.7437 - mse: 4179.7437 - mae: 63.6814 - val_loss: 4167.8428 - val_mse: 4167.8428 - val_mae: 63.6016\n",
      "Epoch 156/300\n",
      "136/136 [==============================] - 84s 613ms/step - loss: 4177.7354 - mse: 4177.7354 - mae: 63.6659 - val_loss: 4165.8364 - val_mse: 4165.8364 - val_mae: 63.5858\n",
      "Epoch 157/300\n",
      "136/136 [==============================] - 63s 456ms/step - loss: 4175.7935 - mse: 4175.7935 - mae: 63.6507 - val_loss: 4163.8452 - val_mse: 4163.8452 - val_mae: 63.5702\n",
      "Epoch 158/300\n",
      "136/136 [==============================] - 47s 348ms/step - loss: 4173.3794 - mse: 4173.3794 - mae: 63.6316 - val_loss: 4161.8721 - val_mse: 4161.8721 - val_mae: 63.5547\n",
      "Epoch 159/300\n",
      "136/136 [==============================] - 30s 221ms/step - loss: 4171.6982 - mse: 4171.6982 - mae: 63.6183 - val_loss: 4159.9160 - val_mse: 4159.9160 - val_mae: 63.5393\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 28s 204ms/step - loss: 4169.9277 - mse: 4169.9277 - mae: 63.6042 - val_loss: 4157.9741 - val_mse: 4157.9741 - val_mae: 63.5240\n",
      "Epoch 161/300\n",
      "136/136 [==============================] - 38s 278ms/step - loss: 4167.7817 - mse: 4167.7817 - mae: 63.5877 - val_loss: 4156.0498 - val_mse: 4156.0498 - val_mae: 63.5088\n",
      "Epoch 162/300\n",
      "136/136 [==============================] - 38s 281ms/step - loss: 4166.0591 - mse: 4166.0591 - mae: 63.5739 - val_loss: 4154.1396 - val_mse: 4154.1396 - val_mae: 63.4938\n",
      "Epoch 163/300\n",
      "136/136 [==============================] - 29s 213ms/step - loss: 4164.0640 - mse: 4164.0640 - mae: 63.5582 - val_loss: 4152.2461 - val_mse: 4152.2461 - val_mae: 63.4789\n",
      "Epoch 164/300\n",
      "136/136 [==============================] - 31s 227ms/step - loss: 4162.3462 - mse: 4162.3462 - mae: 63.5445 - val_loss: 4150.3672 - val_mse: 4150.3672 - val_mae: 63.4641\n",
      "Epoch 165/300\n",
      "136/136 [==============================] - 43s 321ms/step - loss: 4160.4370 - mse: 4160.4370 - mae: 63.5294 - val_loss: 4148.5024 - val_mse: 4148.5024 - val_mae: 63.4494\n",
      "Epoch 166/300\n",
      "136/136 [==============================] - 41s 306ms/step - loss: 4158.3726 - mse: 4158.3726 - mae: 63.5137 - val_loss: 4146.6538 - val_mse: 4146.6538 - val_mae: 63.4348\n",
      "Epoch 167/300\n",
      "136/136 [==============================] - 30s 217ms/step - loss: 4156.7432 - mse: 4156.7432 - mae: 63.5006 - val_loss: 4144.8184 - val_mse: 4144.8184 - val_mae: 63.4204\n",
      "Epoch 168/300\n",
      "136/136 [==============================] - 37s 274ms/step - loss: 4154.8271 - mse: 4154.8271 - mae: 63.4854 - val_loss: 4142.9985 - val_mse: 4142.9985 - val_mae: 63.4060\n",
      "Epoch 169/300\n",
      "136/136 [==============================] - 34s 251ms/step - loss: 4152.9883 - mse: 4152.9883 - mae: 63.4710 - val_loss: 4141.1919 - val_mse: 4141.1919 - val_mae: 63.3918\n",
      "Epoch 170/300\n",
      "136/136 [==============================] - 42s 308ms/step - loss: 4151.0972 - mse: 4151.0972 - mae: 63.4559 - val_loss: 4139.4004 - val_mse: 4139.4004 - val_mae: 63.3776\n",
      "Epoch 171/300\n",
      "136/136 [==============================] - 31s 225ms/step - loss: 4149.4707 - mse: 4149.4707 - mae: 63.4431 - val_loss: 4137.6216 - val_mse: 4137.6216 - val_mae: 63.3636\n",
      "Epoch 172/300\n",
      "136/136 [==============================] - 63s 467ms/step - loss: 4147.1626 - mse: 4147.1626 - mae: 63.4254 - val_loss: 4135.8579 - val_mse: 4135.8579 - val_mae: 63.3497\n",
      "Epoch 173/300\n",
      "136/136 [==============================] - 31s 229ms/step - loss: 4145.6831 - mse: 4145.6831 - mae: 63.4136 - val_loss: 4134.1064 - val_mse: 4134.1064 - val_mae: 63.3358\n",
      "Epoch 174/300\n",
      "136/136 [==============================] - 26s 192ms/step - loss: 4143.9985 - mse: 4143.9985 - mae: 63.4001 - val_loss: 4132.3687 - val_mse: 4132.3687 - val_mae: 63.3221\n",
      "Epoch 175/300\n",
      "136/136 [==============================] - 54s 397ms/step - loss: 4142.2886 - mse: 4142.2886 - mae: 63.3866 - val_loss: 4130.6426 - val_mse: 4130.6426 - val_mae: 63.3085\n",
      "Epoch 176/300\n",
      "136/136 [==============================] - 72s 532ms/step - loss: 4140.7397 - mse: 4140.7397 - mae: 63.3741 - val_loss: 4128.9297 - val_mse: 4128.9297 - val_mae: 63.2950\n",
      "Epoch 177/300\n",
      "136/136 [==============================] - 37s 276ms/step - loss: 4138.7739 - mse: 4138.7739 - mae: 63.3591 - val_loss: 4127.2295 - val_mse: 4127.2295 - val_mae: 63.2815\n",
      "Epoch 178/300\n",
      "136/136 [==============================] - 40s 296ms/step - loss: 4137.3335 - mse: 4137.3335 - mae: 63.3474 - val_loss: 4125.5420 - val_mse: 4125.5420 - val_mae: 63.2682\n",
      "Epoch 179/300\n",
      "136/136 [==============================] - 32s 231ms/step - loss: 4135.3711 - mse: 4135.3711 - mae: 63.3319 - val_loss: 4123.8657 - val_mse: 4123.8657 - val_mae: 63.2549\n",
      "Epoch 180/300\n",
      "136/136 [==============================] - 43s 321ms/step - loss: 4133.7144 - mse: 4133.7144 - mae: 63.3189 - val_loss: 4122.2036 - val_mse: 4122.2036 - val_mae: 63.2418\n",
      "Epoch 181/300\n",
      "136/136 [==============================] - 34s 251ms/step - loss: 4132.0928 - mse: 4132.0928 - mae: 63.3063 - val_loss: 4120.5518 - val_mse: 4120.5518 - val_mae: 63.2287\n",
      "Epoch 182/300\n",
      "136/136 [==============================] - 30s 219ms/step - loss: 4130.3906 - mse: 4130.3906 - mae: 63.2932 - val_loss: 4118.9116 - val_mse: 4118.9116 - val_mae: 63.2158\n",
      "Epoch 183/300\n",
      "136/136 [==============================] - 36s 268ms/step - loss: 4128.4355 - mse: 4128.4355 - mae: 63.2776 - val_loss: 4117.2842 - val_mse: 4117.2842 - val_mae: 63.2029\n",
      "Epoch 184/300\n",
      "136/136 [==============================] - 39s 290ms/step - loss: 4127.1899 - mse: 4127.1899 - mae: 63.2676 - val_loss: 4115.6680 - val_mse: 4115.6680 - val_mae: 63.1901\n",
      "Epoch 185/300\n",
      "136/136 [==============================] - 39s 287ms/step - loss: 4125.4897 - mse: 4125.4897 - mae: 63.2542 - val_loss: 4114.0615 - val_mse: 4114.0615 - val_mae: 63.1774\n",
      "Epoch 186/300\n",
      "136/136 [==============================] - 34s 248ms/step - loss: 4123.8081 - mse: 4123.8081 - mae: 63.2408 - val_loss: 4112.4668 - val_mse: 4112.4668 - val_mae: 63.1648\n",
      "Epoch 187/300\n",
      "136/136 [==============================] - 30s 219ms/step - loss: 4122.2896 - mse: 4122.2896 - mae: 63.2289 - val_loss: 4110.8833 - val_mse: 4110.8833 - val_mae: 63.1522\n",
      "Epoch 188/300\n",
      "136/136 [==============================] - 27s 198ms/step - loss: 4120.7275 - mse: 4120.7275 - mae: 63.2166 - val_loss: 4109.3091 - val_mse: 4109.3091 - val_mae: 63.1398\n",
      "Epoch 189/300\n",
      "136/136 [==============================] - 29s 210ms/step - loss: 4118.9258 - mse: 4118.9258 - mae: 63.2023 - val_loss: 4107.7466 - val_mse: 4107.7466 - val_mae: 63.1274\n",
      "Epoch 190/300\n",
      "136/136 [==============================] - 28s 207ms/step - loss: 4117.8687 - mse: 4117.8687 - mae: 63.1942 - val_loss: 4106.1934 - val_mse: 4106.1934 - val_mae: 63.1151\n",
      "Epoch 191/300\n",
      "136/136 [==============================] - 29s 212ms/step - loss: 4116.0020 - mse: 4116.0020 - mae: 63.1793 - val_loss: 4104.6514 - val_mse: 4104.6514 - val_mae: 63.1029\n",
      "Epoch 192/300\n",
      "136/136 [==============================] - 27s 195ms/step - loss: 4114.7295 - mse: 4114.7295 - mae: 63.1688 - val_loss: 4103.1187 - val_mse: 4103.1187 - val_mae: 63.0907\n",
      "Epoch 193/300\n",
      "136/136 [==============================] - 30s 218ms/step - loss: 4112.5586 - mse: 4112.5586 - mae: 63.1518 - val_loss: 4101.5967 - val_mse: 4101.5967 - val_mae: 63.0787\n",
      "Epoch 194/300\n",
      "136/136 [==============================] - 25s 183ms/step - loss: 4111.3677 - mse: 4111.3677 - mae: 63.1422 - val_loss: 4100.0830 - val_mse: 4100.0830 - val_mae: 63.0667\n",
      "Epoch 195/300\n",
      "136/136 [==============================] - 22s 163ms/step - loss: 4109.8340 - mse: 4109.8340 - mae: 63.1303 - val_loss: 4098.5801 - val_mse: 4098.5801 - val_mae: 63.0548\n",
      "Epoch 196/300\n",
      " 67/136 [=============>................] - ETA: 19s - loss: 4083.9509 - mse: 4083.9509 - mae: 62.9641"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32486/2941267320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStoppingIncreasingValLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincrease_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCH\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       (graph_function,\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7fbc9cceb0d0> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;31m# object handled itself, don't proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfix\u001b[0;34m(args, kwargs, sig)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mFix\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconsistent\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed for test_dan_schult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m         \"\"\"\n\u001b[0;32m-> 3043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m         \u001b[0mparameters_ex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0marg_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lr_list = [5e-4 ,5e-5 ,5e-3]\n",
    "loss_list = ['mse' , 'mae']\n",
    "\n",
    "for loss in loss_list:\n",
    "    for lr in lr_list:\n",
    "        optimizer_list = [RMSprop(learning_rate=lr) ,Adam(learning_rate=lr) ,SGD(learning_rate=lr) ,Adagrad(learning_rate=lr)]\n",
    "        for optimizer in optimizer_list:\n",
    "            \n",
    "            models = Models()\n",
    "            model = models.import_model(model_name)\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss=loss, optimizer=optimizer, metrics=['mse','mae'])\n",
    "            \n",
    "            early_stopping = EarlyStoppingIncreasingValLoss(patience=2, delta=0, check_interval=5, increase_threshold=5)\n",
    "            history = model.fit(X_train, y_train, validation_split=0.2, epochs=NUM_EPOCH ,callbacks=[early_stopping])\n",
    "\n",
    "            optimizer_name = optimizer.get_config()['name']\n",
    "\n",
    "            result_name = f'{model_name}_lr:{lr}_NUM_EPOCH:{NUM_EPOCH}_optimizer:{optimizer_name}_loss:{loss}'\n",
    "            folder_path = f'./hr_optimization_results/{result_name}'\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "            history_dict = history.history\n",
    "            with open(f'{folder_path}/history.json', 'w') as f:\n",
    "                json.dump(history_dict, f)\n",
    "\n",
    "            results = model.evaluate(X_test, y_test)\n",
    "            save_model(model, f\"{folder_path}/model.h5\")\n",
    "\n",
    "            y_predicted = model.predict(X_test)\n",
    "\n",
    "            if scaled == True:\n",
    "                y_predicted_temp = scaler.inverse_transform(y_predicted)\n",
    "                y_test_temp = scaler.inverse_transform(y_test)\n",
    "            else:\n",
    "                y_predicted_temp = y_predicted\n",
    "                y_test_temp = y_test\n",
    "\n",
    "\n",
    "            y_predicted_df = pd.DataFrame(y_predicted_temp)\n",
    "            y_test_df = pd.DataFrame(y_test_temp)\n",
    "\n",
    "            plot_scatter_hr(y_predicted_df,y_test_df ,folder_path)\n",
    "\n",
    "            mae , mse = calculate_loss_hr(y_predicted_df,y_test_df)\n",
    "\n",
    "            plot_loss(history_dict ,folder_path)\n",
    "\n",
    "            out_dict = {\n",
    "                    'model_name':[model_name],\n",
    "                    'lr':[lr],\n",
    "                    'epochs':[NUM_EPOCH],\n",
    "                    'optimizer':[optimizer_name],\n",
    "                    'loss':[loss],\n",
    "                    'results':[results],\n",
    "                    'mae':[mae],\n",
    "                    'mse':[mse],\n",
    "                    'time':[time.time()]\n",
    "\n",
    "                    }\n",
    "\n",
    "            save_data(out_dict, 'sheets/hr_model_optimizing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b3241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HR",
   "language": "python",
   "name": "hr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
